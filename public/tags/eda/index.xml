<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>EDA on Jorge Cimentada</title>
    <link>/tags/eda/</link>
    <description>Recent content in EDA on Jorge Cimentada</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Thu, 14 Dec 2017 00:00:00 +0000</lastBuildDate>
    
    <atom:link href="/tags/eda/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Brief Analysis of Independent/Unionist Vote in Catalonia</title>
      <link>/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/brief-analysis-of-independent-unionist-vote-in-catalonia/</link>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/brief-analysis-of-independent-unionist-vote-in-catalonia/</guid>
      <description><![CDATA[
      


<div id="catalan-elections" class="section level2">
<h2>Catalan elections</h2>
<p>On a train from Barcelona-Madrid I started working with an <code>R</code> package called <code>ggrides</code>. To my surprise, the package contains one dataset that documents the change in independent/unionist vote for all Catalan municipalities from 1980 to 2015. This is very cool! Needless to say, I left what I was doing and started to dive into the dataset.</p>
<p>The data looks like this:</p>
<pre class="r"><code>library(ggridges)
Catalan_elections</code></pre>
<pre><code>## # A tibble: 20,764 x 4
##    Municipality        Year Option Percent
##    &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;
##  1 Abella de la Conca  1980 Indy      68.4
##  2 Abella de la Conca  1984 Indy      95.7
##  3 Abella de la Conca  1988 Indy      89.4
##  4 Abella de la Conca  1992 Indy      81.7
##  5 Abella de la Conca  1995 Indy      80.0
##  6 Abella de la Conca  1999 Indy      74.7
##  7 Abella de la Conca  2003 Indy      84.4
##  8 Abella de la Conca  2006 Indy      73.2
##  9 Abella de la Conca  2010 Indy      75.9
## 10 Abella de la Conca  2012 Indy      84.0
## # ... with 20,754 more rows</code></pre>
<p>Very straight forward. It’s the ‘Indy’ or Independent vote and the ‘Unionist’ vote from 1980 until 2015. The data is complete for nearly all Municipalities, meaning that the data is available for all years. Only a handful (~ 40) do not have data starting from 1980.</p>
<p>Basically, I wanted to answer one question: has the indepence vote grown over time? This question is of general interest considering that the topic is being hotly debated in the media and next week new Catalan elections will be held in a scenario never seen before; after independent parties proclaimed unilateral independece and the government seized control of Catalunya. The elections are predicted to be very contested with Independent parties losing some votes.</p>
<p>With that said, let’s dive into the data!</p>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<pre class="r"><code># Load my libraries
library(scales)
library(tidyverse)

# Change abbreviated Indy and Unionist to long names
Catalan_elections$Option &lt;- with(Catalan_elections, ifelse(Option == &quot;Indy&quot;, &quot;Independent&quot;, &quot;Unionist&quot;))


# Summarize the median independence/unionist vote for
# all municipalities on the first/last year recorded
avg_pl &lt;-
  Catalan_elections %&gt;%
  group_by(Municipality, Option) %&gt;%
  summarize(first_year = first(Percent, Year),
            last_year = last(Percent, Year)) %&gt;%
  group_by(Option) %&gt;%
  summarize(first_year = median(first_year, na.rm = TRUE),
            last_year = median(last_year, na.rm = TRUE)) %&gt;%
  mutate(id = 1:nrow(.)) %&gt;%
  gather(year, value, -id, -Option)

# Summarize the indy/unionist vote for
# the first/last year for Barcelona
bcn_pl &lt;-
  Catalan_elections %&gt;%
  filter(Municipality == &quot;Barcelona&quot;) %&gt;%
  group_by(Municipality, Option) %&gt;%
  summarize(first_year = first(Percent, Year),
            last_year = last(Percent, Year)) %&gt;%
  mutate(id = 1:nrow(.)) %&gt;%
  gather(year, value, ends_with(&quot;year&quot;))

# Create a base parallel plot with both
# unionist/independence votes pooled
base_plot &lt;-
  Catalan_elections %&gt;%
  group_by(Municipality, Option) %&gt;%
  summarize(first_year = first(Percent, Year),
            last_year = last(Percent, Year)) %&gt;%
  mutate(id = paste0(Municipality, &quot;_&quot;, Option)) %&gt;%
  gather(year, value, ends_with(&quot;year&quot;)) %&gt;%
  ggplot(aes(year, value)) +
  geom_point(alpha = 0.1, size = 2) +
  geom_line(aes(group = id), alpha = 0.1)

# Add the median summary line for both indy/unionist
median_plot &lt;-
  base_plot +
  geom_point(data = avg_pl, aes(year, value),
            colour = &quot;red&quot;, alpha = 0.5, size = 2) +
  geom_line(data = avg_pl, aes(year, value, group = id),
            colour = &quot;red&quot;, alpha = 0.5, size = 2)

# Add the change of Barcelona for both indy/unionist vote
bcn_plot &lt;-
  median_plot +
  geom_point(data = bcn_pl, aes(year, value),
             colour = &quot;blue&quot;, alpha = 0.5, size = 2) +
  geom_line(data = bcn_pl, aes(year, value, group = id),
            colour = &quot;blue&quot;, alpha = 0.5, size = 2)


# Separate the plot for indy/unionist in different
# panels and add pretty options
pretty_plot &lt;-
  bcn_plot +
  scale_x_discrete(name = NULL,
                   labels = c(&quot;1980&quot;,
                              &quot;2015&quot;)) +
  scale_y_continuous(name = &quot;% of votes in favour of:&quot;,
                     breaks = seq(0, 100, 20),
                     labels = percent(seq(0, 1, 0.2))) +
  facet_wrap(~ Option, strip.position = &quot;bottom&quot;) +
  labs(
    title = &quot;Independence/Unionist vote in Catalonia in three decades&quot;,
    subtitle = &quot;Red line is the median change for all municipalities - Blue line is Barcelona&quot;,
    caption = &quot;Data collected by @marcbeldata - Plot and analisis by @cimentadaj&quot;
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, family = &quot;Arial-BoldMT&quot;),
    plot.subtitle = element_text(size = 12, color = &quot;#666666&quot;),
    plot.caption = element_text(size = 12, color = &quot;#666666&quot;),
    strip.text.x = element_text(size = 14),
    axis.title.y = element_text(size = 16),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 14)
  )
# Final plot
pretty_plot</code></pre>
<p><img src="/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/2017-12-14-brief-analysis-of-independent-unionist-vote-in-catalonia_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>I was very surprised with this plot. On the left panel we can see the increase of independence votes in the last 35 years. The red line is the median change for all municipalities. There is a huge average increase from around 49% to over 70%. In fact, it’s not just an artifact of mean/median with big variance. If we look at the bulk of the distribution on the right line and then the left line, we see an upwards shift in the whole distribution.</p>
<p>On the other hand, the unionist vote seems to have decreased! The left/right distributions seem to be very similar (but it looks like the distribution of the right line has some outliers shifting upwards). But remember something: these are all municipalities. Municipalities might have 1000 citizen or even less! Consider the lonely town in a mountain with 50 people voting for independent parties: that’s also a municipality.</p>
<p>It is for this reason that we need to pay attention to places like Barcelona, which have over 1 million residents and definately weigh in more in proportion. And that’s where the interesting thing about this plot arises: the Barcelona change is practically the same. Not only have the votes increased very very similarly for both sides, but they’re also at the same level of support. Both blue lines look pretty much identical.</p>
<p>Don’t forget: small differences <strong>can</strong> make a difference, specially in elections. Perhaps they <strong>are</strong> different but we need to take a closer look.</p>
<p>Let’s plot the independence/unionist evolution only for Barcelona.</p>
<pre class="r"><code># Plot for indy/unionist vote over time only for Barcelona
Catalan_elections %&gt;%
  filter(Municipality == &quot;Barcelona&quot;) %&gt;%
  ggplot(aes(Year, Percent, group = Option, colour = Option)) +
  geom_line(alpha = 0.5, size = 2) +
  scale_x_continuous(name = NULL) +
  scale_colour_discrete(name = NULL) +
  scale_y_continuous(name = &quot;% of votes in favour of:&quot;,
                     lim = c(0, 100),
                     breaks = seq(0, 100, 20),
                     labels = percent(seq(0, 1, 0.2))) +
  labs(
    title = &quot;Overtime votes for independence/unionist in Barcelona&quot;,
    caption = &quot;Data collected by @marcbeldata - Plot and analisis by @cimentadaj&quot;
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, family = &quot;Arial-BoldMT&quot;),
    plot.subtitle = element_text(size = 12, color = &quot;#666666&quot;),
    plot.caption = element_text(size = 12, color = &quot;#666666&quot;),
    legend.position = &quot;top&quot;,
    legend.text = element_text(size = 14),
    strip.text.x = element_text(size = 14),
    axis.title.y = element_text(size = 16),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 14)
  )</code></pre>
<p><img src="/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/2017-12-14-brief-analysis-of-independent-unionist-vote-in-catalonia_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Despite most municipalities are for independence, Barcelona, by only a small margin, has a majority of people voting for unionist parties. Be aware that these votes are not ‘referendums’ carried out every year. These are votes towards independence/unionist parties, which is a different thing. Also note that these are not predictions/forecasts, so they don’t have uncertainty intervals or margins of errors. This is empirical data from voter turnout.</p>
<p>I also tried other big municipalities such as Sabadell and found that unionism trumps over independence much strongly. Yet in others like Lleida, Independence seems to be on top. For a look at specific municipalities, <a href="http://marcbeldata.github.io/ggjoy-Catalan-vote-2015/">check the post by Marc Belzunces</a></p>
</div>
<div id="a-note-on-next-weeks-elections" class="section level2">
<h2>A note on next week’s elections</h2>
<p>This data takes us as far as 2015. Catalonia has suffered dramatic changes since 2015 specially due to the independence movement. These data are most likely not a good representation of what’s gonna happen next week. Big municipalities have usually been majority unionists according to polls, but the differences are tiny and we’ve seen dramatic changes with independence parties proclaiming unilateral independence. There are good attempts at predicting catalan elections (<a href="https://politica.elpais.com/politica/2017/12/07/ratio/1512647178_322229.html">in Spanish</a>) so tune in next week to see what happens.</p>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>1..2..3..check!</title>
      <link>/blog/2017-11-18-123check/1-2-3-check/</link>
      <pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-11-18-123check/1-2-3-check/</guid>
      <description><![CDATA[
      


<p>1..2..3..check! This is my first post using <a href="https://cran.r-project.org/web/packages/blogdown/index.html">blogdown</a>. I migrated my website from <code>Jekyll</code> to <code>Hugo</code> and although it took me around 2 days to tweak everything to where I wanted it, the process wasn’t so bad after all. As a celebration, I though of doing a quick analysis!</p>
<p>I live in Barcelona, a city known for sunny weather, great football and for wanting to become an independent state. In fact, just recently there was an unsuccessful attempt to break parts with the Spanish nation. Without delving too much into it, I searched for any question related to nationalism into the European Social Survey and downloaded the last available wave using the <a href="https://cran.r-project.org/web/packages/ess/index.html">ess</a> package.</p>
<pre class="r"><code>library(essurvey)
library(cimentadaj)
library(tidyverse)

spain_df &lt;- import_country(&quot;Spain&quot;, 7, &quot;your_email@gmail.com&quot;)</code></pre>
<p><code>spain_df</code> is now a data frame containing the 7th ESS round. Next we have to recode the autonomous communities which are in a ESS format. We’re interested in two variables, <code>region</code> and <code>fclcntr</code>, the second one asking whether the person feels closer to Spain.</p>
<p><strong>Note: did you notice the comunidades <code>tibble</code> there? I pasted that with no effort with <a href="https://cran.r-project.org/web/packages/datapasta/index.html">datapasta</a>! If you’re using Rstudio, just copy the table from your source and use Shift + CMD + T (on a mac) to paste it as a very nice tibble.</strong></p>
<pre class="r"><code>comunidades &lt;- tibble::tribble(
  ~ round,                      ~ country,
  &quot;ES11&quot;,                      &quot;Galicia&quot;,
  &quot;ES12&quot;,      &quot;Asturias&quot;,
  &quot;ES13&quot;,                   &quot;Cantabria&quot;,
  &quot;ES21&quot;,                  &quot;País Vasco&quot;,
  &quot;ES22&quot;,  &quot;Navarra&quot;,
  &quot;ES23&quot;,                    &quot;La Rioja&quot;,
  &quot;ES24&quot;,                      &quot;Aragón&quot;,
  &quot;ES30&quot;,         &quot;Madrid&quot;,
  &quot;ES41&quot;,             &quot;Castilla y León&quot;,
  &quot;ES42&quot;,          &quot;Castilla-La Mancha&quot;,
  &quot;ES43&quot;,                 &quot;Extremadura&quot;,
  &quot;ES51&quot;,                    &quot;Cataluña&quot;,
  &quot;ES52&quot;,        &quot;Valenciana&quot;,
  &quot;ES53&quot;,               &quot;Illes Balears&quot;,
  &quot;ES61&quot;,                   &quot;Andalucía&quot;,
  &quot;ES62&quot;,            &quot;Región de Murcia&quot;,
  &quot;ES63&quot;,    &quot;Ceuta&quot;,
  &quot;ES64&quot;,  &quot;Melilla&quot;,
  &quot;ES70&quot;,                    &quot;Canarias&quot;
)

var_recode &lt;- reverse_name(attr(spain_df$fclcntr, &quot;labels&quot;))

ready_df &lt;-
  spain_df %&gt;%
  transmute(com_aut = deframe(comunidades)[region],
         close_cnt = factor(var_recode[fclcntr],
                            levels = var_recode[1:4],
                            ordered = TRUE))</code></pre>
<p>Next up we calculate the percentage of respondents within each category and within each region and visualize it.</p>
<pre class="r"><code>perc_table &lt;-
  ready_df %&gt;%
  count(com_aut, close_cnt) %&gt;%
  group_by(com_aut) %&gt;%
  mutate(perc = (n / n())) %&gt;%
  filter(!is.na(com_aut), !is.na(close_cnt))


perc_table %&gt;%
  ggplot(aes(close_cnt, perc)) +
  geom_col() +
  facet_wrap(~ com_aut) +
  labs(
    x = &quot;How close do you feel to Spain?&quot;,
    y = &quot;Percentage&quot;
  ) +
  ggtitle(label = &quot;Closeness to Spain by autonomous communities&quot;) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90),
    plot.title = element_text(size = 16, family = &quot;Arial-BoldMT&quot;),
    plot.subtitle = element_text(size = 14, color = &quot;#666666&quot;),
    plot.caption = element_text(size = 10, color = &quot;#666666&quot;)
  ) +
  coord_flip()</code></pre>
<p><img src="/blog/2017-11-18-123check/2017-11-18-1-2-3-check_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Catalonia does seem to be the region with the highest share of respondents saying that they don’t feel close to Spain, although the vast majority does say they feel very or just close to Spain. On the other hand, Andalucia does comply with stereotypes! They certainly feel very close to the Spanish identity.</p>
<p>My <code>blogdown</code> workflow is very easy:</p>
<ol style="list-style-type: decimal">
<li><p>Create a post with my function <code>cimentadaj::my_new_post</code> which is a wrapper around <code>blogdown::new_post</code></p></li>
<li><p>Run <code>blogdown::serve_site</code> to have a realtime visual of how my blog post is being rendered</p></li>
<li><p>Write blogpost</p></li>
<li><p>Run <code>blogdown::build_site</code>. This can take long if you posts that takea long time to compile</p></li>
<li><p>Push to github (although this is more complicated because I have two branches, one for developing content and the other for pushing to the website. Maybe I’ll write a post about this once)</p></li>
</ol>
<p>Bloggin with <code>blogdown</code> was so easy that I think I’m gonna start bloggin more now…</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Scraping and visualizing How I Met Your Mother</title>
      <link>/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</guid>
      <description><![CDATA[
      


<p>How I Met Your Mother (HIMYM from here after) is a television series very similar to the classical ‘Friends’ series from the 90’s. Following the release of the <a href="http://tidytextmining.com/">tidy text</a> book I was looking for a project in which I could apply these skills. I decided I would scrape all the transcripts from HIMYM and analyze patterns between characters. This post really took me to the limit in terms of web scraping and pattern matching, which was specifically what I wanted to improve in the first place. Let’s begin!</p>
<p>My first task was whether there was any consistency in the URL’s that stored the transcripts. If you ever watched HIMYM, we know there’s around nine seasons, each one with about 22 episodes. This makes about 200 episodes give or take. It would be a big pain to manually write down 200 complicated URL’s. Luckily, there is a way of finding the 200 links without writing them down manually.</p>
<p>First, we create the links for the 9 websites that contain all episodes (1 through season 9)</p>
<pre class="r"><code>library(rvest)
library(tidyverse)
library(stringr)
library(tidytext)

main_url &lt;- &quot;http://transcripts.foreverdreaming.org&quot;
all_pages &lt;- paste0(&quot;http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=&quot;, seq(0, 200, 25))
characters &lt;- c(&quot;ted&quot;, &quot;lily&quot;, &quot;marshall&quot;, &quot;barney&quot;, &quot;robin&quot;)</code></pre>
<p>Each of the URL’s of <code>all_pages</code> contains all episodes for that season (so around 22 URL’s). I also picked the characters we’re gonna concentrate for now. From here the job is very easy. We create a function that reads each link and parses the section containing all links for that season. We can do that using <a href="http://selectorgadget.com/.">SelectorGadget</a> to find the section we’re interested in. We then search for the <code>href</code> attribute to grab all links in that attribute and finally create a tibble with each episode together with it’s link.</p>
<pre class="r"><code>episode_getter &lt;- function(link) {
  title_reference &lt;-
    link %&gt;%
    read_html() %&gt;%
    html_nodes(&quot;.topictitle&quot;) # Get the html node name with &#39;selector gadget&#39;
  
  episode_links &lt;-
    title_reference %&gt;%
    html_attr(&quot;href&quot;) %&gt;%
    gsub(&quot;^.&quot;, &quot;&quot;, .) %&gt;%
    paste0(main_url, .) %&gt;%
    setNames(title_reference %&gt;% html_text()) %&gt;%
    enframe(name = &quot;episode_name&quot;, value = &quot;link&quot;)
  
  episode_links
}

all_episodes &lt;- map_df(all_pages, episode_getter) # loop over all seasons and get all episode links
all_episodes$id &lt;- 1:nrow(all_episodes)</code></pre>
<p>There we go! Now we have a very organized <code>tibble</code>.</p>
<pre class="r"><code>all_episodes
# # A tibble: 208 x 3
#    episode_name                   link                                  id
#    &lt;chr&gt;                          &lt;chr&gt;                              &lt;int&gt;
#  1 01x01 - Pilot                  http://transcripts.foreverdreamin~     1
#  2 01x02 - Purple Giraffe         http://transcripts.foreverdreamin~     2
#  3 01x03 - Sweet Taste of Liberty http://transcripts.foreverdreamin~     3
#  4 01x04 - Return of the Shirt    http://transcripts.foreverdreamin~     4
#  5 01x05 - Okay Awesome           http://transcripts.foreverdreamin~     5
#  6 01x06 - Slutty Pumpkin         http://transcripts.foreverdreamin~     6
#  7 01x07 - Matchmaker             http://transcripts.foreverdreamin~     7
#  8 01x08 - The Duel               http://transcripts.foreverdreamin~     8
#  9 01x09 - Belly Full of Turkey   http://transcripts.foreverdreamin~     9
# 10 01x10 - The Pineapple Incident http://transcripts.foreverdreamin~    10
# # ... with 198 more rows</code></pre>
<p>The remaining part is to actually scrape the text from each episode. We can work that out for a single episode and then turn that into a function and apply for all episodes.</p>
<pre class="r"><code>episode_fun &lt;- function(file) {
  
  file %&gt;%
    read_html() %&gt;%
    html_nodes(&quot;.postbody&quot;) %&gt;%
    html_text() %&gt;%
    str_split(&quot;\n|\t&quot;) %&gt;%
    .[[1]] %&gt;%
    data_frame(text = .) %&gt;%
    filter(str_detect(text, &quot;&quot;), # Lots of empty spaces
           !str_detect(text, &quot;^\\t&quot;), # Lots of lines with \t to delete
           !str_detect(text, &quot;^\\[.*\\]$&quot;), # Text that start with brackets
           !str_detect(text, &quot;^\\(.*\\)$&quot;), # Text that starts with parenthesis
           str_detect(text, &quot;^*.:&quot;), # I want only lines with start with dialogue (:)
           !str_detect(text, &quot;^ad&quot;)) # Remove lines that start with ad (for &#39;ads&#39;, the link of google ads)
}</code></pre>
<p>The above function reads each episode, turns the html text into a data frame and organizes it clearly for text analysis. For example:</p>
<pre class="r"><code>episode_fun(all_episodes$link[15])
# # A tibble: 195 x 1
#    text                                                                   
#    &lt;chr&gt;                                                                  
#  1 Ted from 2030: Kids, something you might not know about your Uncle Mar~
#  2 &quot;Ted: You don&#39;t have to shout out \&quot;poker\&quot; when you win.&quot;             
#  3 Marshall: I know. It&#39;s just fun to say.                                
#  4 &quot;Ted from 2030: We all finally agreed Marshall should be running our g~
#  5 &quot;Marshall: It&#39;s called \&quot;Marsh-gammon.\&quot; It combines all the best feat~
#  6 Robin: Backgammon, obviously.                                          
#  7 &quot;Marshall: No. Backgammon sucks. I took the only good part of backgamm~
#  8 Lily: I&#39;m so excited Victoria&#39;s coming.                                
#  9 Robin: I&#39;m going to go get another round.                              
# 10 Ted: Okay, I want to lay down some ground rules for tonight. Barney, I~
# # ... with 185 more rows</code></pre>
<p>We now have a data frame with only dialogue for each character. We need to apply that function to each episode and <code>bind</code> everything together. We first apply the function to every episode.</p>
<pre class="r"><code>all_episodes$text &lt;- map(all_episodes$link, episode_fun)</code></pre>
<p>The <code>text</code> list-column is an organized list with text for each episode. However, manual inspection of some episodes actually denotes a small error that limits our analysis greatly. Among the main interests of this document is to study relationships and presence between characters. For that, we need each line of text to be accompanied by the character who said it. Unfortunately, some of these scripts don’t have that.</p>
<p>For example, check any episode from season <a href="http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=175">8</a> and <a href="http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=200">9</a>. The writer didn’t write the dialogue and just rewrote the lines. There’s nothing we can do so far to improve that and we’ll be excluding these episodes. This pattern is also present in random episodes like in season 4 or season 6. We can exclude chapters based on the number of lines we parsed. On average, each of these episodes has about 200 lines of dialogue. Anything significantly lower, like 30 or 50 lines, is an episode which doesn’t have a lot of dialogue.</p>
<pre class="r"><code>all_episodes$count &lt;- map_dbl(all_episodes$text, nrow)</code></pre>
<p>We can extend the previous <code>tibble</code> to be a bit more organized by separating the episode-season column into separate season and episo numbers.</p>
<pre class="r"><code>all_episodes &lt;-
  all_episodes %&gt;%
  separate(episode_name, c(&quot;season&quot;, &quot;episode&quot;), &quot;-&quot;, extra = &quot;merge&quot;) %&gt;%
  separate(season, c(&quot;season&quot;, &quot;episode_number&quot;), sep = &quot;x&quot;)</code></pre>
<p>Great! We now have a very organized <code>tibble</code> with all the information we need. Next step is to actually break down the lines into words and start looking for general patterns. We can do that by looping through all episodes that have over 100 lines (just an arbitrary threshold) and unnesting each line for each <strong>valid</strong> character.</p>
<pre class="r"><code>lines_characters &lt;-
  map(filter(all_episodes, count &gt; 100) %&gt;% pull(text), ~ { 
    # only loop over episodes that have over 100 lines
    .x %&gt;%
      separate(text, c(&quot;character&quot;, &quot;text&quot;), sep = &quot;:&quot;, extra = &#39;merge&#39;) %&gt;%
      # separate character dialogue from actual dialogo
      unnest_tokens(character, character) %&gt;%
      filter(str_detect(character, paste0(paste0(&quot;^&quot;, characters, &quot;$&quot;), collapse = &quot;|&quot;))) %&gt;%
      # only count the lines of our chosen characters
      mutate(episode_lines_id = 1:nrow(.))
  }) %&gt;%
  setNames(filter(all_episodes, count &gt; 100) %&gt;% # name according to episode
             unite(season_episode, season, episode_number, sep = &quot;x&quot;) %&gt;%
             pull(season_episode)) %&gt;%
  enframe() %&gt;%
  unnest() %&gt;%
  mutate(all_lines_id = 1:nrow(.))</code></pre>
<p>Ok, our text is sort of ready. Let’s remove some bad words.</p>
<pre class="r"><code>words_per_character &lt;-
  lines_characters %&gt;%
  unnest_tokens(word, text) %&gt;% # expand all sentences into words
  anti_join(stop_words) %&gt;% # remove bad words
  filter(!word %in% characters) %&gt;% # only select characters we&#39;re interested
  arrange(name) %&gt;%
  separate(name, c(&quot;season&quot;, &quot;episode&quot;), sep = &quot;x&quot;, remove = FALSE) %&gt;%
  mutate(name = factor(name, ordered = TRUE),
         season = factor(season, ordered = TRUE),
         episode = factor(episode, ordered = TRUE)) %&gt;%
  filter(season != &quot;07&quot;)</code></pre>
<p>Just to make sure, let’s look at the <code>tibble</code>.</p>
<pre class="r"><code>words_per_character
# # A tibble: 88,174 x 7
#    name     season episode character episode_lines_id all_lines_id word   
#    &lt;ord&gt;    &lt;ord&gt;  &lt;ord&gt;   &lt;chr&gt;                &lt;int&gt;        &lt;int&gt; &lt;chr&gt;  
#  1 &quot;01x01 &quot; 01     &quot;01 &quot;   marshall                 1            1 ring   
#  2 &quot;01x01 &quot; 01     &quot;01 &quot;   marshall                 1            1 marry  
#  3 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 perfect
#  4 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 engaged
#  5 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 pop    
#  6 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 champa~
#  7 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 drink  
#  8 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 toast  
#  9 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 kitchen
# 10 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 floor  
# # ... with 88,164 more rows</code></pre>
<p>Perfect! One row per word, per character, per episode with the id of the line of the word.</p>
<p>Alright, let’s get our hands dirty. First, let visualize the presence of each character in terms of words over time.</p>
<pre class="r"><code># Filtering position of first episode of all seasons to
# position the X axis in the next plot.
first_episodes &lt;-
  all_episodes %&gt;%
  filter(count &gt; 100, episode_number == &quot;01 &quot;) %&gt;%
  pull(id)

words_per_character %&gt;%
  split(.$name) %&gt;%
  setNames(1:length(.)) %&gt;%
  enframe(name = &quot;episode_id&quot;) %&gt;%
  unnest() %&gt;%
  count(episode_id, character) %&gt;%
  group_by(episode_id) %&gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&gt;%
  ggplot(aes(as.numeric(episode_id), perc, group = character, colour = character)) +
  geom_line() +
  geom_smooth(method = &quot;lm&quot;) +
  scale_colour_discrete(guide = FALSE) +
  scale_x_continuous(name = &quot;Seasons&quot;,
                     breaks = first_episodes, labels = paste0(&quot;S&quot;, 1:7)) +
  scale_y_continuous(name = &quot;Percentage of words per episode&quot;) +
  theme_minimal() +
  facet_wrap(~ character, ncol = 3)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-13-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Ted is clearly the character with the highest number of words per episode followed by Barney. Lily and Robin, the only two women have very low presence compared to the men. In fact, if one looks closely, Lily seemed to have decreased slightly over time, having an all time low in season 4. Marshall, Lily’s partner in the show, does have much lower presence than both Barney and Ted but he has been catching up over time.</p>
<p>We also see an interesting pattern where Barney has a lot of peaks, suggesting that in some specific episodes he gains predominance, where Ted has an overall higher level of words per episode. And when Ted has peaks, it’s usually below its trend-line.</p>
<p>Looking at the distribution:</p>
<pre class="r"><code># devtools::install_github(&quot;clauswilke/ggjoy&quot;)
library(ggjoy)

words_per_character %&gt;%
  split(.$name) %&gt;%
  setNames(1:length(.)) %&gt;%
  enframe(name = &quot;episode_id&quot;) %&gt;%
  unnest() %&gt;%
  count(season, episode_id, character) %&gt;%
  group_by(episode_id) %&gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&gt;%
  ggplot(aes(x = perc, y = character, fill = character)) +
  geom_joy(scale = 0.85) +
  scale_fill_discrete(guide = F) +
  scale_y_discrete(name = NULL, expand=c(0.01, 0)) +
  scale_x_continuous(name = &quot;Percentage of words&quot;, expand=c(0.01, 0)) +
  ggtitle(&quot;Percentage of words per season&quot;) +
  facet_wrap(~ season, ncol = 7) +
  theme_minimal()</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>we see the differences much clearer. For example, we see Barney’s peaks through out every season with Season 6 seeing a clear peak of 40%. On the other hand, we see that their distributions don’t change that much over time! Suggesting that the presence of each character is very similar in all seasons. Don’t get me wrong, there are differences like Lily in Season 2 and then in Season 6, but in overall terms the previous plot suggests no increase over seasons, and this plot suggests that between seasons, there’s not a lot of change in their distributions that affects the overall mean.</p>
<p>If you’ve watched the TV series, you’ll remember Barney always repeating one similar trademark word: legendary! Although it is a bit cumbersome for us to count the number of occurrences of that sentence once we unnested each sentence, we can at least count the number of words per character and see whether some characters have particular words.</p>
<pre class="r"><code>count_words &lt;-
  words_per_character %&gt;%
  filter(!word %in% characters) %&gt;%
  count(character, word, sort = TRUE)

count_words %&gt;%
  group_by(character) %&gt;%
  top_n(20) %&gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &quot;free_y&quot;)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Here we see that a lot of the words we capture are actually nouns or expressions which are common to everyone, such as ‘yeah’, ‘hey’ or ‘time’. We can weight down commonly used words for other words which are important but don’t get repeated a lot. We can exclude those words using <code>bind_tf_idf()</code>, which for each character decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection or corpus of documents (see 3.3 in <a href="http://tidytextmining.com/tfidf.html" class="uri">http://tidytextmining.com/tfidf.html</a>).</p>
<pre class="r"><code>count_words %&gt;%
  bind_tf_idf(word, character, n) %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  group_by(character) %&gt;%
  top_n(20) %&gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &quot;free_y&quot;)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Now Barney has a very distinctive word usage, one particularly sexist with words such as couger, bang and tits. Also, we see the word legendary as the thirdly repeated word, something we were expecting! On the other hand, we see Ted with things like professor (him), aunt (because of aunt Lily and such).</p>
<p>Knowing that Ted is the main character in the series is no surprise. To finish off, we’re interested in knowing which characters are related to each other. First, let’s turn the data frame into a suitable format.</p>
<p>Here we turn all lines to lower case and check which characters are present in the text of each dialogue. The loop will return a vector of logicals whether there was a mention of any of the characters. For simplicity I exclude all lines where there is more than 1 mention of a character, that is, 2 or more characters.</p>
<pre class="r"><code>lines_characters &lt;-
  lines_characters %&gt;%
  mutate(text = str_to_lower(text))

rows_fil &lt;-
  map(characters, ~ str_detect(lines_characters$text, .x)) %&gt;%
  reduce(`+`) %&gt;%
  ifelse(. &gt;= 2, 0, .) # excluding sentences which have 2 or more mentions for now
  # ideally we would want to choose to count the number of mentions
  # per line or randomly choose another a person that was mentioned.</code></pre>
<p>Now that we have the rows that have a mention of another character, we subset only those rows. Then we want know which character was mentioned in which line. I loop through each line and test which character is present in that specific dialogue line. The loop returns the actual character name for each dialogue. Because we already filtered lines that <strong>have</strong> a character name mentioned, the loop should return a vector of the same length.</p>
<pre class="r"><code>character_relation &lt;-
  lines_characters %&gt;%
  filter(as.logical(rows_fil)) %&gt;%
  mutate(who_said_what =
           map_chr(.$text, ~ { # loop over all each line
             who_said_what &lt;- map_lgl(characters, function(.y) str_detect(.x, .y))
             # loop over each character and check whether he/she was mentioned
             # in that line
             characters[who_said_what]
             # subset the character that matched
           }))
</code></pre>
<p>Finally, we plot the relationship using the <code>ggraph</code> package.</p>
<pre class="r"><code>library(ggraph)
library(igraph)

character_relation %&gt;%
  count(character, who_said_what) %&gt;%
  graph_from_data_frame() %&gt;%
  ggraph(layout = &quot;linear&quot;, circular = TRUE) +
  geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                width = 2.5, show.legend = FALSE) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>A very clear pattern emerges. There is a strong relationship between Robin and Barney towards Ted. In fact, their direct relationship is very weak, but both are very well connected to Ted. On the other hand, Marshall and Lily are also reasonably connected to Ted but with a weaker link. Both of them are indeed very connected, as should be expected since they were a couple in the TV series.</p>
<p>We also see that the weakest members of the group are Robin and Barney with only strong bonds toward Ted but no strong relationship with the other from the group. Overall, there seems to be a division: Marshall and Lily hold a somewhat close relationship with each other and towards Ted and Barney and Robin tend to be related to Ted but no one else.</p>
<p>As a follow-up question, is this pattern of relationships the same across all seasons? We can do that very quickly by filtering each season using the previous plot.</p>
<pre class="r"><code>library(cowplot)

# Loop through each season
seasons &lt;- paste0(0, 1:7)

all_season_plots &lt;- lapply(seasons, function(season_num) {

  set.seed(2131)
  
  character_relation %&gt;%
    # Extract the season number from the `name` column
    mutate(season = str_replace_all(character_relation$name, &quot;x(.*)$&quot;, &quot;&quot;)) %&gt;%
    filter(season == season_num) %&gt;%
    count(character, who_said_what) %&gt;%
    graph_from_data_frame() %&gt;%
    ggraph(layout = &quot;linear&quot;, circular = TRUE) +
    geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                  width = 2.5, show.legend = FALSE) +
    geom_node_text(aes(label = name), repel = TRUE) +
    theme_void()
})

# Plot all graphs side-by-side
cowplot::plot_grid(plotlist = all_season_plots, labels = seasons)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-20-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>There are reasonable changes for all non-Ted relationship! For example, for season 2 the relationship Marshall-Lily-Ted becomes much stronger and it disappears in season 3. Let’s remember that these results might be affected by the fact that I excluded some episodes because of low number of dialogue lines. Keeping that in mind, we also see that for season 7 the Robin-Barney relationship became much stronger (is this the season the started dating?). All in all, the relationships don’t look dramatically different from the previous plot. Everyone seems to be strongly related to Ted. The main difference is the changes in relationship between the other members of the cast.</p>
<p>This dataset has a lot of potential and I’m sure I’ve scratched the surface of what one can do with this data. I encourage anyone interested in the topic to use the code to analyze the data further. One idea I might explore in the future is to build a model that attempts to predict who said what for all dialogue lines that didn’t have a character member. This can be done by extracting features from all sentences and using these patterns try to classify which. Any feedback is welcome, so feel free to message me at <a href="mailto:cimentadaj@gmail.com">cimentadaj@gmail.com</a></p>
]]>
      </description>
    </item>
    
    <item>
      <title>Children: public or private goods?</title>
      <link>/blog/2016-08-29-children-public-or-private-goods/children-public-or-private-goods/</link>
      <pubDate>Mon, 29 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-08-29-children-public-or-private-goods/children-public-or-private-goods/</guid>
      <description><![CDATA[
      


<p>The role of the welfare state, as agreed by many scholars, is to de-commodify its citizens. Some argue that this should be the way that the generosity of the state should be measured; instead of net public and private social expenditure, it should be the extent to which citizens are de-commodified by the services and transfers of the welfare state.</p>
<p>The aim of educational investment can be seen from different lenses, but today we’ll focus on the economic and social returns to education. An important share of the state budget is aimed at the formation and education of citizens through public education. In countries where there is a substantial investment, human capital, innovation and production are maximized creating a highly productive working population that can sustain the young and the elderly through pensions, health and child care; note that in some scenarios an uneducated workforce can sustain those who depend on them but it will possible be with more quantity of work (hours) and less quality [1]. Following this logic, it is arguable that because most of today’s workers will benefit from pensions, retirement schemes, care assistance and healthcare, they are to take care of their young population, the next generation’s workers. Covering kids’ education through higher taxes could be seen as loan: A young worker covers the expenses of a child, and that child, eventually, will cover the pension of that worker; this is a simplified version of the argument.</p>
<p>The scheme I just outlined is virtually widespread across western European countries. But it is important to note that in other countries, due to the intrinsic organization of the state, it is unimaginable to do so. In the counterfactual situation where you don’t have a strong welfare state, maybe it’s unfair to tax those who are not benefiting from the goods that the taxes are being spent on. In places like the United States, where the generosity of the welfare system is minimal, some people don’t find it reasonable to pay for everyone’s kids, even more, when you’re paying already, for instance, a private school for your own.</p>
<p>There’s also another important case: childless couples. Perhaps the exact reason why they didn’t have children was not to have to spend a big proportion of their lifelong income. So, why should they pay? According to their position it seems very unfair. Their long held argument can be synthesized in one sentence: those who want children should be the ones who pay for them.</p>
<p>After presenting very briefly the arguments of the positions in the debate, it is interesting then to posit the important question concerning it: Should children be a public or private good? I presume this question is easy to answer but not easy to apply; adjusting a state to this premise can take decades and it’s an expensive endeavor. So, evaluating this question must be accompanied by hard scientific thinking.</p>
<p>I will start off by stating my position on this dilemma. I support the notion of kids being a public good. I support it not just for the fact that they will pay or not for my pension schemes but because everyone deserves the same opportunities. And if we don’t provide them with equal opportunities, then the gap we see between poor and rich will be as it is today: very sizable. Despite that I sometimes question myself: can they entirely be a public good? Adopting the notion that the family is a private sphere, and at least the states considers it because it is reluctant to interfere within it, then children should not just be considered as a public good. If we pay attention to authors like McDonald (2000) and Lewis &amp; Astrom (1992), we constantly see that the way the state promotes gender equality in the family is not through hands-on, direct implementation of intrusive policies like reeducating couples and telling them what’s right and what’s not. Instead, most of the family policies try to promote gender equality by integrating women into the labor force, reducing the gender wage gap and promoting female and male parental leave. Kids then can never be entirely public goods.</p>
<p>Nevertheless, do they contribute to society, meaning, are the interrelated with the “public”?</p>
<p>I think this question is rooted in the intrinsic dynamics of society. We are social animals and it is unfashionable to think that we are independent of each other. The most independent citizen living within society is never unattached of its surroundings. Everyone’s paths cross indirectly and directly with the decisions and actions of other people. It might be even irresponsible to think that one’s actions only affect themselves. The principal idea to take from this question is that those who think of themselves as independent actors of society are committing a fallacy that could be the exact reason why the levels and indexes of inequalities are so high nowadays.</p>
<p>The Nobel laureate James Heckman has been arguing for the past decade one of his most controversial findings: the first years of a child are the most important ones. Contrary to the long held thought of concentrating the bigger part of the educational budget on tertiary education, he finds that the return of investment of human beings are the highest in the early years and it decreases with time. This means that the first years should be the ones where the highest investment should be: be it monetary and emotional. With the evaluation of the Perry preschool program and the Abecedarian project, Heckman finds that you can trace inequalities between children as early as 22 months old. The principal reason being that some kids get better early childhood education than others. Among others, I think this an important reason to reflect why children should be a public good.</p>
<p>Inequalities are being reproduced from an early age, leading to a more polarized society where you have lucky and unlucky kids. Those that receive the better education will at the end, have better jobs, less odds of committing crime, more prone of a stable union (possibly due to the fact that they will have a stable job). Furthermore, and I think this is a key argument against those that don’t want to contribute, these educated children will in most of the cases, help society better than the ones who aren’t educated. This help can be manifested in direct and indirect ways: respecting traffic lights, avoiding corruption in public institutions, being fair, meritocratic, reporting robberies, not stealing, respecting others, among an endless number of thing. This point makes an important argument: the focus on which some of the parents and scholars are viewing the debate might be wrongly specified. As I have succinctly showed, the benefits of having an educated population are not only personal, but also public. I think that the principal reason why there is a strong opposition to this idea is because they evaluate it as taking care of someone else’s children when nobody is taking care of theirs; that is a strong point, but I urge everyone to see it with another lense. If you turned the coin around and you start seeing this as an investment towards societal problems, which in the short and long run affects us all, then it becomes a tax deduction, just as any other one.</p>
<p>As I argued before, we are all interrelated and it is our business what our fellow citizens do. So, someone who doesn’t have children, still participates in society, interacts within it, and is affected by criminality, public services, and private services. The problem with providing this argument for a country with a weak welfare state is that you won’t receive quality elderly care from this children when you retire. Conversely, in strong-welfare countries, the argument is the main (pragmatic) reason to do so. However, the second idea, that we should do it for everyone to have the same opportunities, is more than enough to warrant the changes. This would be an even more noble reason because you won’t be contributing to receive something but because everyone deserves the same opportunities. For the U.S, this is a reason to tackle one of the most debatable topics of the last 40 years: inequalities.</p>
<p>Now, if we were to delve more into the inequality problem, which shows itself to be the major issue when comparing educational opportunities, it is also important to think of the struggles of having children. Not particularly of the difficulties of childrearing, which are a consequence of the decision to child bear, but the fact that it is too hard to combine work and childrearing now a days. This is quite notorious in the lean welfare state countries and southern European ones.</p>
<p>The demands of the work space and the schedules that are imposed to parents and workers make it virtually impossible for men or women to dedicated high quality time to their children. The most common solution that we’ve seen is for women to quit their jobs or to sacrifice leisure time to take care of the child. So, in the end, this deems either the education of the future population or the ambitions and life course perspective of a mother.</p>
<p>Catherine Hakim’s preference theory, posits that women now have the preference of having a child but this was not entirely the case in the past where the bargaining power of the man was higher than hers. But a woman who has professional ambitions might also want to have a baby. Does this mean that she has to renounce her ambitions? This will certainly harm the tax system (less taxes paid) but it will help society because she will dedicate more time to the child (And if you acknowledge this, then you are implicitly arguing that children are indeed a public good). But as one of Esping-Andersen’s chapter from his book the Incomplete Revolution shows, permitting the woman to go back to work after parental leave, by providing free high quality childcare, will in the long run pay the costs of this childcare service and even produce a surplus for the welfare state. And that is why I think this is not an easy question to answer, conversely to what Paula England and Nancy Folbre imply. The implication of implementing this idea entails a complete reorganization and readjustment of the tax and welfare system, if there is one [2]. And also, what would be the consequences of this new regime besides the public childcare? Will the problem of inequalities be solved or others problems will arise as side effects?</p>
<p>And here I will talk about something that is intrinsically related to having healthy children: the labor perspectives of the mother. The Incomplete Revolution, book by Esping-Andersen, takes on the job of explaining the problem with many of today’s countries: the poor family policies that they implement (if any) when concerning women’s labor market inclusion and combination of working and childrearing. Specifically in two chapters he argues that children should be a public good because children benefit everyone, but furthermore, an important step is to include women in the labor market given that it will help the income prospects of the household and the egalitarianism in the family. How can this be related to children as being a public good, one may ask? This is a complex relation, on which everything is potentially endogenous, but all present evidence shows that if the mother goes back to work after the parental leave, it does not have an adverse effect on the development of the child. This is, assuming that the child is in early high quality childcare. This has important implications! It means that the inclusion of mothers into the labor force will increase the bargaining power of the women in the family, it will heighten the income of the family household, it will expand the human capital of the women; in short, it will not impose the opportunity costs that she will otherwise would’ve had to bear for the remaining part of her life if she would’ve stayed home. This will increase the chances of being in a more egalitarian family, thus the children will be exposed to more equal surroundings.</p>
<p>From the point of view that I see it, it’s about heightening the bar of educational attainment. Instead of having the two extremes that we see in the liberal welfare state regimes (excluding UK), extremely poor and uneducated groups and highly rich and educated group, you will set a bar of minimum education. By doing this you will inevitable give secureness to families, opportunities, possibly change the economic structure of the country by increasing the odds of turning it to a knowledge based economy (although this is way more complex then I suggest).</p>
<p>As a conclusion, I shall finish off with another question, which I think is as important as the one which initiated this essay. I have tried to argue that children are faring under the destiny that is imposed to them. There has been, since after the World War II, persistent inequalities between classes and all the evidence shows that they are not shrinking. In some countries, namely European ones, the decision to help children might be an economic one or an altruistic one. This is worrisome. Are children being taken care of in Europe because they are the payers of the pensioners or because of the noble argument of equal opportunities? I really don’t know the reason for which Europe or the U.S is doing it for. But should everyone take care of children for the economic benefits or for the equal opportunity argument?</p>
<hr />
<p>[1] Let’s not get into the major changes that the quality of job production will have if the population is highly educated. For a book with interesting ideas, read The Race between Education and Technology by Claudia Goldin &amp; Lawrence Katz.</p>
<p>[2] In the case where there is no welfare state, then this is another story.</p>
]]>
      </description>
    </item>
    
  </channel>
</rss>
