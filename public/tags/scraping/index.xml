<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>scraping on Jorge Cimentada</title>
    <link>/tags/scraping/</link>
    <description>Recent content in scraping on Jorge Cimentada</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Tue, 19 Jun 2018 00:00:00 +0000</lastBuildDate>
    
    <atom:link href="/tags/scraping/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Exploring Google Scholar coauthorship</title>
      <link>/blog/2018-06-19-exploring-google-scholar-coauthorship/exploring-google-scholar-coauthorship/</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-06-19-exploring-google-scholar-coauthorship/exploring-google-scholar-coauthorship/</guid>
      <description><![CDATA[
      


<p>I woke up today to read Maëlle Salmon’s latest blog entry in which she scraped her own <a href="https://masalmon.eu/2018/06/18/mathtree/">mathematical tree</a>. Running through the code I had an idea about scraping the coauthorship list that a Google Scholar profile has. With this, I could visualize the network of coauthorship of important scientists and explore whether they have closed or open collaborations.</p>
<p>I sat down this morning and created the <code>coauthornetwork</code> package that allows you to do just that! It’s actually very simple. First, install it with the usual:</p>
<pre class="r"><code>devtools::install_github(&quot;cimentadaj/coauthornetwork&quot;)</code></pre>
<p>There’s two functions: <code>grab_network</code> and <code>plot_coauthors</code>. The first scrapes and returns a data frame of a Google Scholar profile, their coauthors and the coauthors of their coauthors (what?). More simply, by default, the data frame returns this:</p>
<p>Google Scholar Profile –&gt; Coauthors –&gt; Coauthors</p>
<p>It’s not that hard after all. The only thing you need to provide is the end of the URL of a Google Scholar profile. For example, a typical URL looks like this: <code>https://scholar.google.com/citations?user=F0kCgy8AAAAJ&amp;hl=en</code>. <code>grab_network</code> will accept the latter part of the URL, namely: <code>citations?user=F0kCgy8AAAAJ&amp;hl=en</code>. Let’s test it:</p>
<pre class="r"><code>library(coauthornetwork)

network &lt;- grab_network(&quot;citations?user=F0kCgy8AAAAJ&amp;hl=en&quot;)
network</code></pre>
<pre><code>## # A tibble: 21 x 4
##    author       href                 coauthors     coauthors_href          
##    &lt;fct&gt;        &lt;fct&gt;                &lt;fct&gt;         &lt;fct&gt;                   
##  1 Hans-Peter ~ citations?user=F0kC~ Melinda Mills /citations?user=HX9KQ5M~
##  2 Hans-Peter ~ citations?user=F0kC~ Karl Ulrich ~ /citations?user=iuzu9xw~
##  3 Hans-Peter ~ citations?user=F0kC~ Florian Schu~ /citations?user=MWCt6hQ~
##  4 Hans-Peter ~ citations?user=F0kC~ Yossi Shavit  /citations?user=brfWXKM~
##  5 Hans-Peter ~ citations?user=F0kC~ Jan Skopek    /citations?user=Mmo1hFk~
##  6 Melinda Mil~ /citations?user=HX9~ Hans-Peter B~ /citations?user=F0kCgy8~
##  7 Melinda Mil~ /citations?user=HX9~ Tanturri Mar~ /citations?user=xN3XevQ~
##  8 Melinda Mil~ /citations?user=HX9~ René Veenstra /citations?user=_9OVrqM~
##  9 Melinda Mil~ /citations?user=HX9~ Francesco C.~ /citations?user=-JR6yo4~
## 10 Karl Ulrich~ /citations?user=iuz~ Paul B. Balt~ /citations?user=vcOZeDg~
## # ... with 11 more rows</code></pre>
<p>The main author here is Hans-Peter Blossfeld, a well known Sociologist. We also see that Melinda Mills is one of his coauthors, so we also have the coauthors of Melinda Mills right after him. <code>grab_networks</code> also has the <code>n_coauthors</code> argument to control how many coauthors you can extract (limited to 20 by Google Scholar). You’ll notice that once you go over 10 coauthors things start to get very messy when we visualize this.</p>
<pre class="r"><code>plot_coauthors(network, size_labels = 3)</code></pre>
<p><img src="/blog/2018-06-19-exploring-google-scholar-coauthorship/2018-06-19-exploring-google-scholar-coauthorship_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Cool eh? We can play around with more coauthors as well.</p>
<pre class="r"><code>plot_coauthors(grab_network(&quot;citations?user=F0kCgy8AAAAJ&amp;hl=en&quot;, n_coauthors = 7), size_labels = 3)</code></pre>
<p><img src="/blog/2018-06-19-exploring-google-scholar-coauthorship/2018-06-19-exploring-google-scholar-coauthorship_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Hope you enjoy it!</p>
<!-- To make it more accesible to non-R users, I [created a Shiny app](https://cimentadaj.shinyapps.io/gs_coauthorsip/) where everyone can explore their own coauthors. Enjoy! -->
]]>
      </description>
    </item>
    
    <item>
      <title>Login in, scraping and hidden fields</title>
      <link>/blog/2018-04-05-login-in-scraping-and-hidden-fields/login-in-scraping-and-hidden-fields/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-04-05-login-in-scraping-and-hidden-fields/login-in-scraping-and-hidden-fields/</guid>
      <description><![CDATA[
      


<p>Lightning post. Earlier today I was trying to scrape the emails from all the PhD candidates in my program and I had to log in from our ‘Aula Global’. I did so using <code>httr</code> but something was off: I introduced both my username and password but the website did not log in. Apparently, when loging in through <code>POST</code>, sometimes there’s a thing call hidden fields that you need to fill out! I would’ve never though about this. Below is a case study, that excludes my credentials.</p>
<p>The first thing we have to do is identify the <code>POST</code> method and the inputs to the request. Using Google Chrome, go to the website <a href="https://sso.upf.edu/CAS/index.php/login?service=https%3A%2F%2Faulaglobal.upf.edu%2Flogin%2Findex.php">https://sso.upf.edu/CAS/index.php/login?service=https%3A%2F%2Faulaglobal.upf.edu%2Flogin%2Findex.php</a> and then on the Google Chrome menu go to -&gt; Settings -&gt; More tools -&gt; Developer tools. Here we have the complete html of the website.</p>
<ol style="list-style-type: decimal">
<li>We identify the POST method and the URL</li>
</ol>
<!-- <img src="/img/post_method.png" alt="Drawing" style="width: 600px;"/> -->
<div class="figure">
<img src="/img/post_method.png" />

</div>
<p>It’s the branch with <code>form</code> that has <code>method='post'</code>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Open the <code>POST</code> branch and find all fields. We can see the two ‘hidden’ fields.</li>
</ol>
<div class="figure">
<img src="/img/hidden_fields.png" />

</div>
<p>Below the <code>form</code> tag, we see two <code>input</code> tags set to hidden, there they are! Even though we want to login, we also have to provide the two hidden fields. Take note of both their <code>name</code> and <code>value</code> tags.</p>
<ol start="3" style="list-style-type: decimal">
<li>Dive deeper down the branch and find other fields. In our case, username and password.</li>
</ol>
<p>For username:</p>
<div class="figure">
<img src="/img/username.png" />

</div>
<p>For password:</p>
<div class="figure">
<img src="/img/password.png" />

</div>
<ol start="4" style="list-style-type: decimal">
<li>Write down the field names with the correspoding values.</li>
</ol>
<pre class="r"><code>all_fields &lt;-
  list(
    adAS_username = &quot;private&quot;,
    adAS_password = &quot;private&quot;,
    adAS_i18n_theme = &#39;en&#39;,
    adAS_mode = &#39;authn&#39;
  )</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Load our packages and our URL’s</li>
</ol>
<pre class="r"><code>library(tidyverse)
library(httr)
library(xml2)

login &lt;- &quot;https://sso.upf.edu/CAS/index.php/login?service=https%3A%2F%2Faulaglobal.upf.edu%2Flogin%2Findex.php&quot;
website &lt;- &quot;https://aulaglobal.upf.edu/user/index.php?page=0&amp;perpage=5000&amp;mode=1&amp;accesssince=0&amp;search&amp;roleid=5&amp;contextid=185837&amp;id=9829&quot;</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Login using all of our fields.</li>
</ol>
<pre class="r"><code>upf &lt;- handle(&quot;https://aulaglobal.upf.edu&quot;)

access &lt;- POST(login,
               body = all_fields,
               handle = upf)</code></pre>
<p>Note how I set the <code>handle</code>. If the website you want to visit and the website that hosts the login information have the same root of the URL (<code>aulaglobal.upf.edu</code> for example), then you can avoid using <code>handle</code> (it’s done behind the scenes). In my case, I set the <code>handle</code> to the same root URL of the website I WANT to visit after I log in (because they have different root URL’s). This way the cookies and login information from the login are preserved through out the session.</p>
<ol start="4" style="list-style-type: decimal">
<li>Request the information from the website you’re interested</li>
</ol>
<pre class="r"><code>emails &lt;- GET(website, handle = upf)</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Scrape away!</li>
</ol>
<pre class="r"><code>all_emails &lt;-
  read_html(emails) %&gt;% 
  xml_ns_strip() %&gt;% 
  xml_find_all(&quot;//table//a&quot;) %&gt;% 
  as_list() %&gt;% 
  unlist() %&gt;% 
  str_subset(&quot;.+@upf.edu$&quot;)</code></pre>
<p>Unfortunately you won’t be able to reproduce this script because you don’t have a log information unless you belong to the same PhD program as I do. However, I hope you find the hidden fields explanation useful, I’m sure I will come back to this in the near future for reference!</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Scraping at scale: daily scraping to your database</title>
      <link>/blog/2018-01-31-scraping-at-scale-daily-scraping-to-your-database/scraping-at-scale-daily-scraping-to-your-database/</link>
      <pubDate>Wed, 31 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-01-31-scraping-at-scale-daily-scraping-to-your-database/scraping-at-scale-daily-scraping-to-your-database/</guid>
      <description><![CDATA[
      


<p>I’ve been working on a personal project to gather daily data from public bicycles in Barcelona to create a historical timeline of a few stations. Since the data is only available live, I had to scrape the data and store in a database daily. This is a short tutorial showing the steps I had to take to setup a database on my remote server and connect both from my local computer as well as from my server. I also show the R script that scrapes data, connects to the server and appends the information every day for a certain amouint of time.</p>
<p><em>Note: This worked for my Digital Ocean droplet 512 MB and 20 GB disk with Ubuntu 16.04.3 x64.</em></p>
<p>Let’s get to it. It’s better to do <em>ALL</em> of this as a user in your server but remember to append <code>sudo</code> to everything. Nonetheless, beware of problems like the ones I encountered. For example, when installing R packages that where ran by <code>cron</code> in a script, if installed through a non-root user the packages were said to be <code>'not installed'</code> (when I fact running the script separately was fine). However, when I installed the packages logged in as root the packages were installed successfully.</p>
<div id="setting-up-the-data-base" class="section level2">
<h2>Setting up the data base</h2>
<p>All steps:</p>
<ul>
<li><p><a href="https://www.digitalocean.com/community/tutorials/how-to-install-r-on-ubuntu-16-04-2">Install R</a></p></li>
<li><p><a href="https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-16-04">Install MySQL</a></p></li>
<li><p>Type <code>mysql -u root -p</code> to log in to MySQL</p></li>
<li><p>Follow these steps to create an empty table within a database</p></li>
</ul>
<pre class="sql"><code>CREATE DATABASE bicing;
USE bicing;
CREATE TABLE bicing_station (id VARCHAR(30), slots VARCHAR(30), bikes VARCHAR(30), status VARCHAR(30), time VARCHAR(30), error VARCHAR(30));</code></pre>
<ul>
<li><p><a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-remote-database-to-optimize-site-performance-with-mysql">This</a> is an outdated guide by Digital Ocean which might be helpful. Some of the steps below are taken from that guide.</p></li>
<li><p>Alter <code>sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf</code> and change <code>bind-address</code> to have the ‘0.0.0.0’ This is so your server can listen to IP’s from outside the localhost network.</p></li>
<li><p>Create two users to access the data base: a user from your local computer and a user from your server.</p></li>
</ul>
<pre class="bash"><code>mysql -u root -p # Log in to MySQL. -u stands for user and -p for password</code></pre>
<pre class="sql"><code>/* Create user for local computer. Note that when username and ip are in &#39;&#39; they need to be in those quotes. Also, the ip address you can find easily by writing what&#39;s my ip in Google*/

CREATE USER &#39;username&#39;@&#39;ip_address_of_your_computer&#39; IDENTIFIED BY &#39;password&#39;;
GRANT ALL ON bicing.* TO username@ip_address_of_your_computer;

/* Create user for server. For this user don&#39;t change localhost as that already specifies that it belongs to the same computer. */

CREATE USER &#39;username&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;password&#39;;
GRANT ALL ON bicing.* TO username@localhost;

/* Make sure the privileges are isntalled */
FLUSH PRIVILEGES;

quit /* To quit MySQL*/</code></pre>
<ul>
<li>Test whether the access worked for both users</li>
</ul>
<pre class="bash"><code># Login from your server. Replace username for your username 
# -u stands for user and -p will ask for your password 
mysql -u username -h localhost -p


# Login from your LOCAL computer. Replace username for your username and your_server_ip from the server&#39;s IP
mysql -u username -h your_server_ip -p</code></pre>
<ul>
<li>Now install <code>odbc</code> in your Ubuntu server. I follow <a href="I%20followed%20this:%20https://askubuntu.com/questions/800216/installing-ubuntu-16-04-lts-how-to-install-odbc">this</a></li>
</ul>
<pre class="bash"><code>sudo mkdir mysql &amp;&amp; cd mysql

# Download odbc in mysql folder
sudo wget https://dev.mysql.com/get/Downloads/Connector-ODBC/5.3/mysql-connector-odbc-5.3.9-linux-ubuntu16.04-x86-64bit.tar.gz

# Unzip it and copy it somewhere.
sudo tar -xvf mysql-connector-odbc-5.3.9-linux-ubuntu16.04-x86-64bit.tar.gz 
sudo cp mysql/mysql-connector-odbc-5.3.9-linux-ubuntu16.04-x86-64bit/lib/libmyodbc5a.so /usr/lib/x86_64-linux-gnu/odbc/
# If the odbc folder doesn&#39;t exists, create it with mkdir /usr/lib/x86_64-linux-gnu/odbc/</code></pre>
<p>Note: you might need to change the url’s and directories to a <strong>newer</strong> version of <code>odbc</code> so don’t simply copy and paste the links from below.</p>
<ul>
<li>Create and update the <code>odbc</code> settings.</li>
</ul>
<pre class="bash"><code>sudo touch /etc/odbcinst.ini

sudo nano /etc/odbcinst.ini

# And add

[MySQL Driver]
Description = MySQL
Driver = /usr/lib/x86_64-linux-gnu/odbc/libmyodbc5a.so
Setup = /usr/lib/x86_64-linux-gnu/odbc/libodbcmyS.so
FileUsage = 1

# close the nano
# And continue

sudo touch /etc/odbc.ini

sudo nano /etc/odbc.ini

# and add

[MySQL]
Description           = MySQL connection to database
Driver                = MySQL Driver
Database              = dbname
Server                = 127.0.0.1
User                  = root
Password              = password
Port                  = 3306
Socket                = /var/run/mysqld/mysqld.sock

# Change Database to your database name
# The password to your root password

# Finally, run

sudo ln -s /var/run/mysqld/mysqld.sock /tmp/mysql.sock

# to move the socket to the folder where the DBI pkgs
# search for it

# Finish by

sudo service mysql restart;

# to restart mysql server</code></pre>
</div>
<div id="connecting-to-the-database-locally-and-remotely" class="section level2">
<h2>Connecting to the database locally and remotely</h2>
<p>From my local computer:</p>
<pre class="r"><code>library(DBI)
library(RMySQL)

con &lt;- dbConnect(MySQL(), # If the database changed, change this
                 host = your_server_ip, # in &quot;&quot; quotes.
                 dbname = &quot;bicing&quot;,
                 user = username, # remember to change to your username (in quotes)
                 password = password, # remember to change to your password (in quotes)
                 port = 3306)

dbListTables(con)

bike_stations &lt;- dbReadTable(con, &quot;bicing_station&quot;)</code></pre>
<p>From R in the server</p>
<pre class="r"><code>con &lt;- dbConnect(RMySQL::MySQL(),
                 dbname = &quot;bicing&quot;,
                 user = username, # remember to change to your username (in quotes)
                 password = password, # remember to change to your password (in quotes)
                 port = 3306)

dbListTables(con)

bike_stations &lt;- dbReadTable(con, &quot;bicing_station&quot;)</code></pre>
<p>That did it for me. Now I could connect to the database from R from my local computer and from the server itself.</p>
</div>
<div id="scraping-automatically" class="section level2">
<h2>Scraping automatically</h2>
<p>So far you should have a database in your server which you can connect locally and remotely. I assume you have a working script that can actually add/retrieve information from the remote database. Here I will explain how to set up the scraping to run automatically as a <code>cron</code> job and get a final email with the summary of the scrape.</p>
<ul>
<li><p>Create a text file to save the output of the scraping with <code>sudo touch scrape_log.txt</code></p></li>
<li><p>Write <code>cron -e</code> logged in as your non-root user.</p></li>
<li><p>At the bottom of the interactive <code>cron</code> specify these options:</p></li>
</ul>
<pre class="bash"><code>SHELL=/bin/bash # the path to the predetermined program to run cron jobs. Default bash

PATH=bla/bla/bla # PATH I’m not sure what’s for but I pasted the output of echo $PATH.

HOME= your/dr/ofinteres # Path to the directory where the scripts will be executed (where the script is)

MAILTO=&quot;your@email.com&quot; # Your email to receive emails

# The actual cron jobs. Below each job I explain them
30-59 11 * * * /usr/bin/Rscript scrape_bicing.R &gt;&gt;scrape_log.txt 2&gt;&amp;1

# Run this cron job from 11:30 to 11:59 every day (*), every month (*), every year(*): 30-59 11 * * *

# Use R to run the script: /usr/bin/Rscript
# You can find this directory with which Rscript

# Execute the file scrape_bicing.R (which is looked for in the HOME variable specified above)
# &gt;&gt;scrape_log.txt 2&gt;&amp;1: Save the output to scrape_log.txt (which we created) and DON&#39;T send an email
# because we don&#39;t want to received 29 emails.

00 12 * * * /bin/bash sql_query.sh
# Instead of receiving 29 emails, run a query the minute after the scraping ends
# to filter how many rows were added between 11:30 and 11:59
# By default it will send the result of the query to your email</code></pre>
<p>Great but what does <code>scrape_bicing.R</code> have?</p>
<p>The script should do something along the lines of:</p>
<pre class="r"><code># Load libraries
library(httr)
library(DBI)
library(RMySQL)

# The url of your api
api_url &lt;- &quot;bla/bla&quot;

# Wrap GET so that whenever the request fails it returns an R error
my_GET &lt;- function(x, config = list(), ...) {
  stop_for_status(GET(url = x, config = config, ...))
}

# If it can&#39;t connect to the API will throw an R error
test_bike &lt;- my_GET(api_url)


## Do API calls here
## I assume the result is a data.frame or something like that
## It should have the same column names as the SQL database.

# Establish the connection to the database.
# This script is run within the server, so the connection
# should not specify the server ip, it assumes it&#39;s
# the localhost

con &lt;- dbConnect(MySQL(),
                 dbname = database_name, # in &quot;&quot; quotes
                 user = your_user, # in &quot;&quot; quotes
                 password = your_password, # in &quot;&quot; quotes
                 port = 3306)

# Append the table
write_success &lt;-
  dbWriteTable(conn = con, # connection from above
              &quot;table name&quot;, # name of the table to append (in quotes)
              api output, # data frame from the API output
              append = TRUE, row.names = FALSE) # to append instead of overwrite and ignore row.names

# Write your results to the database. In my API call
# I considered many possible errors and coded the request
# very defensively, running the script many times under certain
# scenarios (no internet, getting different results).
# If you get unexpected results from your API request then this step will
# not succeed.


# If the append was successfull, write_success should be TRUE
if (write_success) print(&quot;Append success&quot;) else print(&quot;No success&quot;)</code></pre>
<p>Something to keep in mind, by default you can connect from the your local computer to the remote DB by port 3306. This port can be closed if you’re in a public internet network or a network connection from a university. If you can’t connect, make you sort this out with the personnel from that network (it happened to me with my university network).</p>
<p>What does <code>sql_query.sh</code> have?</p>
<p>A very simple SQL query:</p>
<pre class="sql"><code>read PASS &lt; pw.txt /* Read the password from a pw.txt file you create with your user pasword*/

mysql -uroot -p$PASS database_name -e &quot;SELECT id, error_msg, COUNT(*) AS count FROM bicing_station WHERE time &gt;= CONCAT(CURDATE(),&#39; &#39;,&#39;11:30:00&#39;) AND time &lt;= CONCAT(CURDATE(),&#39; &#39;,&#39;12:00:00&#39;) GROUP BY id, error_msg;&quot;

/*
mysql: run mysql

-uroot: specify your mysql username (note there are no spaces)

-p$PASS: -p is for password and $PASS is the variable with the password

database_name: is the data base name

-e: is short for -execute a query

The remaining is the query to execute. I would make sure the query
works by running this same line in the server interactively.

What this query means is to get the counts of the id and error messages
where the time is between the scheduele cron of the API request.

This way I get a summary of the error messages and how many lines were
appended between the time the script should&#39;ve started and should&#39;ve ended
*/
</code></pre>
<p>As stated in the first line of the code chunk, create a text file with your password. You can do so with <code>echo &quot;Your SQL username password&quot; &gt;&gt; pw.txt</code>. That should allow PASS to read in the password just fine.</p>
<p>And that should be it! Make sure you run each of these steps separately so that they work on it’s own and you don’get weird errors. This workflow will now run <code>cron</code> jobs at whatever time you set it, return the output to a text file (in case something bad happens and you want to look at the log) and run a query after it finishes so that you only get one email with a summary of API requests.</p>
<p>Hope this was helpful!</p>
<p>PS:</p>
<ul>
<li><p><a href="https://www.digitalocean.com/community/tutorials/a-basic-mysql-tutorial">Basic MySQL tutorial</a></p></li>
<li><p>I use SQL Workbench to run queries from my local computer</p></li>
</ul>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>How long should I wait for my bike?</title>
      <link>/blog/2017-12-01-how-long-should-i-wait-for-my-bike/how-long-should-i-wait-for-my-bike/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-12-01-how-long-should-i-wait-for-my-bike/how-long-should-i-wait-for-my-bike/</guid>
      <description><![CDATA[
      


<p>I’ve just started a project which I’m very excited about. Everyday I take my bike to work and most days I have one of two problems. First, whenever I get to my station there are no bikes available; no problem, there’s an app that shows the closest stations with bikes available. The problem is that these stations might be far and sometimes I’m relucant to walk that much. I’d love for bicing to give me some time estimation until a new bike arrives.</p>
<p>Second, whenever you’re trying to return a bike the station might not have any parking spaces available. Similarly, it would be very cool if bicing (the public bicycle company) gave me an estimate of how much time I should wait until a new bike will be taken. I started thinking on how I could implement this and started looking for bicing data online. To my surprise, bicing actually releases their <strong>live</strong> data as a json! But for this type of estimation I need historical data. I want to know the pattern usage of the station and use that information for the prediction.</p>
<p>With that idea in mind, I got to work. I needed to set up my Virtual Private Server (VPS) to pull the data from the bicing API everyday. Because this is still a work in progress, I will only describe here how I set my VPS to scrape the bicing API everyday and how I set <code>cron</code> to send me an email after every scrape.</p>
<p>I have a VPS from <a href="https://www.digitalocean.com/">Digital Ocean</a> with an Ubuntu OS and 512 mb of RAM and 2 GB of hard disk. That’s enough for this task because the data should not be very big, even in the long run. In any case you can adjust for your VPS to have more memory/ram without losing information. Assuming you have <a href="https://www.digitalocean.com/community/tutorials/how-to-install-r-on-ubuntu-16-04-2">R installed in your Ubuntu VPS</a> with your favorite packages, then make sure your script works by running <code>Rscript path/to/your/script.R</code>. It might be better to type <code>which Rscript</code> in the terminal and paste the path to the executable, similar to <code>/usr/bin/Rscript path/to/your/script.R</code></p>
<p>My workflow is as follows: I first create an empty dataset saved as <code>.rds</code> and my script reads the data, scrapes the bicing data and then saves the data by appending both the empty and the scraped data. It finishes by saving the same <code>.rds</code> for a later scrape. I tested this very thoroughly to make sure the script wouldn’t fail and I always get the expected data.</p>
<p>All good so far, right? This took me no time. The hard problem came when setting the <code>cron</code> job, which is a way of scheduling tasks in OSx and Ubuntu. For an explanation of how <code>cron</code> works, check out how I set <a href="blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/index.html">my PISA twitter bot</a>.</p>
<p>First, make sure you have <code>cron</code> <a href="https://www.digitalocean.com/community/tutorials/how-to-use-cron-to-automate-tasks-on-a-vps">installed</a>. I followed <strong>a lot</strong> of tutorials and dispered information. What worked for me perhaps does not work for you, but here it is.</p>
<p>Type <code>crontab -e</code> and the cron interface should appear. The lines starting with <code>#</code> are coments, so scroll down until the end of the comments. First we have to set a few environmental variables that <code>cron</code> uses to execute your script. I followed <a href="http://krisjordan.com/essays/timesaving-crontab-tips">these tips</a>.</p>
<p>When I finished my crontab looked like this:</p>
<pre class="bash"><code>SHELL=/bin/bash
PATH=/home/cimentadaj/bin:/home/cimentadaj/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
HOME=/home/cimentadaj/bicycle
MAILTO=my_email # set email here!
15,50 16  * * * /usr/bin/Rscript scrape_bicing.R</code></pre>
<ul>
<li><p>SHELL is the path to the pre-determined program to run on the cron job. Be default I set it to bash (but it could be anything else you want).</p></li>
<li><p>PATH I’m not sure what’s for but I pasted the output of <code>echo $PATH</code>, as the tips suggested.</p></li>
<li><p>HOME is the root directory where the script will be executed, I set it to where the script is (or where your project is at).</p></li>
<li><p>MAILTO is the email where I will get the cron job alert when it finishes.</p></li>
<li><p><code>15,50 16  * * * /usr/bin/Rscript scrape_bicing.R</code> is the schedule, program and script to run. Here I set arbitrary times, so the the script is scheduled to run at <code>16:15</code> and <code>16:50</code> every day, every month and every year. I will run using <code>Rscript</code> and the name of the script to run.</p></li>
</ul>
<p><strong>WARNING:</strong> remember that the <code>cron</code> is set relative to the time of where your server is. Mine did not have the same timezone of where I lived, so I had to set the <code>cron</code> one hour before of my actual time. Use <code>date</code> to print the time of your VPS.</p>
<p>Even after this, the <code>cron</code> job was still not running. Nothing, no email, no log, no change in the data. I then figured out that Ubuntu systems have some <a href="https://serverfault.com/a/754104">pecularities</a> when it comes to <code>cron</code>. So I went to <code>./etc/</code> and renamed every <code>cron.</code> file for <code>cron-</code> with <code>rename 's/cron./cron-/g' *</code>, thanks to this <a href="https://stackoverflow.com/a/20657563/3617958">answer</a>.</p>
<p>Run again and it worked! Great. However, I didn’t receive an email stating that the <code>cron</code> job finished. I looked up many solutions and ended up installing <code>ssmtp</code> which is a library for sending emails from terminal. I won’t bore you with the details. Here are the steps I took:</p>
<ul>
<li>Install <code>ssmtp</code> with <code>sudo apt-get update</code> and <code>sudo apt-get install ssmtp</code>.</li>
<li>Edit <code>ssmtp.conf</code> with <code>sudo nano /etc/ssmtp/ssmtp.conf</code></li>
</ul>
<p>Here’s the config that worked for me using <code>gmail</code>:</p>
<pre class="bash"><code># Config file for sSMTP sendmail
#
# The person who gets all mail for userids &lt; 1000
# Make this empty to disable rewriting.
root=your_email@gmail.com

# The place where the mail goes. The actual machine name is required no 
# MX records are consulted. Commonly mailhosts are named mail.domain.com
mailhub=smtp.gmail.com:587

AuthUser=your_email@gmail.com
AuthPass=your_password
UseTLS=YES
UseSTARTTLS=yes
TLS_CA_FILE=/etc/ssl/certs/ca-certificates.crt

# Where will the mail seem to come from?
#rewriteDomain=gmail.com

# The full hostname
hostname=your_host_name

# Are users allowed to set their own From: address?
# YES - Allow the user to specify their own From: address
# NO - Use the system generated From: address
#FromLineOverride=YES</code></pre>
<p>Three caveats that took me a lot of time to figure out.</p>
<ul>
<li><p>First, <a href="https://www.digitalocean.com/community/tutorials/how-to-use-google-s-smtp-server">some docs</a> say you should use another port in <code>mailhub</code>, but <code>587</code> worked for me.</p></li>
<li><p><code>TLS_CA_FILE</code>: make sure that <a href="https://askubuntu.com/questions/342484/etc-pki-tls-certs-ca-bundle-crt-not-found">this file exists</a>! For Ubuntu/Debian the file is at <code>/etc/ssl/certs/ca-certificates.crt</code> while on other platforms it might be in <code>/etc/pki/tls/certs/ca-bundle.crt</code>. Note the different file names!</p></li>
<li><p><code>hostname</code> should be the result of typing <code>hostname</code> in your server.</p></li>
</ul>
<p>Lastly, I also added the line <code>root:your_EMAIL_@gmail.com:smtp.gmail.com:587</code> with <code>sudo nano /etc/ssmtp/revaliases</code>.</p>
<p>After an entire day figuring out all this information, the <code>cron</code> job worked! I now set my <code>cron</code> job and whenever it finished I receive an email directly showing the log of the script.</p>
<p>I wrote this primarily for me not to forget any of this, but it might be useful for other people.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Scraping and visualizing How I Met Your Mother</title>
      <link>/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</guid>
      <description><![CDATA[
      


<p>How I Met Your Mother (HIMYM from here after) is a television series very similar to the classical ‘Friends’ series from the 90’s. Following the release of the <a href="http://tidytextmining.com/">tidy text</a> book I was looking for a project in which I could apply these skills. I decided I would scrape all the transcripts from HIMYM and analyze patterns between characters. This post really took me to the limit in terms of web scraping and pattern matching, which was specifically what I wanted to improve in the first place. Let’s begin!</p>
<p>My first task was whether there was any consistency in the URL’s that stored the transcripts. If you ever watched HIMYM, we know there’s around nine seasons, each one with about 22 episodes. This makes about 200 episodes give or take. It would be a big pain to manually write down 200 complicated URL’s. Luckily, there is a way of finding the 200 links without writing them down manually.</p>
<p>First, we create the links for the 9 websites that contain all episodes (1 through season 9)</p>
<pre class="r"><code>library(rvest)
library(tidyverse)
library(stringr)
library(tidytext)

main_url &lt;- &quot;http://transcripts.foreverdreaming.org&quot;
all_pages &lt;- paste0(&quot;http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=&quot;, seq(0, 200, 25))
characters &lt;- c(&quot;ted&quot;, &quot;lily&quot;, &quot;marshall&quot;, &quot;barney&quot;, &quot;robin&quot;)</code></pre>
<p>Each of the URL’s of <code>all_pages</code> contains all episodes for that season (so around 22 URL’s). I also picked the characters we’re gonna concentrate for now. From here the job is very easy. We create a function that reads each link and parses the section containing all links for that season. We can do that using <a href="http://selectorgadget.com/.">SelectorGadget</a> to find the section we’re interested in. We then search for the <code>href</code> attribute to grab all links in that attribute and finally create a tibble with each episode together with it’s link.</p>
<pre class="r"><code>episode_getter &lt;- function(link) {
  title_reference &lt;-
    link %&gt;%
    read_html() %&gt;%
    html_nodes(&quot;.topictitle&quot;) # Get the html node name with &#39;selector gadget&#39;
  
  episode_links &lt;-
    title_reference %&gt;%
    html_attr(&quot;href&quot;) %&gt;%
    gsub(&quot;^.&quot;, &quot;&quot;, .) %&gt;%
    paste0(main_url, .) %&gt;%
    setNames(title_reference %&gt;% html_text()) %&gt;%
    enframe(name = &quot;episode_name&quot;, value = &quot;link&quot;)
  
  episode_links
}

all_episodes &lt;- map_df(all_pages, episode_getter) # loop over all seasons and get all episode links
all_episodes$id &lt;- 1:nrow(all_episodes)</code></pre>
<p>There we go! Now we have a very organized <code>tibble</code>.</p>
<pre class="r"><code>all_episodes
# # A tibble: 208 x 3
#    episode_name                   link                                  id
#    &lt;chr&gt;                          &lt;chr&gt;                              &lt;int&gt;
#  1 01x01 - Pilot                  http://transcripts.foreverdreamin~     1
#  2 01x02 - Purple Giraffe         http://transcripts.foreverdreamin~     2
#  3 01x03 - Sweet Taste of Liberty http://transcripts.foreverdreamin~     3
#  4 01x04 - Return of the Shirt    http://transcripts.foreverdreamin~     4
#  5 01x05 - Okay Awesome           http://transcripts.foreverdreamin~     5
#  6 01x06 - Slutty Pumpkin         http://transcripts.foreverdreamin~     6
#  7 01x07 - Matchmaker             http://transcripts.foreverdreamin~     7
#  8 01x08 - The Duel               http://transcripts.foreverdreamin~     8
#  9 01x09 - Belly Full of Turkey   http://transcripts.foreverdreamin~     9
# 10 01x10 - The Pineapple Incident http://transcripts.foreverdreamin~    10
# # ... with 198 more rows</code></pre>
<p>The remaining part is to actually scrape the text from each episode. We can work that out for a single episode and then turn that into a function and apply for all episodes.</p>
<pre class="r"><code>episode_fun &lt;- function(file) {
  
  file %&gt;%
    read_html() %&gt;%
    html_nodes(&quot;.postbody&quot;) %&gt;%
    html_text() %&gt;%
    str_split(&quot;\n|\t&quot;) %&gt;%
    .[[1]] %&gt;%
    data_frame(text = .) %&gt;%
    filter(str_detect(text, &quot;&quot;), # Lots of empty spaces
           !str_detect(text, &quot;^\\t&quot;), # Lots of lines with \t to delete
           !str_detect(text, &quot;^\\[.*\\]$&quot;), # Text that start with brackets
           !str_detect(text, &quot;^\\(.*\\)$&quot;), # Text that starts with parenthesis
           str_detect(text, &quot;^*.:&quot;), # I want only lines with start with dialogue (:)
           !str_detect(text, &quot;^ad&quot;)) # Remove lines that start with ad (for &#39;ads&#39;, the link of google ads)
}</code></pre>
<p>The above function reads each episode, turns the html text into a data frame and organizes it clearly for text analysis. For example:</p>
<pre class="r"><code>episode_fun(all_episodes$link[15])
# # A tibble: 195 x 1
#    text                                                                   
#    &lt;chr&gt;                                                                  
#  1 Ted from 2030: Kids, something you might not know about your Uncle Mar~
#  2 &quot;Ted: You don&#39;t have to shout out \&quot;poker\&quot; when you win.&quot;             
#  3 Marshall: I know. It&#39;s just fun to say.                                
#  4 &quot;Ted from 2030: We all finally agreed Marshall should be running our g~
#  5 &quot;Marshall: It&#39;s called \&quot;Marsh-gammon.\&quot; It combines all the best feat~
#  6 Robin: Backgammon, obviously.                                          
#  7 &quot;Marshall: No. Backgammon sucks. I took the only good part of backgamm~
#  8 Lily: I&#39;m so excited Victoria&#39;s coming.                                
#  9 Robin: I&#39;m going to go get another round.                              
# 10 Ted: Okay, I want to lay down some ground rules for tonight. Barney, I~
# # ... with 185 more rows</code></pre>
<p>We now have a data frame with only dialogue for each character. We need to apply that function to each episode and <code>bind</code> everything together. We first apply the function to every episode.</p>
<pre class="r"><code>all_episodes$text &lt;- map(all_episodes$link, episode_fun)</code></pre>
<p>The <code>text</code> list-column is an organized list with text for each episode. However, manual inspection of some episodes actually denotes a small error that limits our analysis greatly. Among the main interests of this document is to study relationships and presence between characters. For that, we need each line of text to be accompanied by the character who said it. Unfortunately, some of these scripts don’t have that.</p>
<p>For example, check any episode from season <a href="http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=175">8</a> and <a href="http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=200">9</a>. The writer didn’t write the dialogue and just rewrote the lines. There’s nothing we can do so far to improve that and we’ll be excluding these episodes. This pattern is also present in random episodes like in season 4 or season 6. We can exclude chapters based on the number of lines we parsed. On average, each of these episodes has about 200 lines of dialogue. Anything significantly lower, like 30 or 50 lines, is an episode which doesn’t have a lot of dialogue.</p>
<pre class="r"><code>all_episodes$count &lt;- map_dbl(all_episodes$text, nrow)</code></pre>
<p>We can extend the previous <code>tibble</code> to be a bit more organized by separating the episode-season column into separate season and episo numbers.</p>
<pre class="r"><code>all_episodes &lt;-
  all_episodes %&gt;%
  separate(episode_name, c(&quot;season&quot;, &quot;episode&quot;), &quot;-&quot;, extra = &quot;merge&quot;) %&gt;%
  separate(season, c(&quot;season&quot;, &quot;episode_number&quot;), sep = &quot;x&quot;)</code></pre>
<p>Great! We now have a very organized <code>tibble</code> with all the information we need. Next step is to actually break down the lines into words and start looking for general patterns. We can do that by looping through all episodes that have over 100 lines (just an arbitrary threshold) and unnesting each line for each <strong>valid</strong> character.</p>
<pre class="r"><code>lines_characters &lt;-
  map(filter(all_episodes, count &gt; 100) %&gt;% pull(text), ~ { 
    # only loop over episodes that have over 100 lines
    .x %&gt;%
      separate(text, c(&quot;character&quot;, &quot;text&quot;), sep = &quot;:&quot;, extra = &#39;merge&#39;) %&gt;%
      # separate character dialogue from actual dialogo
      unnest_tokens(character, character) %&gt;%
      filter(str_detect(character, paste0(paste0(&quot;^&quot;, characters, &quot;$&quot;), collapse = &quot;|&quot;))) %&gt;%
      # only count the lines of our chosen characters
      mutate(episode_lines_id = 1:nrow(.))
  }) %&gt;%
  setNames(filter(all_episodes, count &gt; 100) %&gt;% # name according to episode
             unite(season_episode, season, episode_number, sep = &quot;x&quot;) %&gt;%
             pull(season_episode)) %&gt;%
  enframe() %&gt;%
  unnest() %&gt;%
  mutate(all_lines_id = 1:nrow(.))</code></pre>
<p>Ok, our text is sort of ready. Let’s remove some bad words.</p>
<pre class="r"><code>words_per_character &lt;-
  lines_characters %&gt;%
  unnest_tokens(word, text) %&gt;% # expand all sentences into words
  anti_join(stop_words) %&gt;% # remove bad words
  filter(!word %in% characters) %&gt;% # only select characters we&#39;re interested
  arrange(name) %&gt;%
  separate(name, c(&quot;season&quot;, &quot;episode&quot;), sep = &quot;x&quot;, remove = FALSE) %&gt;%
  mutate(name = factor(name, ordered = TRUE),
         season = factor(season, ordered = TRUE),
         episode = factor(episode, ordered = TRUE)) %&gt;%
  filter(season != &quot;07&quot;)</code></pre>
<p>Just to make sure, let’s look at the <code>tibble</code>.</p>
<pre class="r"><code>words_per_character
# # A tibble: 88,174 x 7
#    name     season episode character episode_lines_id all_lines_id word   
#    &lt;ord&gt;    &lt;ord&gt;  &lt;ord&gt;   &lt;chr&gt;                &lt;int&gt;        &lt;int&gt; &lt;chr&gt;  
#  1 &quot;01x01 &quot; 01     &quot;01 &quot;   marshall                 1            1 ring   
#  2 &quot;01x01 &quot; 01     &quot;01 &quot;   marshall                 1            1 marry  
#  3 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 perfect
#  4 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 engaged
#  5 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 pop    
#  6 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 champa~
#  7 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 drink  
#  8 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 toast  
#  9 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 kitchen
# 10 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 floor  
# # ... with 88,164 more rows</code></pre>
<p>Perfect! One row per word, per character, per episode with the id of the line of the word.</p>
<p>Alright, let’s get our hands dirty. First, let visualize the presence of each character in terms of words over time.</p>
<pre class="r"><code># Filtering position of first episode of all seasons to
# position the X axis in the next plot.
first_episodes &lt;-
  all_episodes %&gt;%
  filter(count &gt; 100, episode_number == &quot;01 &quot;) %&gt;%
  pull(id)

words_per_character %&gt;%
  split(.$name) %&gt;%
  setNames(1:length(.)) %&gt;%
  enframe(name = &quot;episode_id&quot;) %&gt;%
  unnest() %&gt;%
  count(episode_id, character) %&gt;%
  group_by(episode_id) %&gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&gt;%
  ggplot(aes(as.numeric(episode_id), perc, group = character, colour = character)) +
  geom_line() +
  geom_smooth(method = &quot;lm&quot;) +
  scale_colour_discrete(guide = FALSE) +
  scale_x_continuous(name = &quot;Seasons&quot;,
                     breaks = first_episodes, labels = paste0(&quot;S&quot;, 1:7)) +
  scale_y_continuous(name = &quot;Percentage of words per episode&quot;) +
  theme_minimal() +
  facet_wrap(~ character, ncol = 3)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-13-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Ted is clearly the character with the highest number of words per episode followed by Barney. Lily and Robin, the only two women have very low presence compared to the men. In fact, if one looks closely, Lily seemed to have decreased slightly over time, having an all time low in season 4. Marshall, Lily’s partner in the show, does have much lower presence than both Barney and Ted but he has been catching up over time.</p>
<p>We also see an interesting pattern where Barney has a lot of peaks, suggesting that in some specific episodes he gains predominance, where Ted has an overall higher level of words per episode. And when Ted has peaks, it’s usually below its trend-line.</p>
<p>Looking at the distribution:</p>
<pre class="r"><code># devtools::install_github(&quot;clauswilke/ggjoy&quot;)
library(ggjoy)

words_per_character %&gt;%
  split(.$name) %&gt;%
  setNames(1:length(.)) %&gt;%
  enframe(name = &quot;episode_id&quot;) %&gt;%
  unnest() %&gt;%
  count(season, episode_id, character) %&gt;%
  group_by(episode_id) %&gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&gt;%
  ggplot(aes(x = perc, y = character, fill = character)) +
  geom_joy(scale = 0.85) +
  scale_fill_discrete(guide = F) +
  scale_y_discrete(name = NULL, expand=c(0.01, 0)) +
  scale_x_continuous(name = &quot;Percentage of words&quot;, expand=c(0.01, 0)) +
  ggtitle(&quot;Percentage of words per season&quot;) +
  facet_wrap(~ season, ncol = 7) +
  theme_minimal()</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>we see the differences much clearer. For example, we see Barney’s peaks through out every season with Season 6 seeing a clear peak of 40%. On the other hand, we see that their distributions don’t change that much over time! Suggesting that the presence of each character is very similar in all seasons. Don’t get me wrong, there are differences like Lily in Season 2 and then in Season 6, but in overall terms the previous plot suggests no increase over seasons, and this plot suggests that between seasons, there’s not a lot of change in their distributions that affects the overall mean.</p>
<p>If you’ve watched the TV series, you’ll remember Barney always repeating one similar trademark word: legendary! Although it is a bit cumbersome for us to count the number of occurrences of that sentence once we unnested each sentence, we can at least count the number of words per character and see whether some characters have particular words.</p>
<pre class="r"><code>count_words &lt;-
  words_per_character %&gt;%
  filter(!word %in% characters) %&gt;%
  count(character, word, sort = TRUE)

count_words %&gt;%
  group_by(character) %&gt;%
  top_n(20) %&gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &quot;free_y&quot;)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Here we see that a lot of the words we capture are actually nouns or expressions which are common to everyone, such as ‘yeah’, ‘hey’ or ‘time’. We can weight down commonly used words for other words which are important but don’t get repeated a lot. We can exclude those words using <code>bind_tf_idf()</code>, which for each character decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection or corpus of documents (see 3.3 in <a href="http://tidytextmining.com/tfidf.html" class="uri">http://tidytextmining.com/tfidf.html</a>).</p>
<pre class="r"><code>count_words %&gt;%
  bind_tf_idf(word, character, n) %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  group_by(character) %&gt;%
  top_n(20) %&gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &quot;free_y&quot;)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Now Barney has a very distinctive word usage, one particularly sexist with words such as couger, bang and tits. Also, we see the word legendary as the thirdly repeated word, something we were expecting! On the other hand, we see Ted with things like professor (him), aunt (because of aunt Lily and such).</p>
<p>Knowing that Ted is the main character in the series is no surprise. To finish off, we’re interested in knowing which characters are related to each other. First, let’s turn the data frame into a suitable format.</p>
<p>Here we turn all lines to lower case and check which characters are present in the text of each dialogue. The loop will return a vector of logicals whether there was a mention of any of the characters. For simplicity I exclude all lines where there is more than 1 mention of a character, that is, 2 or more characters.</p>
<pre class="r"><code>lines_characters &lt;-
  lines_characters %&gt;%
  mutate(text = str_to_lower(text))

rows_fil &lt;-
  map(characters, ~ str_detect(lines_characters$text, .x)) %&gt;%
  reduce(`+`) %&gt;%
  ifelse(. &gt;= 2, 0, .) # excluding sentences which have 2 or more mentions for now
  # ideally we would want to choose to count the number of mentions
  # per line or randomly choose another a person that was mentioned.</code></pre>
<p>Now that we have the rows that have a mention of another character, we subset only those rows. Then we want know which character was mentioned in which line. I loop through each line and test which character is present in that specific dialogue line. The loop returns the actual character name for each dialogue. Because we already filtered lines that <strong>have</strong> a character name mentioned, the loop should return a vector of the same length.</p>
<pre class="r"><code>character_relation &lt;-
  lines_characters %&gt;%
  filter(as.logical(rows_fil)) %&gt;%
  mutate(who_said_what =
           map_chr(.$text, ~ { # loop over all each line
             who_said_what &lt;- map_lgl(characters, function(.y) str_detect(.x, .y))
             # loop over each character and check whether he/she was mentioned
             # in that line
             characters[who_said_what]
             # subset the character that matched
           }))
</code></pre>
<p>Finally, we plot the relationship using the <code>ggraph</code> package.</p>
<pre class="r"><code>library(ggraph)
library(igraph)

character_relation %&gt;%
  count(character, who_said_what) %&gt;%
  graph_from_data_frame() %&gt;%
  ggraph(layout = &quot;linear&quot;, circular = TRUE) +
  geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                width = 2.5, show.legend = FALSE) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>A very clear pattern emerges. There is a strong relationship between Robin and Barney towards Ted. In fact, their direct relationship is very weak, but both are very well connected to Ted. On the other hand, Marshall and Lily are also reasonably connected to Ted but with a weaker link. Both of them are indeed very connected, as should be expected since they were a couple in the TV series.</p>
<p>We also see that the weakest members of the group are Robin and Barney with only strong bonds toward Ted but no strong relationship with the other from the group. Overall, there seems to be a division: Marshall and Lily hold a somewhat close relationship with each other and towards Ted and Barney and Robin tend to be related to Ted but no one else.</p>
<p>As a follow-up question, is this pattern of relationships the same across all seasons? We can do that very quickly by filtering each season using the previous plot.</p>
<pre class="r"><code>library(cowplot)

# Loop through each season
seasons &lt;- paste0(0, 1:7)

all_season_plots &lt;- lapply(seasons, function(season_num) {

  set.seed(2131)
  
  character_relation %&gt;%
    # Extract the season number from the `name` column
    mutate(season = str_replace_all(character_relation$name, &quot;x(.*)$&quot;, &quot;&quot;)) %&gt;%
    filter(season == season_num) %&gt;%
    count(character, who_said_what) %&gt;%
    graph_from_data_frame() %&gt;%
    ggraph(layout = &quot;linear&quot;, circular = TRUE) +
    geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                  width = 2.5, show.legend = FALSE) +
    geom_node_text(aes(label = name), repel = TRUE) +
    theme_void()
})

# Plot all graphs side-by-side
cowplot::plot_grid(plotlist = all_season_plots, labels = seasons)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-20-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>There are reasonable changes for all non-Ted relationship! For example, for season 2 the relationship Marshall-Lily-Ted becomes much stronger and it disappears in season 3. Let’s remember that these results might be affected by the fact that I excluded some episodes because of low number of dialogue lines. Keeping that in mind, we also see that for season 7 the Robin-Barney relationship became much stronger (is this the season the started dating?). All in all, the relationships don’t look dramatically different from the previous plot. Everyone seems to be strongly related to Ted. The main difference is the changes in relationship between the other members of the cast.</p>
<p>This dataset has a lot of potential and I’m sure I’ve scratched the surface of what one can do with this data. I encourage anyone interested in the topic to use the code to analyze the data further. One idea I might explore in the future is to build a model that attempts to predict who said what for all dialogue lines that didn’t have a character member. This can be done by extracting features from all sentences and using these patterns try to classify which. Any feedback is welcome, so feel free to message me at <a href="mailto:cimentadaj@gmail.com">cimentadaj@gmail.com</a></p>
]]>
      </description>
    </item>
    
  </channel>
</rss>
