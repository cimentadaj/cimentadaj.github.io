<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Jorge Cimentada</title>
    <link>/blog/</link>
    <description>Recent content in Blogs on Jorge Cimentada</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Wed, 25 Nov 2020 00:00:00 +0000</lastBuildDate>
    
    <atom:link href="/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deploying MySQL database using Docker</title>
      <link>/blog/2020-11-25-deploying-mysql-database-using-docker/deploying-mysql-database-using-docker/</link>
      <pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-11-25-deploying-mysql-database-using-docker/deploying-mysql-database-using-docker/</guid>
      <description><![CDATA[
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>I have a few projects on which I use databases to collect data. Each time I begin some of these projects I have to install the databases and what not. This is usually burden with a lot of back and forth, following tutorials on how to install the dependencies and so on. It’s NEVER an easy task, taking more than one day to go from wanting to install a database engine and actually having the database ready to connect from local/remote machines with the correct permissions.</p>
<p>Docker is suppose to make this much easier and here I provide a set of easy steps for going from (1) want to install MySQL to (2) having the database deployed and working in no time.</p>
<p>The only two requirements you’ll need for this is <code>docker</code> and <code>docker-compose</code>. <code>docker</code> is the tool for lifting the containers with MySQL and <code>docker-compose</code> is just a convenient way for us to specify some options related to that container in a readable file (instead of a long <code>docker</code> statement with long arguments). Note that <code>docker-compose</code> is used for orchestrating many containers into a single network but we’ll use it just for the convenience of readability. If you don’t have any of these two tools, follow this tutorial for <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-20-04">docker</a> and <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-20-04">this one</a> for <code>docker-compose</code>.</p>
<div id="summarized-version" class="section level2">
<h2>Summarized version</h2>
<p>If you’re here for the code, here’s the summarized versions. Skip down for detailed explanations:</p>
<pre class="bash"><code># Create folder that will host the data and other files
mkdir ~/Downloads/sql_test/
cd ~/Downloads/sql_test
touch .gitignore
echo &quot;data/&quot; &gt;&gt; ./.gitignore

# Create sql folder to host database/table creation scripts
mkdir -p ./sql/

# Append this SQL code to the file sql/init_db.sql to create database/table
echo &quot;
CREATE DATABASE test;
CREATE USER &#39;test_user&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123&#39;;
GRANT ALL ON test.* TO &#39;test_user&#39;@&#39;%&#39;;

/* Make sure the privileges are installed */
FLUSH PRIVILEGES;

USE test;

CREATE TABLE test_table (
  name VARCHAR(30)
);
&quot; &gt;&gt; sql/init_db.sql

# Create a MySQL config file to allow to append data
mkdir sql_conf
echo &quot;
[mysqld]
# This just makes sure we can append data from outside the container
local-infile=1
&quot; &gt;&gt; sql_conf/mysql.cnf

echo &#39;
version: &quot;3.1&quot;
services:
  db:
    container_name: sql-test
    image: mysql:8.0
    restart: always
    ports:
      - 3306:3306
    environment:
      - MYSQL_ROOT_PASSWORD=123456
    volumes:
      - /home/jorge/Downloads/sql_test/sql:/docker-entrypoint-initdb.d
      - /home/jorge/Downloads/sql_test/data:/var/lib/mysql
      - /home/jorge/Downloads/sql_test/sql_conf:/etc/mysql/conf.d
&#39; &gt;&gt; docker-compose.yml

# Lift the container with MySQL
docker-compose up -d</code></pre>
<p>and with R you can append/read data:</p>
<pre class="r"><code>library(DBI)
library(RMySQL)

con &lt;- dbConnect(MySQL(),
                 host = &#39;127.0.0.1&#39;,
                 dbname = &#39;test&#39;,
                 port = 3306,
                 user = &#39;test_user&#39;,
                 password = &#39;123&#39;)

# Append data
dt &lt;- data.frame(name = &quot;Jorge is my name&quot;)
field_types &lt;- c(name = &quot;TEXT&quot;)
dbWriteTable(conn = con,
             name = &quot;test_table&quot;,
             value = dt,
             append = TRUE,
             row.names = FALSE,
             overwrite = FALSE,
             field.types = field_types)</code></pre>
<pre><code># [1] TRUE</code></pre>
<p>and then extract the data you just appended:</p>
<pre class="r"><code>dbGetQuery(con, &quot;SELECT * FROM test.test_table&quot;)</code></pre>
<pre><code>#               name
# 1 Jorge is my name</code></pre>
</div>
<div id="detailed-version" class="section level2">
<h2>Detailed version</h2>
<p>Here are the detailed steps:</p>
<ul>
<li>Create folder that will contain the data</li>
</ul>
<pre class="bash"><code>mkdir ~/Downloads/sql_test/</code></pre>
<ul>
<li>Navigate to the folder and add stuff we wouldn’t like to commit to Github. Note that <em>we haven’t</em> created a data folder. You’ll see why this is important later.</li>
</ul>
<pre class="bash"><code>cd ~/Downloads/sql_test
touch .gitignore
echo &quot;data/&quot; &gt;&gt; ./.gitignore</code></pre>
<ul>
<li>Create a <code>sql</code> folder and create a file inside (the name doesn’t matter as long as it ends in <code>.sql</code>) that creates the database, the users and the tables that you’re interested in:</li>
</ul>
<pre class="bash"><code># Create sql foldr
mkdir -p ./sql/

# Append this SQL code to the file sql/init_db.sql
echo &quot;
CREATE DATABASE test;
CREATE USER &#39;test_user&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123&#39;;
GRANT ALL ON test.* TO &#39;test_user&#39;@&#39;%&#39;;

/* Make sure the privileges are installed */
FLUSH PRIVILEGES;

USE test;

CREATE TABLE test_table (
  name VARCHAR(30)
);
&quot; &gt;&gt; sql/init_db.sql</code></pre>
<p>Just to make sure we’re on the right track, the previous statement just added some SQL code to a file called <code>init_db.sql</code> which will create all the databases/tables/users you’re interested in the database to have. If you DON’T want to create any database/table, you don’t even have to create a <code>sql</code> folder.</p>
<ul>
<li>Create MySQL config to allow to append data to the database</li>
</ul>
<pre class="bash"><code>mkdir sql_conf
echo &quot;
[mysqld]
# This just makes sure we can append data from outside the container
local-infile=1
&quot; &gt;&gt; sql_conf/mysql.cnf</code></pre>
<ul>
<li>Create a <code>docker-compose</code> file which has the <code>MySQL</code> image with some options.</li>
</ul>
<pre class="bash"><code>echo &#39;
version: &quot;3.1&quot;
services:
  db:
    container_name: sql-test
    image: mysql:8.0
    restart: always
    ports:
      - 3306:3306
    environment:
      - MYSQL_ROOT_PASSWORD=123456
    volumes:
      - /home/jorge/Downloads/sql_test/sql:/docker-entrypoint-initdb.d
      - /home/jorge/Downloads/sql_test/data:/var/lib/mysql
      - /home/jorge/Downloads/sql_test/sql_conf:/etc/mysql/conf.d
&#39; &gt;&gt; docker-compose.yml</code></pre>
<p>I’ll stop here to explain the important fields.</p>
<p><code>container_name</code> is the name that the container will be assigned. You can refer to this name and even log in to that container to execute code inside.</p>
<p><code>image</code> contains the <code>MySQL</code> image tagged at version 8.0. This makes sure that we always download the same version of <code>MySQL</code> and is reproducible within the same OSx.</p>
<p><code>MYSQL_ROOT_PASSWORD</code> is an environmental variable that is integrated in the image <code>mysql:8.0</code>. It assigns a password to the root user in <code>MySQL</code>. This is useful to have a default root password if you want to log in as root to execute higher level privileges.</p>
<p><code>volumes</code> is perhaps the most important thing to explain here. It has three entries with some paths. In docker parlance, volumes are links between your <em>local</em> computer and the virtual docker container. This means that if I link my <em>local</em> folder <code>~/Downloads/my_sql/data/</code> to the container’s folder <code>/var/lib/mysql/</code> everything that is inside <code>~/Downloads/my_sql/data/</code> will be inside <code>/var/lib/mysql/</code> and vice versa. It essentially creates a mirror between the folders on your <em>local</em> computer and your container.</p>
<p>We have three fields here. This first one is <code>/home/jorge/Downloads/sql_test/sql:/docker-entrypoint-initdb.d</code>. The first part, <code>/home/jorge/Downloads/sql_test/sql</code> points to our <code>sql</code> folder containing the SQL code that creates the database/users/tables. <strong>This</strong> particular MySQL image has a folder <code>/docker-entrypoint-initdb.d</code> inside the container which the container will <strong>execute</strong> once it is created. In other words, anything inside <code>/docker-entrypoint-initdb.d</code> with a <code>sql</code> file extension will be executed once the container is deployed. That’s why we’re making a mirror between our script to create the database/table and the folder that will execute everything inside the container.</p>
<p>To specify volums, we separate the two paths with a <code>:</code> where the left path is the local directory and the right path is the container’s directory: <code>/home/jorge/Downloads/sql_test/sql</code><strong>:</strong><code>/docker-entrypoint-initdb.d</code></p>
<p>The second field is <code>/home/jorge/Downloads/sql_test/data:/var/lib/mysql</code>. This field links our data folder <code>/home/jorge/Downloads/sql_test/data</code> to the folder <em>inside</em> the container <code>/var/lib/mysql</code> <em>where</em> <code>MySQL</code> saves all the data. Yep, this means that even if we have MySQL inside the docker container, the data will be saved in our <em>local</em> computer. Best of both worlds! No burden of installing MySQL/dependencies, yet we can transfer all the MySQL data to our local machine. This means that we can stop the container any time, rerun it with the same <code>docker-compose.yml</code> and it will populate all databases with the data in the local computer. We’ll do an example in just a second.</p>
<p>Note that we <em>haven’t</em> created a data folder yet. This is because our <code>MySQL</code> image works this way: if it finds that the link between the data folders has been created, it assumes that there is some data in there and it <em>won’t</em> execute the SQL files for creating the database/tables. So for the first deployment of the docker container, we don’t create the folder, allowing the container to create the database/tables. Once we stop the container and rerun it again, it won’t execute the SQL scripts and just read the data in the linked volumes.</p>
<p>The third volume <code>/home/jorge/Downloads/sql_test/sql_conf:/etc/mysql/conf.d/</code> links our MySQL config to the place where MySQL searches for config files inside the container (<code>/etc/mysql/conf.d</code>). This small config just allows users outside the container to append data.</p>
<ul>
<li>Deploy the docker container</li>
</ul>
<p>The previous step might be the most daunting but let me break it to you: that is it. There’s nothing else to do but deploy everything.</p>
<pre class="bash"><code>docker-compose up -d</code></pre>
<pre><code># Creating network &quot;sql_test_default&quot; with the default driver
# Creating sql-test ... </code></pre>
<p>This command can take some time, as it’s downloading the image at first.</p>
<p>If you get some errors due to some port already in use this could be due to two reasons. Either you have another container redirecting to the port 3306 (use <code>docker ps</code> and <code>docker stop</code> to stop it) or you have a MySQL instance running locally already serving the port. You can stop it with <code>sudo service mysql stop</code>.</p>
<ul>
<li>Log in to the container and check the test database is there.</li>
</ul>
<p>Note that the container needs a few seconds to create the database/users/tables so wait a minute or two before running the queries below.</p>
<pre class="bash"><code># Remember that the password we specified was 123
mysql -h 127.0.0.1 -P 3306 -u test_user  -p -e &quot;SHOW DATABASES;&quot;</code></pre>
<pre><code># +--------------------+
# | Database           |
# +--------------------+
# | information_schema |
# | test               |
# +--------------------+</code></pre>
<p>You should see a database called test (the one we created). You can also connect from R and append data:</p>
<pre class="r"><code>library(DBI)
library(RMySQL)

con &lt;- dbConnect(MySQL(),
                 host = &#39;127.0.0.1&#39;,
                 dbname = &#39;test&#39;,
                 port = 3306,
                 user = &#39;test_user&#39;,
                 password = &#39;123&#39;)

# Append data
dt &lt;- data.frame(name = &quot;Jorge is my name&quot;)
field_types &lt;- c(name = &quot;TEXT&quot;)
dbWriteTable(conn = con,
             name = &quot;test_table&quot;,
             value = dt,
             append = TRUE,
             row.names = FALSE,
             overwrite = FALSE,
             field.types = field_types)</code></pre>
<pre><code># [1] TRUE</code></pre>
<p>and then extract the data you just appended:</p>
<pre class="r"><code>dbGetQuery(con, &quot;SELECT * FROM test.test_table&quot;)</code></pre>
<pre><code>#               name
# 1 Jorge is my name</code></pre>
<p>The cool thing about this is that we can take down the database with <code>docker-compose down</code>, put it up again with <code>docker-compose up -d</code> and the data we saved is still there:</p>
<pre class="bash"><code>docker-compose down</code></pre>
<pre><code># Stopping sql-test ... 
# Removing sql-test ... 
# Removing network sql_test_default</code></pre>
<pre class="bash"><code>docker-compose up -d</code></pre>
<pre><code># Creating network &quot;sql_test_default&quot; with the default driver
# Creating sql-test ... </code></pre>
<pre class="bash"><code># Remember the password is 123
mysql -h 127.0.0.1 -P 3306 -u test_user  -p -e &quot;SELECT * FROM test.test_table;&quot;</code></pre>
<pre><code># +------------------+
# | name             |
# +------------------+
# | Jorge is my name |
# +------------------+</code></pre>
<p>If you were deploying this on a remote server, everything applies and you would only have to change the IP to your server’s IP.</p>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>The simplest tidy machine learning workflow</title>
      <link>/blog/2020-02-06-the-simplest-tidy-machine-learning-workflow/the-simplest-tidy-machine-learning-workflow/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020-02-06-the-simplest-tidy-machine-learning-workflow/the-simplest-tidy-machine-learning-workflow/</guid>
      <description><![CDATA[
      


<p><code>caret</code> is a magical package for doing machine learning in R. Look at this code for running a regularized regression:</p>
<pre class="r"><code>library(caret)

inTrain &lt;- createDataPartition(y = mtcars$mpg,
                               p = 0.75,
                               list = FALSE)  

reg_mod &lt;- train(
  mpg ~ .,
  data = mtcars[inTrain, ],
  method = &quot;glmnet&quot;,
  tuneLength = 10,
  preProc = c(&quot;center&quot;, &quot;scale&quot;),
  trControl = trainControl(method = &quot;cv&quot;, number = 10)
)</code></pre>
<p>The two function calls in the expression above perform these operations:</p>
<ol style="list-style-type: decimal">
<li>Create a training set containing a random sample of 75% of the initial sample</li>
<li>Center and scale all predictors in the model</li>
<li>Identifies 10 alpha values (0 to 1) and then 10 additional lambda values</li>
<li>For each parameter set (one alpha value and another lambda value), <a href="http://topepo.github.io/caret/model-training-and-tuning.html">run a cross-validation 10 times</a></li>
<li>Effectively run 1000 models (10 alpha * 10 alpha) each one cross-validated (10)</li>
<li>Save the best model in the result together with the optimized tuning parameteres</li>
</ol>
<p>That is a lot of modelling, optimization and computation done with almost no mental load. However, in case you didn’t know, <code>caret</code> is doomed to be left behind. The creator of the package has stated that he will give maintenance to the package but <a href="https://twitter.com/topepos/status/1026939727910461440">most active development</a> will be given to <code>tidymodels</code>, its successor.</p>
<p><code>tidymodels</code> is more or less a restructuring of the <code>caret</code> package (as it aims to do the same thing and more) but with an interface and design philosophy that resembles the <code>Unix</code> philosophy. This means that instead of having one package and one function (<code>caret</code> and <code>train</code>) that does much of the work, all operations described above are performed by different packages.</p>
<p><code>tidymodels</code> has been in development for the past two years and the main pieces for effective modelling have been implemented (packages such as <code>parsnip</code>, <code>tune</code>, <code>yardstick</code>, etc…). However, there still isn’t a completely unified workflow that allows them to be as succint and elegant as <code>train</code>. I’ve been keeping an eye on the development of the different packages from <code>tidymodels</code> and I really want to understand the key workflow that will allow users to make modelling with <code>tidymodels</code> easy.</p>
<p>The objective of this post is to present what I think is currently the most succint and barebones workflow that a user should need using <code>tidymodels</code>. I reached this workflow by looking at the machine learning tutorials from the RStudio conference and stripped most of the details to see the link between the high-level steps in the modelling workflow and where <code>tidymodels</code> fits . In particular, I was curious on how <code>tidymodels</code> makes the workflow fit a logical set of steps without much mental load.</p>
<ul>
<li>What this post isn’t about:
<ul>
<li>This post won’t introduce you to the <code>tidymodels</code> package. It assumes you are familiar with some of the main packages</li>
<li>This post won’t show you everything that <code>tidymodels</code> can do (no fancy modelling or deep learning)</li>
<li>This post won’t delve into specific details on every single function</li>
</ul></li>
</ul>
<p>In fact, I’ve always had some issues using <code>tidymodels</code> because there are so many functions that are difficult to think as isolated entities that remembering every step is quite difficult (unlike the <code>tidyverse</code> where each package can be thought of as a different entity independent of the others but that you use them because they work well together).</p>
<ul>
<li>What this post is about:
<ul>
<li>This post will divide the key operations in <strong>modelling</strong> and how they fit <code>tidymodels</code></li>
<li>It will describe the specific functions that perform each step</li>
<li>It will describe what I think the current workflow is missing</li>
</ul></li>
</ul>
<p>This post is slightly longer than my usual posts, so here’s the <em>too long don’t read</em> version of the workflow:</p>
<pre class="r"><code>library(AmesHousing)
# devtools::install_github(&quot;tidymodels/tidymodels&quot;)
library(tidymodels)

ames &lt;- make_ames()

############################# Data Partitioning ###############################
###############################################################################

ames_split &lt;- rsample::initial_split(ames, prop = .7)
ames_train &lt;- rsample::training(ames_split)
ames_cv &lt;- rsample::vfold_cv(ames_train)

############################# Preprocessing ###################################
###############################################################################

mod_rec &lt;-
  recipes::recipe(Sale_Price ~ Longitude + Latitude + Neighborhood,
                  data = ames_train) %&gt;%
  recipes::step_log(Sale_Price, base = 10) %&gt;%
  recipes::step_other(Neighborhood, threshold = 0.05) %&gt;%
  recipes::step_dummy(recipes::all_nominal())


############################# Model Training/Tuning ###########################
###############################################################################

## Define a regularized regression and explicitly leave the tuning parameters
## empty for later tuning.
lm_mod &lt;-
  parsnip::linear_reg(penalty = tune::tune(), mixture = tune::tune()) %&gt;%
  parsnip::set_engine(&quot;glmnet&quot;)

## Construct a workflow that combines your recipe and your model
ml_wflow &lt;-
  workflows::workflow() %&gt;%
  workflows::add_recipe(mod_rec) %&gt;%
  workflows::add_model(lm_mod)

# Find best tuned model
res &lt;-
  ml_wflow %&gt;%
  tune::tune_grid(resamples = ames_cv,
                  grid = 10,
                  metrics = yardstick::metric_set(yardstick::rmse))

############################# Validation ######################################
###############################################################################
# Select best parameters
best_params &lt;-
  res %&gt;%
  tune::select_best(metric = &quot;rmse&quot;, maximize = FALSE)

# Refit using the entire training data
reg_res &lt;-
  ml_wflow %&gt;%
  tune::finalize_workflow(best_params) %&gt;%
  parsnip::fit(data = ames_train)

# Predict on test data
ames_test &lt;- rsample::testing(ames_split)
reg_res %&gt;%
  parsnip::predict(new_data = recipes::bake(mod_rec, ames_test)) %&gt;%
  bind_cols(ames_test, .) %&gt;%
  mutate(Sale_Price = log10(Sale_Price)) %&gt;% 
  select(Sale_Price, .pred) %&gt;% 
  rmse(Sale_Price, .pred)</code></pre>
<p>and here’s what I think it should look like in pseudocode:</p>
<pre class="r"><code>############################# Pseudocode ######################################
###############################################################################

library(AmesHousing)
# devtools::install_github(&quot;tidymodels/tidymodels&quot;)
library(tidymodels)

ames &lt;- make_ames()

ml_wflow &lt;-
  # Original data (unsplit)
  ames %&gt;%
  workflow() %&gt;%
  # Split test/train
  initial_split(prop = .75) %&gt;%
  # Specify cross-validation
  vfold_cv() %&gt;%
  # Start preprocessing
  recipe(Sale_Price ~ Longitude + Latitude + Neighborhood) %&gt;%
  step_log(Sale_Price, base = 10) %&gt;%
  step_other(Neighborhood, threshold = 0.05) %&gt;%
  step_dummy(recipes::all_nominal()) %&gt;%
  # Define model
  linear_reg(penalty = tune(), mixture = tune()) %&gt;%
  set_engine(&quot;glmnet&quot;) %&gt;%
  # Define grid of tuning parameters
  tune_grid(grid = 10)

# ml_wflow shouldn&#39;t run anything -- it&#39;s just a specification
# of all the different steps. `fit` should run everything
ml_wflow &lt;- fit(ml_wflow)

# Plot results of tuning parameters
ml_wflow %&gt;%
  autoplot()

# Automatically extract best parameters and fit to the training data
final_model &lt;-
  ml_wflow %&gt;%
  fit_best_model(metrics = metric_set(rmse))

# Predict on the test data using the last model
# Everything is bundled into a workflow object
# and everything can be extracted with separate
# functions with the same verb
final_model %&gt;%
  holdout_error()</code></pre>
<p>If you want more details on each step, continue reading :).</p>
<div id="a-data-science-workflow" class="section level2">
<h2>A Data Science Workflow</h2>
<p>Let’s recycle the operations I described above from <code>caret::train</code> and redefine them as general principles:</p>
<ul>
<li><strong>Data Preparation</strong>
<ul>
<li>Create a separate training set which represent 75% of the initial sample</li>
</ul></li>
<li><strong>Preprocessing (or Feature Engineering, for those liking fancy CS names)</strong>
<ul>
<li>Center and scale all predictors in the model</li>
</ul></li>
<li><strong>Model Training/Tuning</strong>
<ul>
<li>Identifies 10 alpha values (0 to 1) and then 10 additional lambda values</li>
<li>For each parameter set (1 alpha value and another lambda value), <a href="http://topepo.github.io/caret/model-training-and-tuning.html">run a cross-validation 10 times</a></li>
<li>Effectively run 1000 models (10 alpha * 10 alpha) each one cross-validated (10)</li>
<li>Record the validation metrics for each model on the assessment dataset</li>
</ul></li>
<li><strong>Validation</strong>
<ul>
<li>Save the best model in the result together with the optimized tuning parameters</li>
</ul></li>
</ul>
<p>Before we start, let’s load the two packages and data we’ll use:</p>
<pre class="r"><code>library(AmesHousing)
# devtools::install_github(&quot;tidymodels/tidymodels&quot;)
library(tidymodels)</code></pre>
<pre><code>## ── Attaching packages ────────────────────────────────────── tidymodels 0.0.4 ──</code></pre>
<pre><code>## ✔ broom     0.5.4          ✔ recipes   0.1.9     
## ✔ dials     0.0.4          ✔ rsample   0.0.5.9000
## ✔ dplyr     0.8.4          ✔ tibble    2.1.3     
## ✔ ggplot2   3.2.1          ✔ tune      0.0.1     
## ✔ infer     0.5.1          ✔ workflows 0.1.0.9000
## ✔ parsnip   0.0.5.9000     ✔ yardstick 0.0.5     
## ✔ purrr     0.3.3</code></pre>
<pre><code>## ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
## ✖ purrr::discard()    masks scales::discard()
## ✖ dplyr::filter()     masks stats::filter()
## ✖ dplyr::lag()        masks stats::lag()
## ✖ ggplot2::margin()   masks dials::margin()
## ✖ recipes::step()     masks stats::step()
## ✖ recipes::yj_trans() masks scales::yj_trans()</code></pre>
<pre class="r"><code>ames &lt;- make_ames()</code></pre>
</div>
<div id="data-preparation" class="section level2">
<h2>Data Preparation</h2>
<p>This step is performed by the <code>rsample</code> package. It allows you to do two basic things in machine learning: separate your training/test set and create resamples sets for tuning. Since nearly all machine learning modelling requires model tuning, I will create a cross-validation set in this example.</p>
<pre class="r"><code>ames_split &lt;- rsample::initial_split(ames, prop = .75)
ames_train &lt;- rsample::training(ames_split)
ames_cv &lt;- rsample::vfold_cv(ames_train)</code></pre>
<p>I believe the code above is quite easy to understand and (even if slightly more verbose than the <code>caret</code> equivalent) is quite elegant. For now, there are two things to keep in mind: we have a training set (<code>ames_train</code>) and we have a cross-validation set (<code>ames_cv</code>). We can forget about the testing set all together since it’ll be used in the end.</p>
</div>
<div id="preprocessing" class="section level2">
<h2>Preprocessing</h2>
<p><code>caret</code> takes care of doing the preprocessing behind the scenes while the user only needs to specify which steps are needed. In <code>tidymodels</code>, the <code>recipes</code> package takes care of preprocessing and you have to perform each step explicitly:</p>
<pre class="r"><code>mod_rec &lt;-
  recipes::recipe(Sale_Price ~ Longitude + Latitude + Neighborhood,
                  data = ames_train) %&gt;%
  recipes::step_log(Sale_Price, base = 10) %&gt;%
  recipes::step_other(Neighborhood, threshold = 0.05) %&gt;%
  recipes::step_dummy(recipes::all_nominal())</code></pre>
<p>I find this preprocessing statement very intuitive as well. You define the formula for your analysis, provide the training dataset and then apply whatever transformation to the prediction variables. So far the workflow is simple but growing:</p>
<p><code>Divide training set</code> -&gt; <code>Define model formula</code> -&gt; <code>Specify the data is the training set</code> -&gt; <code>Apply preprocessing</code></p>
<p>Previously, <code>recipes</code> was a bit confusing because there were steps which are not easy to remember: <code>prep</code> the dataset and <code>juice</code> or <code>bake</code> it depending on what you want to do (even more verbose and complex when applying this to a cross-validation set). With the <code>workflows</code> package, these steps have been completely eliminated from the users mental load.</p>
</div>
<div id="model-trainingtuning" class="section level2">
<h2>Model Training/Tuning</h2>
<p>Model training and tuning is the step on which I think <code>tidymodels</code> brings in too many moving parts. This has been partially ameliorated with <code>workflows</code>. For this step there are three to four packages: <code>parsnip</code> for modelling, <code>workflows</code> for creating modelling workflows, <code>tune</code> for tuning models and <code>yardstick</code> for validating the results. Let’s see how they fit together:</p>
<pre class="r"><code>## Define a regularized regression and explicitly leave the tuning parameters
## empty for later tuning.
lm_mod &lt;-
  parsnip::linear_reg(penalty = tune::tune(), mixture = tune::tune()) %&gt;%
  parsnip::set_engine(&quot;glmnet&quot;)

## Construct a workflow that combines your recipe and your model
ml_wflow &lt;-
  workflows::workflow() %&gt;%
  workflows::add_recipe(mod_rec) %&gt;%
  workflows::add_model(lm_mod)</code></pre>
<p>The expression above adds much more flexibility as you can swap models by just changing the <code>linear_reg</code> to another model. However, it also adds more complexity. <code>tune()</code> requires you to know about <code>parameters()</code> to extract the parameters to create the grid. For that you have to be aware of the <code>grid_*</code> functions to create a grid of values. However, this comes from the <code>dials</code> package and not the <code>tune</code> package. However, all of these moving parts return different things, so they’re not very easy to remember at first glance.</p>
<p>Having said that, the actual tuning is done with <code>tune_grid</code> where we specify the cross-validated set from the first step. Here <code>tune_grid</code> is quite elegant since it allows you specify a grid of values or an integer which it will use to create a grid of parameters:</p>
<pre class="r"><code>res &lt;-
  ml_wflow %&gt;%
  tune::tune_grid(resamples = ames_cv,
                  grid = 10,
                  metrics = yardstick::metric_set(yardstick::rmse))</code></pre>
<p>And finally, you can get the summary of the metrics with <code>collect_metrics</code>:</p>
<pre class="r"><code>res %&gt;%
  tune::collect_metrics()</code></pre>
<pre><code>## # A tibble: 10 x 7
##     penalty mixture .metric .estimator  mean     n std_err
##       &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;
##  1 4.99e-10   0.577 rmse    standard   0.141    10 0.00327
##  2 3.11e- 9   0.655 rmse    standard   0.141    10 0.00327
##  3 2.74e- 8   0.476 rmse    standard   0.141    10 0.00327
##  4 1.86e- 7   0.795 rmse    standard   0.141    10 0.00327
##  5 8.39e- 6   0.976 rmse    standard   0.141    10 0.00327
##  6 8.47e- 5   0.177 rmse    standard   0.141    10 0.00327
##  7 6.00e- 4   0.394 rmse    standard   0.141    10 0.00327
##  8 4.45e- 3   0.268 rmse    standard   0.141    10 0.00329
##  9 1.28e- 2   0.143 rmse    standard   0.142    10 0.00331
## 10 1.66e- 1   0.863 rmse    standard   0.175    10 0.00387</code></pre>
<p>Or choose the best parameters with <code>select_best</code>:</p>
<pre class="r"><code>best_params &lt;-
  res %&gt;%
  tune::select_best(metric = &quot;rmse&quot;, maximize = FALSE)

best_params</code></pre>
<pre><code>## # A tibble: 1 x 2
##     penalty mixture
##       &lt;dbl&gt;   &lt;dbl&gt;
## 1 0.0000847   0.177</code></pre>
</div>
<div id="validation" class="section level2">
<h2>Validation</h2>
<p>The final step is to extract the best model and contrast the training and test error. Here <code>workflows</code> offers some convenience to replace the model with the best parameters and fit the complete training data with the best parameters. This step is currently completely automatized with <code>train</code> where you can extract the best model even after exploring the results of different tuning parameters.</p>
<pre class="r"><code>reg_res &lt;-
  ml_wflow %&gt;%
  # Attach the best tuning parameters to the model
  tune::finalize_workflow(best_params) %&gt;%
  # Fit the final model to the training data
  parsnip::fit(data = ames_train)

ames_test &lt;- rsample::testing(ames_split)

reg_res %&gt;%
  predict(new_data = ames_test) %&gt;%
  bind_cols(ames_test, .) %&gt;%
  mutate(Sale_Price = log10(Sale_Price)) %&gt;% 
  select(Sale_Price, .pred) %&gt;% 
  yardstick::rmse(Sale_Price, .pred)</code></pre>
<pre><code>## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       0.139</code></pre>
<p>One of the things I don’t like about <code>fit</code> for this current scenario is that I have to think about specifying the training data again. I understand that the data specified in <code>recipe</code> could be even an empty data frame, as it is used only to detect the column names. However, in nearly all the applications I can think of, I will specify the training data at the beginning (in my recipe). So I find that having to specify the data again is a step that can be eliminated altogether if the data is in the workflow.</p>
</div>
<div id="what-to-remember" class="section level2">
<h2>What to remember</h2>
<p>There are many things to remember from the workflow above. Below is a kind of cheatsheet:</p>
<ul>
<li><strong>Data Preparation</strong>
<ul>
<li><code>rsample::initial_split</code>: splits your data into training/testing</li>
<li><code>rsample::training</code>: extract the training data</li>
<li><code>rsample::vfold_cv</code>: create a cross-validated set from the training data</li>
</ul></li>
<li><strong>Preprocessing (or Feature Engineering, for those liking fancy CS names)</strong>
<ul>
<li><code>recipes::recipe</code>: define your formula with the training data</li>
<li><code>recipes::step_*</code>: add any preprocessing steps your data</li>
</ul></li>
<li><strong>Model Training/Tuning</strong>
<ul>
<li><code>parsnip::linear_reg</code>: define your model. This example shows a linear regression but it could be anything else (random forest)</li>
<li><code>tune::tune</code>: leave the tuning parameters empty for later</li>
<li><code>parsnip::set_engine</code>: set the engine to run the models (which package to use)</li>
<li><code>workflows::workflow</code>: create a workflow object to hold your model/recipe</li>
<li><code>workflows::add_recipe</code>: add the recipe to your workflow</li>
<li><code>workflows::add_model</code>: add the model to your workflow</li>
<li><code>yardstick::metric_set</code>: create a set of metrics</li>
<li><code>yardstick::rmse</code>: specify the root-mean-square-error as the loss function</li>
<li><code>tune::tune_grid</code> run the workflow across all resamples with the desired tuning parameters</li>
<li><code>tune::collect_metrics</code>: collect which are the best tuning parameters</li>
<li><code>tune::select_best</code>: select the best tuning parameter</li>
</ul></li>
<li><strong>Validation</strong>
<ul>
<li><code>tune::finalize_workflow</code>: replace the empty parameters of the model with the best tuned parameters</li>
<li><code>parsnip::fit</code>: fit the final model to the training data</li>
<li><code>rsample::testing</code>: extract the testing data from the initial split</li>
<li><code>parsnip::predict</code>: predict the trained model on the testing data</li>
</ul></li>
</ul>
<p>This is currently what I think is the simplest workflow to train models in <code>tidymodels</code>. This is of course a very simplified example which doesn’t create tuning grids or tune parameters in the recipes. This is supposed to be the barebones workflow that is currently available in <code>tidymodels</code>. Having said that, I still think there are too many steps which makes the workflow convoluted.</p>
</div>
<div id="thoughts-on-the-workflow" class="section level2">
<h2>Thoughts on the workflow</h2>
<p><code>tidymodels</code> is currently being designed to be decoupled into several packages and the key steps for modelling are currently implemented. This offers greater flexibility for defining models, making some of the steps in modelling less obscure and explicit.</p>
<p>Having said that, there is too much to remember. <code>dplyr::select</code> is a function which is easy to remember because it can be thought of as an independent entity which I can use with a <code>data.table</code> or base <code>R</code>. On top of that, I know it follows the general principle of the <code>tidyverse</code> where it only accepts a data frame and only returns a data frame. This makes it much more memorable. Due to its simplicity, it’s easy to think of it like a hammer: I can apply it to so many different problems that I don’t have to memorize it, it becomes a general tool that represents an abstract idea.</p>
<p>Some of the functions/packages from <code>tidymodels</code> are difficult to think like that. I believe this is because they are supposed to be almost always used together, otherwise they have no practical applications. <code>tune</code>, <code>workflows</code> and <code>parsnip</code> introduce several ideas which I think are difficult to remember (mainly because you have to <strong>remember</strong> them and they don’t come off naturally, as an abstract concept).</p>
<p><code>workflows</code> seems to be a package that combines some of the steps performed by <code>parsnip</code> and <code>recipes</code>, suggesting that you can build a logical workflow with it. However, <code>workflows</code> is introduced <strong>after</strong> you define your preprocessing and model. My intuition would tell me that the workflow should begin at first rather than in the middle. For example, in pseucode a logical workflow could look like this:</p>
<pre class="r"><code>ml_wflow &lt;-
  # Original data (unsplit)
  ames %&gt;%
  # Begin workflow
  workflow() %&gt;%
  # No need to extract training/testing, they&#39;re already in the workflow
  # This eliminates the mental load of mixing up training/testing and
  # mistakenly predict one over the other.
  initial_split(prop = .75) %&gt;%
  # Apply directly the cross-validation to the training set. No resaving
  # the data into different names, adding more and more objects to remember
  vfold_cv() %&gt;%
  # Introduce preprocessing
  # No need to specify the data, the training data is already inside
  # the workflow. This simplifies having to specify your training
  # data in many different places (recipes, fit, vfold_cv). The data
  # was specified at the beginning and that&#39;s it.
  recipe(Sale_Price ~ Longitude + Latitude + Neighborhood) %&gt;%
  step_log(Sale_Price, base = 10) %&gt;%
  step_other(Neighborhood, threshold = 0.05) %&gt;%
  step_dummy(recipes::all_nominal()) %&gt;%
  # Add your model definition and include placeholders for your tuning
  # parameters
  linear_reg(penalty = tune(), mixture = tune()) %&gt;%
  set_engine(&quot;glmnet&quot;)</code></pre>
<p>I believe the code above is much more logical than the current setup for three reasons which are very much related to each other.</p>
<p>First, it follows the ‘traditional’ workflow of machine learning more clearly without intermediate steps. You begin with your data and add the key modelling steps one by one. Second, it avoids creating too many intermediate steps which add mental load. Whenever I’m using <code>tidymodels</code> I have to remember so many things: the training data, the cross-validated set, the recipe, the tuning grid, the model, etc. I often forget what I need to add to <code>tune_grid</code>: is it the recipe and the resample set? Is it the workflow? Did I mistakenly add the test set to the recipe and fit the data with the training set? It’s very easy to get lost along the way. And third, I think the workflow from above fits with the <code>tidyverse</code> philosophy much better, where you can read the steps from left to right, in a linear fashion.</p>
<p>The power of the pseudocode above is that the workflow is thought of as the holder of your workflow since the beginning, meaning you can add or remove stuff from it. For example, it would very easy to add <strong>another model</strong> to be compared:</p>
<pre class="r"><code>ml_wflow &lt;-
  # Original data (unsplit)
  ames %&gt;%
  workflow() %&gt;%
  initial_split(prop = .75) %&gt;%
  vfold_cv() %&gt;%
  recipe(Sale_Price ~ Longitude + Latitude + Neighborhood) %&gt;%
  step_log(Sale_Price, base = 10) %&gt;%
  step_other(Neighborhood, threshold = 0.05) %&gt;%
  step_dummy(recipes::all_nominal()) %&gt;%
  linear_reg(penalty = tune(), mixture = tune()) %&gt;%
  set_engine(&quot;glmnet&quot;) %&gt;%
  # Adds another model
  rand_forest(mtry = tune(), tress = tune(), min_n = tune()) %&gt;%
  set_engine(&quot;rf&quot;)</code></pre>
<p>The code above could also include additional steps for adding tuning grids for each model and then a final call to <code>fit</code> would fit all models/tuning parameters directly into the cross-validated set. Additionally, since the original data is in the workflow, methods for fitting the best model to the complete training data could be implemented as well as methods for running the best tuned model on the test data. No objects laying around to remember and everything is unified into a bundle of logical steps which begin with your data.</p>
<p>This workflow idea doesn’t introduce anything new programatically in <code>tidymodels</code>: all ingredients are currently implemented. The idea is to rearrange specific methods to handle a workflow in this fashion. <em>This workflow idea is just a prototype idea and I’m sure that many things can be improved</em>. I do think, however, that this is the direction which would make <code>tidymodels</code> a truly friendly interface. At least to me, it would make it as easy to use as the <code>tidyverse</code>.</p>
</div>
<div id="wrap-up" class="section level2">
<h2>Wrap-up</h2>
<p>This post is intended to be thought-provoking take on the current development of <code>tidymodels</code>. I’m a big fan of RStudio and their work and I’m looking forward to the “official release” of <code>tidymodels</code>. I wrote this piece with the intention of understanding the currently workflow but noticed that I’m not comfortable with it, nor did it come off naturally. I hope these ideas can help exemplify some of the bottlenecks that future <code>tidymodels</code> users can face with the aim of improving the user experience of the modelling framework from <code>tidymodels</code>.</p>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>Locating parts of a string with `stringr`</title>
      <link>/blog/2019-12-08-locating-parts-of-a-string-with-stringr/locating-parts-of-a-string-with-stringr/</link>
      <pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-12-08-locating-parts-of-a-string-with-stringr/locating-parts-of-a-string-with-stringr/</guid>
      <description><![CDATA[
      


<p>I was wondering the realms of StackOver Flow answering some questions when I encoutered a question that looked to extract some parts of a string based on a regex. I thought I knew how to do this with the package <code>stringr</code> using, for example, <code>str_sub</code> but I found it a bit difficult to map how <code>str_locate</code> complements <code>str_sub</code>.</p>
<p><code>str_locate</code> and <code>str_locate_all</code> give back the locations of your regex inside the desired string as a <code>matrix</code> or a <code>list</code> respectively. However, that didn’t look very intuitive to pass on to <code>str_sub</code> which (I thought) only accepted numeric vectors with the indices of the parts of the strings that you want to extract. However, to my surprise, <code>str_sub</code> accepts not only numeric vectors but also a matrix with two columns, precisely the result of <code>str_locate</code>.</p>
<p>Let’s create a set of random strings from which we want to extract the word <code>special*word</code>, where <code>*</code> represents a random number.</p>
<pre class="r"><code>library(stringr)    

test_string &lt;-
  replicate(
    100,
    paste0(
      sample(c(letters, LETTERS, paste0(&quot;special&quot;, sample(1:10, 1),&quot;word&quot;)), 15),
      collapse = &quot;&quot;)
  )

head(test_string)</code></pre>
<pre><code>## [1] &quot;pZTQHcDVObnaCFS&quot;             &quot;qBxfbIHjauyEmgspecial10word&quot;
## [3] &quot;TKgbmQAEFoJHOVh&quot;             &quot;VoBdUAuzfPrmCGX&quot;            
## [5] &quot;dGgJOspecial5wordiFpbvXzUD&quot;  &quot;WOfLjNospecial4wordEeGkyTA&quot;</code></pre>
<p>Using <code>str_locate</code> returns a matrix with the positions of all matches for <strong>every string</strong>. This is what’s called <strong>vectorised</strong> functions in R.</p>
<pre class="r"><code>location_matrix &lt;-
  str_locate(test_string, pattern =  &quot;special[0-9]word&quot;)

head(location_matrix)</code></pre>
<pre><code>##      start end
## [1,]    NA  NA
## [2,]    NA  NA
## [3,]    NA  NA
## [4,]    NA  NA
## [5,]     6  17
## [6,]     8  19</code></pre>
<p>For this example this wouldn’t work, but I was also interested in checking how the result of <code>str_locate_all</code> would fit in this workflow. <code>str_locate_all</code> is the same as <code>str_locate</code> but since it can find <strong>more</strong> than one match per string, it returns a list with the same slots as there are strings in <code>test_string</code> with a matrix per slot showing the indices of the matches. Since many of the strings in <code>test_string</code> might not have <code>special*word</code>, we need to fill out those matches with <code>NA</code>:</p>
<pre class="r"><code>location_list &lt;-
  str_locate_all(test_string, pattern =  &quot;special[0-9]word&quot;) %&gt;% 
  lapply(function(.x) if (all(is.na(.x))) matrix(c(NA, NA), ncol = 2) else .x) %&gt;%
  {do.call(rbind, .)}

head(location_list)</code></pre>
<pre><code>##      start end
## [1,]    NA  NA
## [2,]    NA  NA
## [3,]    NA  NA
## [4,]    NA  NA
## [5,]     6  17
## [6,]     8  19</code></pre>
<p>Now that we have everything ready, <code>str_sub</code> can give our desires results using both numeric vectors as well as the entire matrix:</p>
<pre class="r"><code># Using numeric vectors from str_locate
str_sub(test_string, location_matrix[, 1], location_matrix[, 2])</code></pre>
<pre><code>##   [1] NA             NA             NA             NA             &quot;special5word&quot;
##   [6] &quot;special4word&quot; NA             NA             &quot;special5word&quot; NA            
##  [11] NA             NA             NA             NA             NA            
##  [16] NA             NA             NA             NA             NA            
##  [21] NA             NA             NA             &quot;special5word&quot; &quot;special6word&quot;
##  [26] NA             NA             NA             NA             NA            
##  [31] &quot;special4word&quot; NA             NA             NA             NA            
##  [36] NA             NA             NA             &quot;special7word&quot; NA            
##  [41] NA             NA             NA             NA             NA            
##  [46] NA             NA             NA             NA             NA            
##  [51] NA             NA             NA             NA             NA            
##  [56] NA             NA             NA             NA             NA            
##  [61] NA             NA             &quot;special4word&quot; NA             NA            
##  [66] NA             NA             NA             NA             NA            
##  [71] NA             NA             NA             &quot;special7word&quot; &quot;special9word&quot;
##  [76] NA             NA             NA             NA             NA            
##  [81] &quot;special4word&quot; NA             NA             &quot;special5word&quot; NA            
##  [86] NA             NA             NA             &quot;special9word&quot; &quot;special9word&quot;
##  [91] NA             NA             NA             NA             NA            
##  [96] &quot;special6word&quot; NA             NA             &quot;special3word&quot; &quot;special1word&quot;</code></pre>
<pre class="r"><code># Using numeric vectors from str_locate_all
str_sub(test_string, location_list[, 1], location_list[, 2])</code></pre>
<pre><code>##   [1] NA             NA             NA             NA             &quot;special5word&quot;
##   [6] &quot;special4word&quot; NA             NA             &quot;special5word&quot; NA            
##  [11] NA             NA             NA             NA             NA            
##  [16] NA             NA             NA             NA             NA            
##  [21] NA             NA             NA             &quot;special5word&quot; &quot;special6word&quot;
##  [26] NA             NA             NA             NA             NA            
##  [31] &quot;special4word&quot; NA             NA             NA             NA            
##  [36] NA             NA             NA             &quot;special7word&quot; NA            
##  [41] NA             NA             NA             NA             NA            
##  [46] NA             NA             NA             NA             NA            
##  [51] NA             NA             NA             NA             NA            
##  [56] NA             NA             NA             NA             NA            
##  [61] NA             NA             &quot;special4word&quot; NA             NA            
##  [66] NA             NA             NA             NA             NA            
##  [71] NA             NA             NA             &quot;special7word&quot; &quot;special9word&quot;
##  [76] NA             NA             NA             NA             NA            
##  [81] &quot;special4word&quot; NA             NA             &quot;special5word&quot; NA            
##  [86] NA             NA             NA             &quot;special9word&quot; &quot;special9word&quot;
##  [91] NA             NA             NA             NA             NA            
##  [96] &quot;special6word&quot; NA             NA             &quot;special3word&quot; &quot;special1word&quot;</code></pre>
<pre class="r"><code># Using the entire matrix
str_sub(test_string, location_matrix)</code></pre>
<pre><code>##   [1] NA             NA             NA             NA             &quot;special5word&quot;
##   [6] &quot;special4word&quot; NA             NA             &quot;special5word&quot; NA            
##  [11] NA             NA             NA             NA             NA            
##  [16] NA             NA             NA             NA             NA            
##  [21] NA             NA             NA             &quot;special5word&quot; &quot;special6word&quot;
##  [26] NA             NA             NA             NA             NA            
##  [31] &quot;special4word&quot; NA             NA             NA             NA            
##  [36] NA             NA             NA             &quot;special7word&quot; NA            
##  [41] NA             NA             NA             NA             NA            
##  [46] NA             NA             NA             NA             NA            
##  [51] NA             NA             NA             NA             NA            
##  [56] NA             NA             NA             NA             NA            
##  [61] NA             NA             &quot;special4word&quot; NA             NA            
##  [66] NA             NA             NA             NA             NA            
##  [71] NA             NA             NA             &quot;special7word&quot; &quot;special9word&quot;
##  [76] NA             NA             NA             NA             NA            
##  [81] &quot;special4word&quot; NA             NA             &quot;special5word&quot; NA            
##  [86] NA             NA             NA             &quot;special9word&quot; &quot;special9word&quot;
##  [91] NA             NA             NA             NA             NA            
##  [96] &quot;special6word&quot; NA             NA             &quot;special3word&quot; &quot;special1word&quot;</code></pre>
<p>A much easier approach to doing the above (which is cumbersome and verbose) is to use <code>str_extract</code>:</p>
<pre class="r"><code>str_extract(test_string, &quot;special[0-9]word&quot;)</code></pre>
<pre><code>##   [1] NA             NA             NA             NA             &quot;special5word&quot;
##   [6] &quot;special4word&quot; NA             NA             &quot;special5word&quot; NA            
##  [11] NA             NA             NA             NA             NA            
##  [16] NA             NA             NA             NA             NA            
##  [21] NA             NA             NA             &quot;special5word&quot; &quot;special6word&quot;
##  [26] NA             NA             NA             NA             NA            
##  [31] &quot;special4word&quot; NA             NA             NA             NA            
##  [36] NA             NA             NA             &quot;special7word&quot; NA            
##  [41] NA             NA             NA             NA             NA            
##  [46] NA             NA             NA             NA             NA            
##  [51] NA             NA             NA             NA             NA            
##  [56] NA             NA             NA             NA             NA            
##  [61] NA             NA             &quot;special4word&quot; NA             NA            
##  [66] NA             NA             NA             NA             NA            
##  [71] NA             NA             NA             &quot;special7word&quot; &quot;special9word&quot;
##  [76] NA             NA             NA             NA             NA            
##  [81] &quot;special4word&quot; NA             NA             &quot;special5word&quot; NA            
##  [86] NA             NA             NA             &quot;special9word&quot; &quot;special9word&quot;
##  [91] NA             NA             NA             NA             NA            
##  [96] &quot;special6word&quot; NA             NA             &quot;special3word&quot; &quot;special1word&quot;</code></pre>
<p>However, the whole objecive behind this exercise was to clearly map out how to connect <code>str_locate</code> to <code>str_sub</code> and it’s much clearer if you can pass the entire matrix. However, converting <code>str_locate_all</code> is still a bit tricky.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>essurvey release</title>
      <link>/blog/2019-11-15-release-essurvey/essurvey-release/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-11-15-release-essurvey/essurvey-release/</guid>
      <description><![CDATA[
      


<p>The new <code>essurvey</code> 1.0.3 is here! This release is mainly about downloading weight data from the European Social Survey (ESS), which <a href="https://github.com/ropensci/essurvey/issues/9">has been on the works since</a> 2017! As usual, you can install from CRAN or Github with:</p>
<pre class="r"><code># From CRAN
install.packages(&quot;essurvey&quot;)

# or development version from Github
devtools::install_github(&quot;ropensci/essurvey&quot;)

# and load
library(essurvey)
set_email(&quot;your@email.com&quot;)</code></pre>
<p>Remember to set your registered email with <code>set_email</code> to download ESS data. This is as easy as running <code>set_email("your@email.com")</code>, with your email. The package now has two main functions to download weight data (called SDDF by the ESS): <code>show_sddf_cntrounds</code> and <code>import_sddf_country</code>. The first one returns the available weight rounds for a specific country. For example, for which rounds does Italy have weight data?</p>
<pre class="r"><code>ita_rnds &lt;- show_sddf_cntrounds(&quot;Italy&quot;)

ita_rnds</code></pre>
<pre><code>## [1] 6 8</code></pre>
<p>How about Germany?</p>
<pre class="r"><code>show_sddf_cntrounds(&quot;Germany&quot;)</code></pre>
<pre><code>## [1] 1 2 3 4 5 6 7 8</code></pre>
<p>For some rounds, some countries used complete random sampling, so they didn’t need any weight data for correct estimation. Italy did not use a random sample for round 8 so let’s focus on that wave for the example. To actually download this round, we use <code>import_sddf_country</code>:</p>
<pre class="r"><code># Download weight data
ita_dt &lt;- import_sddf_country(&quot;Italy&quot;, 8)

ita_dt</code></pre>
<pre><code>## # A tibble: 2,626 x 10
##    name  essround edition proddate cntry  idno   psu domain stratum    prob
##    &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 ESS8…        8 1.2     11.02.2… IT        1 11029      2     658 1.01e-4
##  2 ESS8…        8 1.2     11.02.2… IT        2 11170      2     665 1.11e-4
##  3 ESS8…        8 1.2     11.02.2… IT        4 11127      2     660 1.03e-4
##  4 ESS8…        8 1.2     11.02.2… IT        5 10771      2     671 1.04e-4
##  5 ESS8…        8 1.2     11.02.2… IT        6 11148      2     666 1.06e-4
##  6 ESS8…        8 1.2     11.02.2… IT        9 11163      1     667 1.05e-4
##  7 ESS8…        8 1.2     11.02.2… IT       14 11183      1     657 1.06e-4
##  8 ESS8…        8 1.2     11.02.2… IT       15 11184      2     661 9.97e-5
##  9 ESS8…        8 1.2     11.02.2… IT       16 10928      2     652 1.01e-4
## 10 ESS8…        8 1.2     11.02.2… IT       22 11171      2     664 9.97e-5
## # … with 2,616 more rows</code></pre>
<p>Notice that the weight data has an <code>idno</code> column. This column can be used to match each respondent from each country to the main ESS data. This means that you can now actually do proper weighted analysis using the ESS data on the fly! How would we match the data for Italy, for example?</p>
<p>We download the main data:</p>
<pre class="r"><code>library(dplyr)

# Download main data
ita_main &lt;- import_country(&quot;Italy&quot;, 8)</code></pre>
<p>And then merge it with the weight data:</p>
<pre class="r"><code># Let&#39;s keep only the important weight columns
ita_dt &lt;- ita_dt %&gt;% select(idno, psu, domain, stratum, prob)

# Merged main data and weight data
complete_data &lt;- inner_join(ita_main, ita_dt, by = &quot;idno&quot;)</code></pre>
<pre><code>## Warning: Column `idno` has different attributes on LHS and RHS of join</code></pre>
<pre class="r"><code># There we have the matched data
complete_data %&gt;%
  select(essround, idno, cntry, psu, stratum, prob)</code></pre>
<pre><code>## # A tibble: 2,626 x 6
##    essround  idno cntry   psu stratum      prob
##       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;
##  1        8     1 IT    11029     658 0.000101 
##  2        8     2 IT    11170     665 0.000111 
##  3        8     4 IT    11127     660 0.000103 
##  4        8     5 IT    10771     671 0.000104 
##  5        8     6 IT    11148     666 0.000106 
##  6        8     9 IT    11163     667 0.000105 
##  7        8    14 IT    11183     657 0.000106 
##  8        8    15 IT    11184     661 0.0000997
##  9        8    16 IT    10928     652 0.000101 
## 10        8    22 IT    11171     664 0.0000997
## # … with 2,616 more rows</code></pre>
<p>There we have the matched data! This can be easily piped to the <code>survey</code> package and perform properly weighted analysis of the ESS data. In fact, an official ESS package for analyzing data is something we’re thinking of developing to making analyzing ESS data very easy.</p>
<p>Weight data (or SDDF data) is a bit tricky because not all country/rounds data have the same extension (some have SPSS, some have Stata, etc..) nor the same format (number of columns, name of columns, etc..). We would appreciate if you can submit any errors you find on <a href="https://github.com/ropensci/essurvey/issues">Github</a> and we’ll try taking care of them as soon as possible.</p>
<p>Special thanks to <a href="https://twitter.com/phnk?lang=en">phnk</a>, <a href="https://twitter.com/djhurio/">djhurio</a> and Stefan Zins for helping out to push this.</p>
<p>Enjoy this new release!</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Saving missing categories from R to Stata</title>
      <link>/blog/2019-03-16-saving-missing-categories-from-r-to-stata/saving-missing-categories-from-r-to-stata/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-03-16-saving-missing-categories-from-r-to-stata/saving-missing-categories-from-r-to-stata/</guid>
      <description><![CDATA[
      


<p>I’m finishing a project from the RECSM institute where we developed a <a href="https://essurvey.shinyapps.io/ess_castellano/">Shiny application</a> to download data from the European Social Survey with Spanish translated labels. This was one hell of a project since I had to build some wrappers around the Google Translate API to generate translations for over 1300 questions and stream line this to be interactive while users download the data. That’s a long story which I will not delve into.</p>
<p>This post is about a bug I found in the <code>haven</code> package while doing the project. The bug is simple to explain and <a href="https://github.com/tidyverse/haven/issues/435">I filed it in <code>haven</code> already</a>:</p>
<p>Let’s define a labelled double with only one tagged NA value.</p>
<pre class="r"><code>library(haven)
#&gt; Warning: package &#39;haven&#39; was built under R version 3.4.4

tst &lt;-
  labelled(
    c(
      1:5,
      tagged_na(&quot;d&quot;)
    ),
    c(
      &quot;Agree Strongly&quot; = 1,
      &quot;Agree&quot; = 2,
      &quot;Neither agree nor disagree&quot; = 3,
      &quot;Disagree&quot; = 4,
      &quot;Disagree strongly&quot; = 5,
      &quot;No answer&quot; = tagged_na(&quot;d&quot;)
    )
  )

tst</code></pre>
<pre><code>## &lt;Labelled double&gt;
## [1]     1     2     3     4     5 NA(d)
## 
## Labels:
##  value                      label
##      1             Agree Strongly
##      2                      Agree
##      3 Neither agree nor disagree
##      4                   Disagree
##      5          Disagree strongly
##  NA(d)                  No answer</code></pre>
<pre class="r"><code>write_dta(data.frame(freehms = tst), &quot;test.dta&quot;, version = 13)</code></pre>
<p>If I load this in Stata and type tab freehms, all labels are correct:</p>
<p><img src="/img/stata1.png" /></p>
<p>Now, if I take the code above and add another tagged NA value, then <code>write_dta</code> drops the last label for some reason:</p>
<pre class="r"><code>library(haven)

tst &lt;-
  labelled(c(1:5,
             tagged_na(&#39;d&#39;),
             ## Only added this
             tagged_na(&#39;c&#39;)
          ),
        c(&#39;Agree Strongly&#39; = 1,
          &#39;Agree&#39; = 2,
          &#39;Neither agree nor disagree&#39; = 3,
          &#39;Disagree&#39; = 4,
          &#39;Disagree strongly&#39; = 5,
          &#39;No answer&#39; = tagged_na(&#39;d&#39;),
            ## And this
          &#39;Dont know&#39; = tagged_na(&#39;c&#39;)
          )
        )

tst</code></pre>
<pre><code>## &lt;Labelled double&gt;
## [1]     1     2     3     4     5 NA(d) NA(c)
## 
## Labels:
##  value                      label
##      1             Agree Strongly
##      2                      Agree
##      3 Neither agree nor disagree
##      4                   Disagree
##      5          Disagree strongly
##  NA(d)                  No answer
##  NA(c)                  Dont know</code></pre>
<pre class="r"><code>write_dta(data.frame(freehms = tst), &quot;test.dta&quot;, version = 13)</code></pre>
<p><img src="/img/stata2.png" /></p>
<p>Well, the bug is evident (notice the 5 without a label?). However, since the project is on a deadline I had to come up with a solution. It’s very simple: avoid tagged NA’s but recode them as traditional labels. Here’s a solution:</p>
<pre class="r"><code>library(sjlabelled)
library(sjmisc)

# Labels tags present in the ESS data
old_label_names &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)

# Grab the labels with tagged NA&#39;s with a regex
na_available &lt;- unname(gsub(&quot;NA|\\(|\\)&quot;, &quot;&quot;, get_na(tst, TRUE)))

# Identify which of the existent labels are actually valid ESS missings
which_ones_use &lt;- old_label_names %in% na_available

# Subset only the ones which need recoding
value_code &lt;- c(666, 777, 888, 999)[which_ones_use]
new_label_names &lt;- c(&quot;.a&quot;, &quot;.b&quot;, &quot;.c&quot;, &quot;.d&quot;)[which_ones_use]

# Recode them
for (i in seq_along(na_available)) {
  tst &lt;- replace_na(tst,
                    value = value_code[i],
                    na.label = new_label_names[i],
                    tagged.na = na_available[i]
                    )
}

tst</code></pre>
<pre><code>## &lt;Labelled double&gt;
## [1]   1   2   3   4   5 888 999
## 
## Labels:
##  value                      label
##      1             Agree Strongly
##      2                      Agree
##      3 Neither agree nor disagree
##      4                   Disagree
##      5          Disagree strongly
##    888                         .c
##    999                         .d</code></pre>
<p>There we go. Those labels would clearly be interpreted as missings and Stata would read them as traditional labels (well, it’s not perfect, but it’s a workaround). What I did was wrap the above code into a function and apply it to all questions in all rounds (&gt; 1300!).</p>
<pre class="r"><code>recode_stata_labels &lt;- function(x) {
  # Labels tags present in the ESS data
  old_label_names &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)

  # Grab the labels with tagged NA&#39;s with a regex
  na_available &lt;- unname(gsub(&quot;NA|\\(|\\)&quot;, &quot;&quot;, get_na(x, TRUE)))

  # Identify which of the existent labels are actually valid ESS missings
  which_ones_use &lt;- old_label_names %in% na_available

  # Subset only the ones which need recoding
  value_code &lt;- c(666, 777, 888, 999)[which_ones_use]
  new_label_names &lt;- c(&quot;.a&quot;, &quot;.b&quot;, &quot;.c&quot;, &quot;.d&quot;)[which_ones_use]

  for (i in seq_along(na_available)) {
    x &lt;- replace_na(x,
                    value = value_code[i],
                    na.label = new_label_names[i],
                    tagged.na = na_available[i]
    )
  }

  x
}</code></pre>
<p>Now, what happens if a <code>labelled</code> class only has tagged NA’s?</p>
<pre class="r"><code>tst &lt;-
  labelled(c(1:5,
             tagged_na(&#39;d&#39;),
             tagged_na(&#39;c&#39;)
             ),
           c(&#39;No answer&#39; = tagged_na(&#39;d&#39;), &#39;Dont know&#39; = tagged_na(&#39;c&#39;)))

tst</code></pre>
<pre><code>## &lt;Labelled double&gt;
## [1]     1     2     3     4     5 NA(d) NA(c)
## 
## Labels:
##  value     label
##  NA(d) No answer
##  NA(c) Dont know</code></pre>
<pre class="r"><code>recode_stata_labels(tst)</code></pre>
<pre><code>## Error: `x` must be a double vector</code></pre>
<p>That’s weird. I was in such a rush that I didn’t really want to debug the source code in <code>haven</code>. However, I had the intuition that this was related to the fact that there were only tagged NA’s as labels. How do I fixed it? Just add a toy label at the beginning of the function and remove it after the recoding.</p>
<pre class="r"><code>recode_stata_labels &lt;- function(x) {
    # I add a random label (here) and delete it at the end (end of the function)
    x &lt;- add_labels(x, labels = c(&#39;test&#39; = 111111))

    # Note that this vector is in the same order as the `value_code` and `new_label_names`
    # because they&#39;re values correspond to each other in this order.
    old_label_names &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)

    na_available &lt;- unname(gsub(&quot;NA|\\(|\\)&quot;, &quot;&quot;, sjlabelled::get_na(x, TRUE)))
    which_ones_use &lt;- old_label_names %in% na_available

    value_code &lt;- c(666, 777, 888, 999)[which_ones_use]
    new_label_names &lt;- c(&quot;.a&quot;, &quot;.b&quot;, &quot;.c&quot;, &quot;.d&quot;)[which_ones_use]

    for (i in seq_along(na_available)) {
      x &lt;- replace_na(x, value = value_code[i], na.label = new_label_names[i], tagged.na = na_available[i])
    }

    x &lt;- remove_labels(x, labels = &quot;test&quot;)

  x
}

recode_stata_labels(tst)</code></pre>
<pre><code>## &lt;Labelled double&gt;
## [1]   1   2   3   4   5 888 999
## 
## Labels:
##  value label
##    888    .c
##    999    .d</code></pre>
<p>There we are. The <code>replace_na</code> function is actually doing most of the work and I found it extremely useful (comes from the <code>sjmisc</code> package).</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Why does R drop attributes when subsetting?</title>
      <link>/blog/2019-03-17-why-does-r-drop-attributes-when-subsetting/one-thing-i-hate-about-r/</link>
      <pubDate>Sat, 16 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-03-17-why-does-r-drop-attributes-when-subsetting/one-thing-i-hate-about-r/</guid>
      <description><![CDATA[
      


<p>I had to spend about 1 hour yesterday because R did something completely unpredictable (for my taste). It dropped an attribute without a warning.</p>
<pre class="r"><code>df &lt;- data.frame(x = rep(c(1, 2), 20))

attr(df$x, &quot;label&quot;) &lt;- &quot;This is clearly a label&quot;

df$x</code></pre>
<pre><code>##  [1] 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1
## [36] 2 1 2 1 2
## attr(,&quot;label&quot;)
## [1] &quot;This is clearly a label&quot;</code></pre>
<p>The label is clearly there. To my surprise, if I subset this data frame, R drops the attribute.</p>
<pre class="r"><code>new_df &lt;- df[df$x == 2, , drop = FALSE]

new_df$x</code></pre>
<pre><code>##  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</code></pre>
<p>It doesn’t matter if it’s using bracket subsetting (<code>[</code>) or <code>subset</code>.</p>
<pre class="r"><code>new_df &lt;- subset(df, x == 2)

new_df$x</code></pre>
<pre><code>##  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2</code></pre>
<p>That’s not good. R’s dropping attributes silently. For my specific purpose I ended up using <code>dplyr::filter</code> which safely enough preserves attributes.</p>
<pre class="r"><code>library(dplyr)

df %&gt;% 
  filter(df, x == 2) %&gt;% 
  pull(x)</code></pre>
<pre><code>##  [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## attr(,&quot;label&quot;)
## [1] &quot;This is clearly a label&quot;</code></pre>
]]>
      </description>
    </item>
    
    <item>
      <title>Turning a pdf book into machine readable format</title>
      <link>/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/turning-a-pdf-book-into-machine-readable-format/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/turning-a-pdf-book-into-machine-readable-format/</guid>
      <description><![CDATA[
      


<p>A few days ago a well known Sociologist, Erik Olin Wright, died from Leukemia. Torkild Lyngstand then <a href="https://twitter.com/torkildl/status/1088325262758969344">posted on twitter</a> his <a href="https://www.ssc.wisc.edu/~wright/Published%20writing/FallingIntoMarxismChoosingToStay.pdf">‘intellectual biography’</a> which is an interesting document that outlines how he ended up being a Marxist. This document is a pdf book that has two actual book pages per pdf page.</p>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-1-1.png" width="702" /></p>
<p>Although this is perfectly fine for reading on a computer, I usually don’t like to read anything longer than 15 pages on my computer. So I decided I would turn this book into machine readable text with R for my Kindle.</p>
<p>Spoiler: I couldn’t do it, so help me out!</p>
<p>Firs things first. I will use the <code>magick</code> and <code>tabulizer</code> packages. <code>tabulizer</code> has a dependency with <code>rJava</code> which is a bit difficult to handle. I wrote <a href="blog/2018-05-25-installing-rjava-on-windows-10/installing-rjava-on-windows-10/index.html">this blogpost</a> explaining how to install <code>rJava</code> on Windows 10 and it’s helped me inmensely not to waste time in the installation process.</p>
<p>After installing both packages successfully, I loaded them, and split the pdf into separate pages using <code>tabulizer::split_pdf</code>.</p>
<pre class="r"><code>library(magick)
Sys.setenv(JAVA_HOME=&quot;C:/Program Files/Java/jdk-11.0.2/&quot;)
library(tabulizer)

url &lt;- &quot;https://www.ssc.wisc.edu/~wright/Published%20writing/FallingIntoMarxismChoosingToStay.pdf&quot;
all_pages &lt;- tabulizer::split_pdf(url)

all_pages</code></pre>
<pre><code>##  [1] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce401.pdf&quot;
##  [2] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce402.pdf&quot;
##  [3] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce403.pdf&quot;
##  [4] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce404.pdf&quot;
##  [5] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce405.pdf&quot;
##  [6] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce406.pdf&quot;
##  [7] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce407.pdf&quot;
##  [8] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce408.pdf&quot;
##  [9] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce409.pdf&quot;
## [10] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce410.pdf&quot;
## [11] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce411.pdf&quot;
## [12] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce412.pdf&quot;
## [13] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce413.pdf&quot;
## [14] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce414.pdf&quot;</code></pre>
<p><code>tabulizer::split_df</code> saved each page on a separate pdf in a temporary directory. Now we only have to develop a function to clean one page and apply it to all middle pages (that is, excluding the first and last because they have a slightly different format).</p>
<p>After hard work, I developed the function <code>convert_page</code> which accepts one pdf page crops all the corners so that only text is available.</p>
<pre class="r"><code>convert_page &lt;- function(page) {
  page &lt;- magick::image_read_pdf(page)
  separator &lt;- image_info(page)$width / 2
  first_page &lt;- image_crop(page, geometry_area(width = separator))
  second_page &lt;- image_crop(page, geometry_area(x_off = separator, y_off = 1))
  
  size &lt;- geometry_area(width = 1400,
                        height = 2200,
                        x_off = 300,
                        y_off = 200)
  
  first_page &lt;- image_crop(first_page, size)
  
  
  size &lt;- geometry_area(width = 1400,
                        height = 2200,
                        x_off = 130,
                        y_off = 200)
  
  second_page &lt;- image_crop(second_page, size)
  
  f_text &lt;- image_ocr(first_page)
  s_text &lt;- image_ocr(second_page)
  
  complete_page &lt;- paste0(f_text, s_text)
  
  complete_page
}</code></pre>
<p>Let’s look at an actual example. Below is a picture of page 4:</p>
<pre class="r"><code>page_four &lt;- magick::image_read_pdf(all_pages[4])
image_resize(page_four, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-5-1.png" width="702" /></p>
<p><code>convert_page</code> crops the sides to obtain the leftmost page:</p>
<pre class="r"><code>separator &lt;- image_info(page_four)$width / 2
first_page &lt;- image_crop(page_four, geometry_area(width = separator))
second_page &lt;- image_crop(page_four, geometry_area(x_off = separator, y_off = 1))

size &lt;- geometry_area(width = 1400,
                      height = 2200,
                      x_off = 300,
                      y_off = 200)

first_page &lt;- image_crop(first_page, size)


size &lt;- geometry_area(width = 1400,
                      height = 2200,
                      x_off = 130,
                      y_off = 200)

second_page &lt;- image_crop(second_page, size)

image_resize(first_page, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-6-1.png" width="280" /></p>
<p>And for the rightmost page:</p>
<pre class="r"><code>image_resize(second_page, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-7-1.png" width="280" /></p>
<p>Finally, it converts and merges both pages into text with:</p>
<pre class="r"><code>f_text &lt;- image_ocr(first_page)
s_text &lt;- image_ocr(second_page)

complete_page &lt;- paste0(f_text, s_text)

cat(complete_page)</code></pre>
<pre><code>## thusiastic and involved in their children’s school projects and intellectual pur-
## suits. My mother would carefully go over term papers with each of us, giving us
## both editorial advice and substantive suggestions. We were members of the Law-
## rence Unitarian Fellowship, which was made up of, to a substantial extent, uni-
## versity families. Sunday morning services were basically interdisciplinary semi-
## nars on matters of philosophical and social concern; Sunday school was an
## extended curriculum on world religions. | knew by about age ten that | wanted
## to be a professor. Both of my parents were academics. Both of my siblings be-
## came academics. Both of their spouses are academics. (Only my wife, a clinical
## psychologist, is not an academic, although her father was a professor.) ‘The only
## social mobility in my family was interdepartmental. It just felt natural to go into
## the family business.
## 
## Lawrence was a delightful, easy place to grow up. Although Kansas was a po-
## litically conservative state, Lawrence was a vibrant, liberal community. My ear-
## liest form of political activism centered on religion: | was an active member of a
## Unitarian youth group called Liberal Religious Youth, and in high school { went
## out of my way to argue with Bible Belt Christians about their belief in God. The
## early 1960s also witnessed my earliest engagement with social activism. The civil
## rights movement came to Lawrence first in the form of an organized boycott of
## a local segregated swimming pool in the 1950s and then in the form of civil rights
## rallies in the 1960s. In 1963 I went to the Civil Rights March on Washington and
## heard Martin Luther King Jr’s “I have a dream” speech. My earliest sense of pol-
## itics was that at its core it was about moral questions of social justice, not prob-
## lems of economic power and interests.
## 
## My family, also, was liberal, supporting the civil rights movement and other
## liberal causes; but while the family culture encouraged an intellectual interest in
## social and moral concerns, it was not intensely political. We would often talk
## about values, and the Unitarian Fellowship we attended also stressed humanis-
## tic, socially concerned values, but these were mostly framed as matters of indi-
## vidual responsibility and morality not as the grounding of a coherent political
## challenge to social injustice. My only real exposure toa more radical political per-
## spective came through my maternal grandparents, Russian Jewish immigrants
## who had come to the United States before World War I and lived near us in Law-
## rence, and my mother’s sister’s family in New York. Although I was not aware of
## this at the time, my grandparents and the New York relatives were Communists.
## This was never openly talked about, but from time to time I would hear glowing
## things said about the Soviet Union, socialism would be held out as an ideal, and
## America and capitalism would be criticized in emotionally laden ways. My cous-
## ins in New York were especially vocal about this, and in the mid-1g60s when I be-
## came more engaged in political matters, intense political discussions with my
## New York relatives contributed significantly to anchoring my radical sensibilities.
## 
## My interest in social sciences began in earnest in high school. fn Lawrence it
## was easy for academically oriented kids to take courses at the University of Kan-
## sas, and in my senior year | took a political science course on American politics.
## For my term project | decided to do a survey of children’s attitudes toward the
## American presidency and got permission to administer a questionnaire to several
## hundred students from grades 1-12 in the public schools. | then organized a party
## with my friends to code the data and produce graphs of how various attitudes
## changed by age. I&#39;he most striking finding was that, in response to the question,
## “Would you like to be President of the United States when you grow up?” there
## were more girls who said yes than boys through third grade, after which the rate
## for girls declined dramatically.
## 
## By the time I graduated from high school in 1964, I had enough university
## credits and advanced placement credits to enter KU as a second-semester soph-
## omore, and that is what | had planned to do. Nearly all of my friends were going
## to KU. It just seemed like the thing to do. A friend of my parents, Karl Heider,
## gave me, as a Christmas present in my senior year in high school, an application
## form to Harvard. He was a graduate student at Harvard in anthropology at the
## time. I filled it out and sent it in. Harvard was the only place to which I applied,
## not out of inflated self-confidence but because it was the only application I got as
## a Christmas present. When | eventually was accepted (initially I was on the wait-
## ing list), the choice was thus between KU and Harvard. I suppose this was a
## “choice” since I could have decided to stay at KU. However, it just seemed so ob-
## vious; there was no angst, no weighing of alternatives, no thinking about the pros
## and cons. Thus, going to Harvard in a way just happened.
## 
## Like many students who began university in the mid-1960s, my political ideas
## were rapidly radicalized as the Viet Nam War escalated and began to impinge on
## our lives. I was not a student leader in activist politics, but I did actively partici-
## pate in demonstrations, rallies, fasts for peace, and endless political debate. At
## Harvard I majored in social studies, an intense interdisciplinary social science
## major centering on the classics of social theory, and in that program I was first ex-
## posed to the more abstract theoretical issues that bore on the political concerns
## of the day: the dynamics of capitalism, the nature of power and domination, the
## importance of elites in shaping American foreign policy, and the problem of</code></pre>
<p>If we pass the pdf page directly to <code>convert_page</code>, it will do it all in one take:</p>
<pre class="r"><code>cat(convert_page(all_pages[4]))</code></pre>
<pre><code>## thusiastic and involved in their children’s school projects and intellectual pur-
## suits. My mother would carefully go over term papers with each of us, giving us
## both editorial advice and substantive suggestions. We were members of the Law-
## rence Unitarian Fellowship, which was made up of, to a substantial extent, uni-
## versity families. Sunday morning services were basically interdisciplinary semi-
## nars on matters of philosophical and social concern; Sunday school was an
## extended curriculum on world religions. | knew by about age ten that | wanted
## to be a professor. Both of my parents were academics. Both of my siblings be-
## came academics. Both of their spouses are academics. (Only my wife, a clinical
## psychologist, is not an academic, although her father was a professor.) ‘The only
## social mobility in my family was interdepartmental. It just felt natural to go into
## the family business.
## 
## Lawrence was a delightful, easy place to grow up. Although Kansas was a po-
## litically conservative state, Lawrence was a vibrant, liberal community. My ear-
## liest form of political activism centered on religion: | was an active member of a
## Unitarian youth group called Liberal Religious Youth, and in high school { went
## out of my way to argue with Bible Belt Christians about their belief in God. The
## early 1960s also witnessed my earliest engagement with social activism. The civil
## rights movement came to Lawrence first in the form of an organized boycott of
## a local segregated swimming pool in the 1950s and then in the form of civil rights
## rallies in the 1960s. In 1963 I went to the Civil Rights March on Washington and
## heard Martin Luther King Jr’s “I have a dream” speech. My earliest sense of pol-
## itics was that at its core it was about moral questions of social justice, not prob-
## lems of economic power and interests.
## 
## My family, also, was liberal, supporting the civil rights movement and other
## liberal causes; but while the family culture encouraged an intellectual interest in
## social and moral concerns, it was not intensely political. We would often talk
## about values, and the Unitarian Fellowship we attended also stressed humanis-
## tic, socially concerned values, but these were mostly framed as matters of indi-
## vidual responsibility and morality not as the grounding of a coherent political
## challenge to social injustice. My only real exposure toa more radical political per-
## spective came through my maternal grandparents, Russian Jewish immigrants
## who had come to the United States before World War I and lived near us in Law-
## rence, and my mother’s sister’s family in New York. Although I was not aware of
## this at the time, my grandparents and the New York relatives were Communists.
## This was never openly talked about, but from time to time I would hear glowing
## things said about the Soviet Union, socialism would be held out as an ideal, and
## America and capitalism would be criticized in emotionally laden ways. My cous-
## ins in New York were especially vocal about this, and in the mid-1g60s when I be-
## came more engaged in political matters, intense political discussions with my
## New York relatives contributed significantly to anchoring my radical sensibilities.
## 
## My interest in social sciences began in earnest in high school. fn Lawrence it
## was easy for academically oriented kids to take courses at the University of Kan-
## sas, and in my senior year | took a political science course on American politics.
## For my term project | decided to do a survey of children’s attitudes toward the
## American presidency and got permission to administer a questionnaire to several
## hundred students from grades 1-12 in the public schools. | then organized a party
## with my friends to code the data and produce graphs of how various attitudes
## changed by age. I&#39;he most striking finding was that, in response to the question,
## “Would you like to be President of the United States when you grow up?” there
## were more girls who said yes than boys through third grade, after which the rate
## for girls declined dramatically.
## 
## By the time I graduated from high school in 1964, I had enough university
## credits and advanced placement credits to enter KU as a second-semester soph-
## omore, and that is what | had planned to do. Nearly all of my friends were going
## to KU. It just seemed like the thing to do. A friend of my parents, Karl Heider,
## gave me, as a Christmas present in my senior year in high school, an application
## form to Harvard. He was a graduate student at Harvard in anthropology at the
## time. I filled it out and sent it in. Harvard was the only place to which I applied,
## not out of inflated self-confidence but because it was the only application I got as
## a Christmas present. When | eventually was accepted (initially I was on the wait-
## ing list), the choice was thus between KU and Harvard. I suppose this was a
## “choice” since I could have decided to stay at KU. However, it just seemed so ob-
## vious; there was no angst, no weighing of alternatives, no thinking about the pros
## and cons. Thus, going to Harvard in a way just happened.
## 
## Like many students who began university in the mid-1960s, my political ideas
## were rapidly radicalized as the Viet Nam War escalated and began to impinge on
## our lives. I was not a student leader in activist politics, but I did actively partici-
## pate in demonstrations, rallies, fasts for peace, and endless political debate. At
## Harvard I majored in social studies, an intense interdisciplinary social science
## major centering on the classics of social theory, and in that program I was first ex-
## posed to the more abstract theoretical issues that bore on the political concerns
## of the day: the dynamics of capitalism, the nature of power and domination, the
## importance of elites in shaping American foreign policy, and the problem of</code></pre>
<p>We pass all middle pages to <code>convert_page</code> to convert them to text:</p>
<pre class="r"><code>middle_pages &lt;- lapply(all_pages[3:(length(all_pages) - 1)], convert_page)
cat(middle_pages[[1]])</code></pre>
<pre><code>## versity of Western Australia); music camp (1 played viola); assisting in a lab. And
## in college, it was much the same: volunteering as a photographer on an archae-
## ological dig in Hawaii; teaching in a high school enrichment program for mi-
## nority kids; traveling in urope. The closest thing to an ordinary paying job |
## ever had was occasionally selling hot dogs at football games in my freshman year
## in college. What is more, the ivory towers that [ have inhabited since the mid-
## 1960s have been located in beautiful physical settings, filled with congenial and
## interesting colleagues and students, and animated by exciting ideas. This, then,
## is the first fundamental fact of my life as an academic: [ have been extraordinar-
## ily lucky and have always lived what can only be considered a life of extreme priv-
## ilege. Nearly all of the time [ am doing what [ want to do; what I do gives me a
## sense of fulfillment and purpose; and | am paid well for doing it.
## 
## Here is the second fundamental fact of my academic life: since the early
## 19708, my intellectual life has been firmly anchored in the Marxist tradition. The
## core of my teaching as a professor has centered on communicating the central
## ideas and debates of contemporary Marxism and allied traditions of emancipa-
## tory social theory. The courses I have taught have had names like Class, State and
## Ideology: An Introduction to Marxist Sociology; Envisioning Real Utopias; Mars-
## ist Theories of the State; Alternative Foundations of Class Analysis. My energies
## in institution building have all involved creating and expanding arenas within
## which radical system-challenging ideas could flourish: creating a graduate pro-
## gram in class analysis and historical change in the Sociology Department at the
## University of Wisconsin—Madison; establishing the A. E. Havens Center, a re-
## search institute for critical scholarship at Wisconsin; organizing an annual con-
## ference for activists and academics, now called RadFest, which has been held
## every year since 1983. And my scholarship has been primarily devoted to recon-
## structing Marxism as a theoretical framework and research tradition. While the
## substantive preoccupations of this scholarship have shifted over the past thirty
## years, its central mission has not.
## 
## As in any biography, this pair of facts is the result of a trajectory of circum-
## stances and choices: circumstances that formed me and shaped the range of
## choices I encountered, and choices that in turn shaped my future circumstances.
## Some of these choices were made easily, with relatively little weighing of alter-
## natives, sometimes even without much awareness that a choice was actually be-
## ing made; others were the result of protracted reflection and conscious decision
## making, sometimes with the explicit understanding that the choice being made
## would constrain possible choices in the future. Six such junctures of circum-
## stance and choice seem especially important to me in shaping the contours of
## my academic career. ‘The first was posed incrementally in the early 1970s: the
## choice to identify my work primarily as contributing to Marxism rather than
## simply using Marxism. The second concerns the choice, made just before grad-
## uate school at the University of California, Berkeley, to be a sociologist, rather
## than some other ist. ‘The third was the choice to become what some people de-
## scribe as multivariate Marxist: to be a Marxist sociologist who engages in grandi-
## ose, perhaps overblown, quantitative research, The fourth choice was the choice
## of which academic department to be in. This choice was acutely posed to me
## in 1987 when I spent a year as a visiting professor at the University of Califor-
## nia, Berkeley. | had been offered a position there, and | had to decide whether
## I wanted to return to Wisconsin. Returning to Madison was unquestionably a
## choice that shaped subsequent contexts of choice. The fifth choice has been
## posed and reposed to me with increasing intensity since the late 1980s: the
## choice to stay a Marxist in this world of post-Marxisms when many of my intel-
## lectual comrades have decided for various good, and sometimes perhaps not so
## good, reasons to recast their intellectual agenda as being perhaps friendly to, but
## outside of, the Marxist tradition. Finally, the sixth important choice was to shift
## my central academic work from the study of class structure to the problem of en-
## visioning real utopias.
## 
## To set the stage for this reflection on choice and constraint, I need to give a
## brief account of the circumstances of my life that brought me into the arena of
## these choices.
## 
## Growing Up
## 
## I was born in Berkeley, California, in 1947 while my father, who had received a
## PhD in psychology before World War II, was in medical school on the GI Bill.
## When he finished his medical training in 1951, we moved to Lawrence, Kansas,
## where he became the head of the program in clinical psychology at Kansas Uni-
## versity (KU) and a professor of psychiatry in the KU Medical School. Because of
## antinepotism rules at the time, my mother, who also had a PhD in psychology,
## was not allowed to be employed at the university, so throughout the 1950s she did
## research on various research grants. In 1961, when the state law on such things
## changed, she became a professor of rehabilitation psychology.
## 
## Life in my family was intensely intellectual. Dinner table conversation would
## often revolve around intellectual matters, and my parents were always deeply en-</code></pre>
<p>Ok, everything’s looking good. Because the first and last pages have different croping dimensions, I slightly adapt the <code>geometry_area</code> to do it manually:</p>
<pre class="r"><code>### First page
first_page &lt;- magick::image_read_pdf(all_pages[2])
image_resize(first_page, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-11-1.png" width="702" /></p>
<pre class="r"><code>separator &lt;- image_info(first_page)$width / 2

size &lt;- geometry_area(width = 1400,
                      height = 1700,
                      x_off = separator + 100,
                      y_off = 650)

first_page &lt;- image_crop(first_page, size)
image_resize(first_page, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-11-2.png" width="280" /></p>
<pre class="r"><code>first_page &lt;- image_ocr(first_page)
###


### Last page
last_page &lt;- magick::image_read_pdf(all_pages[14])
image_resize(last_page, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-11-3.png" width="702" /></p>
<pre class="r"><code>separator &lt;- image_info(last_page)$width / 2

size &lt;- geometry_area(width = separator - 400,
                      height = 500,
                      x_off = 150,
                      y_off = 260)

last_page &lt;- image_crop(last_page, size)
image_resize(last_page, geometry_size_percent(width = 70))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-11-4.png" width="474" /></p>
<pre class="r"><code>last_page &lt;- image_ocr(last_page)
###</code></pre>
<p>Ok, the hard work is over! Now we need to merge all of the pages together and print a subset of the text:</p>
<pre class="r"><code>final_document &lt;- paste0(first_page, Reduce(paste0, middle_pages), last_page)
cat(paste0(substring(final_document, 0, 5000), &quot;...&quot;))</code></pre>
<pre><code>## Falling into Marxism; Choosing to Stay
## 
## Erik Olin Wright received his PhD from the University of California, Berkeley, and
## has taught at the University of Wisconsin since then. His academic work has been
## centrally concerned with reconstructing the Marxist tradition of social theory and
## research in ways that attempt to make it more relevant to contemporary concerns
## and more cogent as a scientific framework of analysis. His empirical research has
## focused especially on the changing character of class relations in developed capi-
## talist societies. Since 1992 he has directed the Real Utopias Project, which explores
## a range of proposals for new institutional designs that embody emancipatory ideals
## and yet are attentive to issues of pragmatic feasibility. His principle publications
## include The Politics of Punishment: A Critical Analysis of Prisons in America;
## Class, Crisis and the State; Classes; Reconstructing Marxism (with Elliott Sober
## and Andrew Levine); Interrogating Inequality; Class Counts: Comparative Stud-
## ies in Class Analysis; and Deepening Democracy: Innovations in Empowered
## Participatory Governance (with Archon Fung). He is married to Marcia Kahn
## Wright, a clinical psychologist working in community mental health, and has tvo
## grown daughters, Jennifer and Rebecca.
## 
## [ have been in school continuously for more than fifty vears: since I entered
## kindergarten in 1952, there has never been a September when I wasn’t beginning
## a school year. | have never held a nine-to-five job with fixed hours and a boss
## telling me what to do. In high school, my summers were always spent in vari-
## ous kinds of interesting and engaging activities — traveling home from Australia
## where my family spent a year (my parents were Fulbright professors at the Uni-
## versity of Western Australia); music camp (1 played viola); assisting in a lab. And
## in college, it was much the same: volunteering as a photographer on an archae-
## ological dig in Hawaii; teaching in a high school enrichment program for mi-
## nority kids; traveling in urope. The closest thing to an ordinary paying job |
## ever had was occasionally selling hot dogs at football games in my freshman year
## in college. What is more, the ivory towers that [ have inhabited since the mid-
## 1960s have been located in beautiful physical settings, filled with congenial and
## interesting colleagues and students, and animated by exciting ideas. This, then,
## is the first fundamental fact of my life as an academic: [ have been extraordinar-
## ily lucky and have always lived what can only be considered a life of extreme priv-
## ilege. Nearly all of the time [ am doing what [ want to do; what I do gives me a
## sense of fulfillment and purpose; and | am paid well for doing it.
## 
## Here is the second fundamental fact of my academic life: since the early
## 19708, my intellectual life has been firmly anchored in the Marxist tradition. The
## core of my teaching as a professor has centered on communicating the central
## ideas and debates of contemporary Marxism and allied traditions of emancipa-
## tory social theory. The courses I have taught have had names like Class, State and
## Ideology: An Introduction to Marxist Sociology; Envisioning Real Utopias; Mars-
## ist Theories of the State; Alternative Foundations of Class Analysis. My energies
## in institution building have all involved creating and expanding arenas within
## which radical system-challenging ideas could flourish: creating a graduate pro-
## gram in class analysis and historical change in the Sociology Department at the
## University of Wisconsin—Madison; establishing the A. E. Havens Center, a re-
## search institute for critical scholarship at Wisconsin; organizing an annual con-
## ference for activists and academics, now called RadFest, which has been held
## every year since 1983. And my scholarship has been primarily devoted to recon-
## structing Marxism as a theoretical framework and research tradition. While the
## substantive preoccupations of this scholarship have shifted over the past thirty
## years, its central mission has not.
## 
## As in any biography, this pair of facts is the result of a trajectory of circum-
## stances and choices: circumstances that formed me and shaped the range of
## choices I encountered, and choices that in turn shaped my future circumstances.
## Some of these choices were made easily, with relatively little weighing of alter-
## natives, sometimes even without much awareness that a choice was actually be-
## ing made; others were the result of protracted reflection and conscious decision
## making, sometimes with the explicit understanding that the choice being made
## would constrain possible choices in the future. Six such junctures of circum-
## stance and choice seem especially important to me in shaping the contours of
## my academic career. ‘The first was posed incrementally in the early 1970s: the
## choice to identify my work primarily as contributing to Marxism rather than
## simply using Marxism. The second concerns the choice, made just before grad-
## uate school at the University ...</code></pre>
<p>There we go, nicely formatted text all obtained from pdf images (after carefully revising the text there are many mistakes, but this was a lightning post, so no time to tidy up the text).</p>
<div id="converting-the-text-to-an-epub" class="section level3">
<h3>Converting the text to an epub</h3>
<p>I thought this was going to be much easier, but <code>knitr</code> seems to crash when compiling this text. According to <a href="https://bookdown.org/yihui/bookdown/build-the-book.html">bookdown</a>, I would need a <code>.Rmd</code> file and then use <code>bookdown::render_book(&quot;my_book.Rmd&quot;, bookdown::epub_book())</code>. However, I cannot compile the <code>.Rmd</code> file using this text because it runs out of memory. Run the example below:</p>
<pre class="r"><code>rmd_path &lt;- tempfile(pattern = &#39;our_book&#39;, fileext = &quot;.Rmd&quot;)

rmd_preamble &lt;-&quot;---
  title: &#39;Final Book&#39;
  output: html_document
---\n\n&quot;

final_document &lt;- paste0(rmd_preamble, final_document)
  
writeLines(final_document, con = rmd_path, useBytes = TRUE)

# Bookdown compiles all .Rmd in the working directory, so we move
# to the temporary directory where the book is
setwd(dirname(rmd_path))
bookdown::render_book(rmd_path, bookdown::epub_book())</code></pre>
<p>If you figure out how make to this work, I’d love to hear about it in the comment section.</p>
<p>EDIT:</p>
<p>Thanks to the <a href="https://twitter.com/leonawicz/status/1089537068550651907">tweet by Matthew Leonawicz</a> I managed to do it!</p>
<pre class="r"><code>txt_path &lt;- tempfile(pattern = &#39;our_book&#39;, fileext = &quot;.txt&quot;)

writeLines(final_document, con = txt_path, useBytes = TRUE)

# First download Calibre
path &lt;- paste0(Sys.getenv(&quot;PATH&quot;), &quot;;&quot;, &quot;C:\\Program Files\\Calibre2&quot;)
Sys.setenv(PATH = path)
bookdown::calibre(txt_path, paste0(dirname(txt_path), &quot;/erik_wright.mobi&quot;))</code></pre>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>Some guides and pre-project documents on ML</title>
      <link>/blog/2019-01-22-some-guides-and-preproject-documents-on-ml/some-guides-and-pre-project-documents-on-ml/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019-01-22-some-guides-and-preproject-documents-on-ml/some-guides-and-pre-project-documents-on-ml/</guid>
      <description><![CDATA[
      


<p>I was just browsing the web and found <a href="https://developers.google.com/machine-learning/guides/rules-of-ml/">this cool resource on ML from Google</a>. They’re not like you’re typical tutorial but rather bullet-point type questions with advice on what to do on certain scenarios. I just came up with the idea of an interesting series of posts where each of the questions outlined in the documents is accompanied with a concrete example that shows how it works under certain scenarios.</p>
<p>There’s also other tutorial on that website such as with <a href="https://developers.google.com/machine-learning/guides/text-classification/">text classification</a>. This reminds me of the <a href="https://github.com/cimentadaj/info_to_read">list I keep of interesting blog posts/books/courses</a> I would to follow through in the future</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Exploring Google Scholar coauthorship</title>
      <link>/blog/2018-06-19-exploring-google-scholar-coauthorship/exploring-google-scholar-coauthorship/</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-06-19-exploring-google-scholar-coauthorship/exploring-google-scholar-coauthorship/</guid>
      <description><![CDATA[
      


<p>I woke up today to read Maëlle Salmon’s latest blog entry in which she scraped her own <a href="https://masalmon.eu/2018/06/18/mathtree/">mathematical tree</a>. Running through the code I had an idea about scraping the coauthorship list that a Google Scholar profile has. With this, I could visualize the network of coauthorship of important scientists and explore whether they have closed or open collaborations.</p>
<p>I sat down this morning and created the <code>coauthornetwork</code> package that allows you to do just that! It’s actually very simple. First, install it with the usual:</p>
<pre class="r"><code>devtools::install_github(&quot;cimentadaj/coauthornetwork&quot;)</code></pre>
<p>There’s two functions: <code>grab_network</code> and <code>plot_coauthors</code>. The first scrapes and returns a data frame of a Google Scholar profile, their coauthors and the coauthors of their coauthors (what?). More simply, by default, the data frame returns this:</p>
<p>Google Scholar Profile –&gt; Coauthors –&gt; Coauthors</p>
<p>It’s not that hard after all. The only thing you need to provide is the end of the URL of a Google Scholar profile. For example, a typical URL looks like this: <code>https://scholar.google.com/citations?user=F0kCgy8AAAAJ&amp;hl=en</code>. <code>grab_network</code> will accept the latter part of the URL, namely: <code>citations?user=F0kCgy8AAAAJ&amp;hl=en</code>. Let’s test it:</p>
<pre class="r"><code>library(coauthornetwork)

network &lt;- grab_network(&quot;citations?user=F0kCgy8AAAAJ&amp;hl=en&quot;)
network</code></pre>
<pre><code>## # A tibble: 21 x 4
##    author       href                 coauthors     coauthors_href          
##    &lt;fct&gt;        &lt;fct&gt;                &lt;fct&gt;         &lt;fct&gt;                   
##  1 Hans-Peter ~ citations?user=F0kC~ Melinda Mills /citations?user=HX9KQ5M~
##  2 Hans-Peter ~ citations?user=F0kC~ Karl Ulrich ~ /citations?user=iuzu9xw~
##  3 Hans-Peter ~ citations?user=F0kC~ Florian Schu~ /citations?user=MWCt6hQ~
##  4 Hans-Peter ~ citations?user=F0kC~ Yossi Shavit  /citations?user=brfWXKM~
##  5 Hans-Peter ~ citations?user=F0kC~ Jan Skopek    /citations?user=Mmo1hFk~
##  6 Melinda Mil~ /citations?user=HX9~ Hans-Peter B~ /citations?user=F0kCgy8~
##  7 Melinda Mil~ /citations?user=HX9~ Tanturri Mar~ /citations?user=xN3XevQ~
##  8 Melinda Mil~ /citations?user=HX9~ René Veenstra /citations?user=_9OVrqM~
##  9 Melinda Mil~ /citations?user=HX9~ Francesco C.~ /citations?user=-JR6yo4~
## 10 Karl Ulrich~ /citations?user=iuz~ Paul B. Balt~ /citations?user=vcOZeDg~
## # ... with 11 more rows</code></pre>
<p>The main author here is Hans-Peter Blossfeld, a well known Sociologist. We also see that Melinda Mills is one of his coauthors, so we also have the coauthors of Melinda Mills right after him. <code>grab_networks</code> also has the <code>n_coauthors</code> argument to control how many coauthors you can extract (limited to 20 by Google Scholar). You’ll notice that once you go over 10 coauthors things start to get very messy when we visualize this.</p>
<pre class="r"><code>plot_coauthors(network, size_labels = 3)</code></pre>
<p><img src="/blog/2018-06-19-exploring-google-scholar-coauthorship/2018-06-19-exploring-google-scholar-coauthorship_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Cool eh? We can play around with more coauthors as well.</p>
<pre class="r"><code>plot_coauthors(grab_network(&quot;citations?user=F0kCgy8AAAAJ&amp;hl=en&quot;, n_coauthors = 7), size_labels = 3)</code></pre>
<p><img src="/blog/2018-06-19-exploring-google-scholar-coauthorship/2018-06-19-exploring-google-scholar-coauthorship_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Hope you enjoy it!</p>
<!-- To make it more accesible to non-R users, I [created a Shiny app](https://cimentadaj.shinyapps.io/gs_coauthorsip/) where everyone can explore their own coauthors. Enjoy! -->
]]>
      </description>
    </item>
    
    <item>
      <title>Installing rJava on Windows 10</title>
      <link>/blog/2018-05-25-installing-rjava-on-windows-10/installing-rjava-on-windows-10/</link>
      <pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-05-25-installing-rjava-on-windows-10/installing-rjava-on-windows-10/</guid>
      <description><![CDATA[
      


<p>Struggled for about two hours to install <code>rJava</code> on my Windows 10 machine. Post here the steps that made it work in case anyone is interested (that is, future me).</p>
<ul>
<li><p>Check whether R is 32/64 bit with <code>sessionInfo()</code>. Check Platform.</p></li>
<li><p>Download the specific 32/64 bit of Java. This is <strong>really</strong> important. R and Java must have the same memory signature, either 32 or 64 bit. I had 64 bit so I downloaded the Offline 64-bit version from <a href="https://www.java.com/en/download/manual.jsp">here</a>.</p></li>
<li><p>Download Java JDK for 32/64 bit. For 64-bit I had to download the Windows version from <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk10-downloads-4416644.html">here</a>.</p></li>
<li><p>If you installed 32-bit Java then everything should be saved in <code>C:/Program Files (x86)/Java/</code>. Conversely, if you installed 64-bit then everything should be installed in <code>C:/Program Files/Java/</code>.</p></li>
<li><p>Install <code>rJava</code> with <code>install.packages(&quot;rJava&quot;)</code>.</p></li>
<li><p>Set your <code>JAVA_HOME</code> environment with <code>Sys.setenv(JAVA_HOME=&quot;C:/Program Files/Java/jdk-10.0.1/&quot;)</code> so that it points to your specific (64-bit in my case) folder that contains the <code>jdk</code>. Don’t worry about <code>jdk-10.0.1</code> as this might change for future releases.</p></li>
<li><p><code>library(rJava)</code> throws no errors to me!</p></li>
</ul>
<p>Good luck!</p>
]]>
      </description>
    </item>
    
    <item>
      <title>A list of must pre-project questions</title>
      <link>/blog/2018-05-23-a-list-of-must-preproject-questions/a-list-of-must-pre-project-questions/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-05-23-a-list-of-must-preproject-questions/a-list-of-must-pre-project-questions/</guid>
      <description><![CDATA[
      


<p>Rumbling through Twitter I found a Jupyter Notebook of Paige Bailey written at the rOpensci unconf about Ethical Machine Learning which you can read <a href="https://github.com/ropenscilabs/proxy-bias-vignette/blob/master/EthicalMachineLearning.ipynb">here</a>. It was very interesting to look at her workflow but even more interesting was the set of questions she asked herself before and during the analysis. I paste them here just to keep them as a reference.</p>
<p><strong>As you design the goal and the purpose of your machine learning product, you must first ask: Who is your audience?</strong></p>
<ul>
<li>Is your product or analysis meant to include all people?</li>
<li>And, if not: is it targeted to an exclusive audience?</li>
<li>Is there a person on your team tasked specifically with identifying and resolving bias and discrimination issues?</li>
</ul>
<p><strong>Once the concept and scope have been defined, it is time to focus on the acquisition, evaluation, and cleaning of data. We have received a single .csv file filled with information on customers from the bank’s manager. Some questions to consider:</strong></p>
<ul>
<li>Did the data come from a system prone to human error?</li>
<li>Is the data current?</li>
<li>What technology facilitated the collection of the data?</li>
<li>Was participation of the data subjects voluntary?</li>
<li>Does the context of the collection match the context of your use?</li>
<li>Was your data collected by people or a system that was operating with quotas or a particular incentive structure?</li>
</ul>
<p><strong>Now that your data has been collected, it would be a great idea to evaluate and describe it:</strong></p>
<ul>
<li>Who is represented in the data?</li>
<li>Who is under-represented or absent from your data?</li>
<li>Can you find additional data, or use statistical methods, to make your data more inclusive?</li>
<li>Was the data collected in an environment where data subjects had meaningful choices?</li>
<li>How does the data reflect the perspective of the institution that collected it?</li>
<li>Were fields within the data inferred or appended beyond what was clear to the data subject?
Would this use of the data surprise the data subjects?</li>
</ul>
<p><strong>The next step would be cleaning the data.</strong></p>
<ul>
<li>Are there any fields that should be eliminated from your data?</li>
<li>Can you use anonymization or pseudonymization techniques to avoid needless evaluation or processing of individual data?</li>
</ul>
<p><strong>Establishing logic for variables</strong></p>
<ul>
<li>Can you describe the logic that connects the variables to the output of your equation?</li>
<li>Do your variables have a causal relationship to the results they predict?</li>
<li>How did you determine what weight to give each variable?</li>
</ul>
<p><strong>Identifying assumptions</strong></p>
<ul>
<li>Will your variables apply equally across race, gender, age, disability, ethnicity, socioeconomic status, education, etc.?</li>
<li>What are you assuming about the kinds of people in your data set?</li>
<li>Would you be comfortable explaining your assumptions to the public?</li>
<li>What assumptions are you relying on to determine the relevant variables and their weights?</li>
</ul>
<p><strong>Defining success</strong>
- What amount and type of error do you expect?
- How will you ensure your system is behaving the way you intend? How reliable is it?</p>
<p><strong>How will you choose your analytical method? For example, predictive analytics, machine learning (supervised, unsupervised), neural networks or deep learning, etc.</strong></p>
<ul>
<li>How much transparency does this method allow your end users and yourself?</li>
<li>Are non-deterministic outcomes acceptable given your legal or ethical obligations around transparency and explainability?</li>
<li>Does your choice of analytical method allow you to sufficiently explain your results?</li>
<li>What particular tasks are associated with the type of analytical method you are using?</li>
</ul>
<p><strong>Tools</strong></p>
<ul>
<li>How could results that look successful still contain bias?</li>
<li>Is there a trustworthy or audited source for the tools you need?</li>
<li>Have the tools you are using been associated with biased products?</li>
<li>Or, if you build from scratch: can you or a third-party test your tools for any features that can result in biased or unfair outcomes?</li>
</ul>
]]>
      </description>
    </item>
    
    <item>
      <title>The Monty Hall problem</title>
      <link>/blog/2018-04-20-the-monty-hall-problem/the-monty-hall-problem/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-04-20-the-monty-hall-problem/the-monty-hall-problem/</guid>
      <description><![CDATA[
      


<p>Just recently, I’ve been completing the online version of <code>CS109</code> from Harvard which is a class for understanding machine learning algorithms in Python. In the very first section the homework of the class asked us to develope a working simulation of the Monty Hall problem.</p>
<p>The Monty Hall problem is very simple. In a gameshow, contestants try to guess which of 3 closed doors contain a cash prize (goats are behind the other two doors). Of course, the odds of choosing the correct door are 1 in 3. As a twist, the host of the show occasionally opens a door after a contestant makes his or her choice. This door is always one of the two the contestant did not pick, and is also always one of the goat doors (note that it is always possible to do this, since there are two goat doors). At this point, the contestant has the option of keeping his or her original choice, or swtiching to the other unopened door. The question is: is there any benefit to switching doors?</p>
<p>I implemented this in Python below. Let’s go step by step. Let’s load the packages we’ll use.</p>
<ol style="list-style-type: decimal">
<li>Load packages</li>
</ol>
<pre class="python"><code>import numpy as np</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Define a function that simulates an array of prize doors. Optionally, supply any number of doors available.</li>
</ol>
<pre class="python"><code>def simulate_prizedoor(nsim, doors = 3):
    answer = np.array([np.random.randint(0, doors) for x in range(nsim)])
    return answer
    
print(simulate_prizedoor(10, 3))</code></pre>
<pre><code>## [2 1 2 0 2 1 0 2 2 1]</code></pre>
<p>This array shows a random prize door from 10 hypothetical (independent) tries in which a persons will choose a guess.</p>
<ol start="3" style="list-style-type: decimal">
<li>Define a function that simulates an array of guesses.</li>
</ol>
<pre class="python"><code>def simulate_guess(nsim, doors = 3):
    return(simulate_prizedoor(nsim, doors = doors))
print(simulate_guess(10))</code></pre>
<pre><code>## [1 0 0 1 0 1 0 1 1 1]</code></pre>
<p>This array shows a random choosen door from 10 hypothetical (independent) tries in which a persons will choose a guess.</p>
<ol start="3" style="list-style-type: decimal">
<li>Define a <code>goat_door</code> function that returns the first ‘goat’ door that the host of the show will open.</li>
</ol>
<pre class="python"><code># Find elements in `first_set` not present in `second_set`
def diff(first_set, second_set):
    res = [x for x in first_set if x not in second_set]
    return res
    
# Choose the goat door
def goat_door(prizedoors, guesses, doors = 3):
    new_stack = np.column_stack((prizedoors, guesses))
    full_ops = [x for x in range(doors)]
    res = [diff(full_ops, new_stack[index, ])[0] for index in range(new_stack.shape[0])]
    return np.array(res)
    
print(goat_door(simulate_prizedoor(10), simulate_guess(10)))</code></pre>
<pre><code>## [2 1 1 0 1 2 0 0 2 1]</code></pre>
<p>For example, if the prizedoor is 2 and the person guessed door 2, the <code>goat_door</code> function will return either <code>0</code> or <code>1</code> (in this case it will return always <code>0</code> because I was a bit lazy).</p>
<ol start="4" style="list-style-type: decimal">
<li>Define a function that switches the guess of the respondent in case he/she wants to change doors after the first door has been opened.</li>
</ol>
<pre class="python"><code>#your code here
def switch_guess(guesses, goatdoors):
    return goat_door(guesses, goatdoors)
    
print(switch_guess(np.array([0, 1, 2]), np.array([1, 2, 1])))</code></pre>
<pre><code>## [2 0 0]</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Finally, define a function that calculates the percentage of correct doors the person guessed.</li>
</ol>
<pre class="python"><code>def win_percentage(guesses, prizedoors):
    return (guesses == prizedoors).mean()</code></pre>
<p>The exercise adds this:</p>
<p>Simulate 10000 games where contestant keeps his original guess, and 10000 games where the contestant switches his door after a goat door is revealed. Compute the percentage of time the contestant wins under either strategy. Is one strategy better than the other?</p>
<p>I implement it below.</p>
<pre class="python"><code># Number of simulations
nsim = 10000
# Numer of doors
doors = 3
# The user picks nsim random guesses
first_guess = simulate_guess(nsim, doors = doors)
# The equivalent 10000 random prize_doors
prize_door = simulate_prizedoor(nsim, doors = doors)
# The 10000 doors which the host opened
chosen_goat = goat_door(first_guess, prize_door)
## For the scenario where the users keeps the first guess,
## how many wins does he/she has?
first_perc = win_percentage(first_guess, prize_door)
print(first_perc)</code></pre>
<pre><code>## 0.3405</code></pre>
<p>What happens if the user switches the door after the host has opened one door</p>
<pre class="python"><code># After you had your first guess, switch to a door that is not the door opened by the
# host
second_guess = switch_guess(first_guess, chosen_goat)
# Calculate the % win with this new guess
second_perc = win_percentage(second_guess, prize_door)
print(second_perc)</code></pre>
<pre><code>## 0.6595</code></pre>
<p>What? It’s not 50/50? This is a bit counterintuitive as most of the internet suggests but I found this explanation very intuitive. With 3 doors, the odds of winning are as simple as <code>1/3</code> and the odds of losing is <code>2/3</code>. When Monty opens a second door, the odds don’t go to <code>50/50</code> because you already knew from before that you had a <code>2/3</code> chance of losing that <strong>just</strong> became <code>1/3</code>. If the odds of losing were <code>2/3</code> and a new door was opened, then the odds of losing become <code>1/3</code>, which means that odds of winning became <code>2/3</code>! And that fraction becomes <code>66%</code>, the percentage we got above. The key thing to understanding the riddle is <strong>not</strong> reestimating your odds but merely <strong>updating</strong> your odds like a true bayesian :).</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Login in, scraping and hidden fields</title>
      <link>/blog/2018-04-05-login-in-scraping-and-hidden-fields/login-in-scraping-and-hidden-fields/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-04-05-login-in-scraping-and-hidden-fields/login-in-scraping-and-hidden-fields/</guid>
      <description><![CDATA[
      


<p>Lightning post. Earlier today I was trying to scrape the emails from all the PhD candidates in my program and I had to log in from our ‘Aula Global’. I did so using <code>httr</code> but something was off: I introduced both my username and password but the website did not log in. Apparently, when loging in through <code>POST</code>, sometimes there’s a thing call hidden fields that you need to fill out! I would’ve never though about this. Below is a case study, that excludes my credentials.</p>
<p>The first thing we have to do is identify the <code>POST</code> method and the inputs to the request. Using Google Chrome, go to the website <a href="https://sso.upf.edu/CAS/index.php/login?service=https%3A%2F%2Faulaglobal.upf.edu%2Flogin%2Findex.php">https://sso.upf.edu/CAS/index.php/login?service=https%3A%2F%2Faulaglobal.upf.edu%2Flogin%2Findex.php</a> and then on the Google Chrome menu go to -&gt; Settings -&gt; More tools -&gt; Developer tools. Here we have the complete html of the website.</p>
<ol style="list-style-type: decimal">
<li>We identify the POST method and the URL</li>
</ol>
<!-- <img src="/img/post_method.png" alt="Drawing" style="width: 600px;"/> -->
<div class="figure">
<img src="/img/post_method.png" />

</div>
<p>It’s the branch with <code>form</code> that has <code>method='post'</code>.</p>
<ol start="2" style="list-style-type: decimal">
<li>Open the <code>POST</code> branch and find all fields. We can see the two ‘hidden’ fields.</li>
</ol>
<div class="figure">
<img src="/img/hidden_fields.png" />

</div>
<p>Below the <code>form</code> tag, we see two <code>input</code> tags set to hidden, there they are! Even though we want to login, we also have to provide the two hidden fields. Take note of both their <code>name</code> and <code>value</code> tags.</p>
<ol start="3" style="list-style-type: decimal">
<li>Dive deeper down the branch and find other fields. In our case, username and password.</li>
</ol>
<p>For username:</p>
<div class="figure">
<img src="/img/username.png" />

</div>
<p>For password:</p>
<div class="figure">
<img src="/img/password.png" />

</div>
<ol start="4" style="list-style-type: decimal">
<li>Write down the field names with the correspoding values.</li>
</ol>
<pre class="r"><code>all_fields &lt;-
  list(
    adAS_username = &quot;private&quot;,
    adAS_password = &quot;private&quot;,
    adAS_i18n_theme = &#39;en&#39;,
    adAS_mode = &#39;authn&#39;
  )</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Load our packages and our URL’s</li>
</ol>
<pre class="r"><code>library(tidyverse)
library(httr)
library(xml2)

login &lt;- &quot;https://sso.upf.edu/CAS/index.php/login?service=https%3A%2F%2Faulaglobal.upf.edu%2Flogin%2Findex.php&quot;
website &lt;- &quot;https://aulaglobal.upf.edu/user/index.php?page=0&amp;perpage=5000&amp;mode=1&amp;accesssince=0&amp;search&amp;roleid=5&amp;contextid=185837&amp;id=9829&quot;</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Login using all of our fields.</li>
</ol>
<pre class="r"><code>upf &lt;- handle(&quot;https://aulaglobal.upf.edu&quot;)

access &lt;- POST(login,
               body = all_fields,
               handle = upf)</code></pre>
<p>Note how I set the <code>handle</code>. If the website you want to visit and the website that hosts the login information have the same root of the URL (<code>aulaglobal.upf.edu</code> for example), then you can avoid using <code>handle</code> (it’s done behind the scenes). In my case, I set the <code>handle</code> to the same root URL of the website I WANT to visit after I log in (because they have different root URL’s). This way the cookies and login information from the login are preserved through out the session.</p>
<ol start="4" style="list-style-type: decimal">
<li>Request the information from the website you’re interested</li>
</ol>
<pre class="r"><code>emails &lt;- GET(website, handle = upf)</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>Scrape away!</li>
</ol>
<pre class="r"><code>all_emails &lt;-
  read_html(emails) %&gt;% 
  xml_ns_strip() %&gt;% 
  xml_find_all(&quot;//table//a&quot;) %&gt;% 
  as_list() %&gt;% 
  unlist() %&gt;% 
  str_subset(&quot;.+@upf.edu$&quot;)</code></pre>
<p>Unfortunately you won’t be able to reproduce this script because you don’t have a log information unless you belong to the same PhD program as I do. However, I hope you find the hidden fields explanation useful, I’m sure I will come back to this in the near future for reference!</p>
]]>
      </description>
    </item>
    
    <item>
      <title>ess is now essurvey</title>
      <link>/blog/2018-03-26-ess-is-now-essurvey/ess-is-now-essurvey/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-03-26-ess-is-now-essurvey/ess-is-now-essurvey/</guid>
      <description><![CDATA[
      


<p>My <code>ess</code> package has be renamed to <code>essurvey</code>. For quite some time I’ve been pondering whether I should change the name. All of this comes from a dicussion we had on the <a href="http://r.789695.n4.nabble.com/R-pkgs-Release-of-ess-0-0-1-td4746540.html">R-pkg mailing list</a> where many R users suggested that the name was unfortunate given that Emacs Speaks Statistics (ESS) has a long precedence in the R community and the names are very similar. Later on, when submitting the package to <a href="https://ropensci.org/">rOpensci</a>, Jim Hester <a href="https://github.com/ropensci/onboarding/issues/201#issuecomment-372304003">raised the fact once again</a>, without being aware of the previous email thread.</p>
<p>Considering that I was already changing some of the functionalities of the package due to the <a href="https://github.com/ropensci/onboarding/issues/201">rOpensci review</a>, I decided to change the package name and republish an improved version of <code>ess</code> as <code>essurvey 1.0.0</code>. <code>essurvey</code> is now on CRAN and the repository has been moved to rOpensci’s <a href="https://github.com/ropensci/essurvey">github account</a>.</p>
<p>The new package is mostly similar although there are now some deprecated functions and new features. Below are the main changes.</p>
<ul>
<li><p>You can login <strong>once</strong> using <code>set_email(&quot;your_email&quot;)</code> and avoid rewriting your email in every call to the <code>ess_*</code> functions.</p></li>
<li><p>All <code>ess_*</code> functions have been deprecated in favour of similar <code>import_*</code> functions. For example:</p></li>
</ul>
<pre class="r"><code>ess_rounds(1:7)</code></pre>
<p>becomes..</p>
<pre class="r"><code>import_rounds(1:7)</code></pre>
<p>But that’s the same you would say. The only difference is that with <code>ess_rounds</code> you could download data in Stata, SPSS or SAS formats directly. For that, there’s now the <code>download_*</code> functions.</p>
<pre class="r"><code>download_rounds(
  1:5,
  output_dir = getwd(),
  format = &quot;spss&quot;
)</code></pre>
<p>All of the above applies to <code>ess_country</code> and the <code>ess_all_*</code> functions. There’s also some other minor changes you can checkout in the <a href="https://github.com/ropensci/essurvey/blob/master/NEWS.md">NEWS</a> file. If you haven’t tried <code>essurvey</code>, you can visit the package website for more detailed examples at <a href="https://ropensci.github.io/essurvey/" class="uri">https://ropensci.github.io/essurvey/</a>.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>RSelenium and scraping Catalan educational data</title>
      <link>/blog/2018-03-22-rselenium-and-scraping-catalan-educational-data/rselenium-and-scraping-catalan-educational-data/</link>
      <pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-03-22-rselenium-and-scraping-catalan-educational-data/rselenium-and-scraping-catalan-educational-data/</guid>
      <description><![CDATA[
      


<p>Yesterday I found this public dataset on schools from Barcelona and their performance on tests on 6th grade. I wanted to scrape them to investigate the relationship between performance and schools that receive special government funds for social integration. I found this dataset <a href="https://view-awesome-table.com/-L4lo3r-JA2iaWk1puUT/view">here</a> but it was different from the types of websites I usually scape (<code>html</code> or <code>xml</code>). Although the website has some <code>html</code> the engine swiping the schools is actually based on <code>Javascript</code>. Well, that’s a job for <code>RSelenium</code>, an R package that allows you to browse a website with R.</p>
<p>The process was actually much easier than I thought using Docker. I follow the answer of setting docker from <a href="https://stackoverflow.com/questions/45395849/cant-execute-rsdriver-connection-refused">this</a> post. Note that this is for Windows 10.</p>
<ul>
<li><a href="https://download.docker.com/win/stable/DockerToolbox.exe">install docker</a></li>
<li>run it, restart computer as requested</li>
<li>pull image by running in command line: <code>docker pull selenium/standalone-firefox</code> (or chrome instead of firefox) or in R <code>shell('docker pull selenium/standalone-firefox')</code></li>
<li>start server by running in command line: <code>docker run -d -p 4445:4444 selenium/standalone-firefox</code> or in R <code>shell('docker run -d -p 4445:4444 selenium/standalone-firefox')</code></li>
<li>Then run <code>remDr &lt;- remoteDriver(remoteServerAddr = &quot;localhost&quot;, port = 4445L, browserName = &quot;firefox'&quot;)</code>. The doc suggests something different with a virtual machine but i couldn’t get it to work. Replacing <code>&quot;localhost&quot;</code> with the <code>ip</code> the your docker server provides.</li>
</ul>
<p>I used <code>chrome</code> for all of the above and got this working just fine in no time!</p>
<p>Now that we got that down, I scraped the data with not much hassle.</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>Load packages and create empty data frame to fill out (I looked at the website to get the columns)</li>
</ol></li>
</ul>
<pre class="r"><code>library(RSelenium)
library(xml2)
library(tidyverse)

the_df &lt;-
  as_tibble(set_names(rerun(4, character()),
                      c(&quot;school_name&quot;, &quot;complexity&quot;, &quot;social_fund&quot;, &quot;score_6th&quot;)))</code></pre>
<ul>
<li><ol start="2" style="list-style-type: decimal">
<li>Open the website with <code>RSelenium</code></li>
</ol></li>
</ul>
<pre class="r"><code>remDr &lt;- remoteDriver(remoteServerAddr = &quot;192.168.99.100&quot;,
                      port = 4445L,
                      browserName = &quot;chrome&quot;)

remDr$open()
remDr$navigate(&quot;https://view-awesome-table.com/-L4lo3r-JA2iaWk1puUT/view&quot;)</code></pre>
<p>At this point you can use <code>remDr$screenshot(display = TRUE)</code> to print a screenshot of the website that you’re at.</p>
<ul>
<li><ol start="3" style="list-style-type: decimal">
<li>Define a function that clicks one time on the swiping key on the right, scrapes the table and turns it into a <code>tibble</code></li>
</ol></li>
</ul>
<pre class="r"><code>navigate_click &lt;- function() {
  webElem &lt;- remDr$findElement(using = &quot;class name&quot;,
                               &quot;google-visualization-table-div-page&quot;)
  
  Sys.sleep(0.5)
  webElem$clickElement()
  
  remDr$getPageSource()[[1]] %&gt;% 
    read_xml() %&gt;%
    xml_ns_strip() %&gt;%
    xml_find_all(xpath = &#39;//td&#39;) %&gt;%
    xml_text() %&gt;%
    set_names(c(&quot;school_name&quot;, &quot;complexity&quot;, &quot;social_fund&quot;, &quot;score_6th&quot;)) %&gt;%
    as.list() %&gt;% as_tibble()
}</code></pre>
<ul>
<li><ol start="4" style="list-style-type: decimal">
<li>Run that function <code>160</code> times (# of schools in that data) and bind all of these datasets together</li>
</ol></li>
</ul>
<pre class="r"><code>complete_df &lt;-
  map(1:160, ~ navigate_click()) %&gt;%
  bind_rows()</code></pre>
<p>Aaaaandddd, we got our nicely formatted dataset ready for some analysis.</p>
<pre class="r"><code>complete_df</code></pre>
<pre><code>## # A tibble: 160 x 4
##    school_name                   complexity   social_fund score_6th   
##    &lt;chr&gt;                         &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;       
##  1 Escuela Collaso i Gil         Muy alta     52%         Bajo        
##  2 Escuela Ruben Darío           Muy alta     66%         Bajo        
##  3 Escuela Castella              Muy alta     25%         Mediano-bajo
##  4 Escuela Drassanes             Muy alta     41%         Bajo        
##  5 Escuela Milà i Fontanals      Muy alta     49%         Bajo        
##  6 Escuela Baixeras              Mediana-alta 24%         Bajo        
##  7 Escuela Cervantes             Mediana-alta 38%         Mediano-alto
##  8 Escuela Parc de la Ciutadella Mediana-baja 15%         Mediano-bajo
##  9 Escuela Pere Vila             Alta         30%         Mediano-alto
## 10 Escuela Alexandre Galí        Alta         27%         Bajo        
## # ... with 150 more rows</code></pre>
<p>PS: If they ever remove that dataset from the website this post might not work in the future, but at least there’s a traceback on how to user docker with <code>RSelenium</code>.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>ess 0.1.1 is out!</title>
      <link>/blog/2018-03-04-ess-011-is-out/ess-0-1-1-is-out/</link>
      <pubDate>Sun, 04 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-03-04-ess-011-is-out/ess-0-1-1-is-out/</guid>
      <description><![CDATA[
      


<p>The new version of the ess package is out! <code>ess 0.1.1</code> fixes some bugs and inconsistencies across the package and has one important new feature and a change that breaks backward compatibility. You can see all changes <a href="https://cimentadaj.github.io/ess/news/index.html">here</a>.</p>
<p>Install the latest version <code>0.1.1</code> from CRAN with <code>install.packages(&quot;ess&quot;)</code>.</p>
<div id="new-features" class="section level2">
<h2>New features</h2>
<p>When downloading any round(s) from the European Social Survey the files are always accompanied by a script that recodes values like 6, 7, 8 and 9 or 96, 97, 98 and 99 to missings, depending on the question. This is a bit tricky because a question with a scale from 1 to 5 will have 6 to 9 as missing values and a question with a scale from 1 to 10 will have the missing values set as 96 to 99. The new <code>remove_missings()</code> function removes all missing values from all questions.</p>
<p>For example…</p>
<pre class="r"><code>library(tidyverse)
library(ess)

clean_df &lt;-
  ess_rounds(1, &quot;your_email@gmail.com&quot;) %&gt;%
  recode_missings()</code></pre>
<p>… will set all the missing categories to NA. That is, the 6 to 9 and 96 to 99 categories on the specific questions. It gives you flexibility in recoding specific categories such as ‘Don’t Know’, ‘Refusal’ or both.</p>
<pre class="r"><code>another_clean_df &lt;-
  ess_rounds(1, &quot;your_email@gmail.com&quot;) %&gt;%
  recode_missings(c(&quot;Refusal&quot;, &quot;No answer&quot;))</code></pre>
<p>See <code>?recode_missings</code> for the missing categories that are available for recode.</p>
<p>However, I do not advise recoding missing values right away if you’re exploring the dataset. If you want to manually recode missing values you can use the <code>recode_numeric_missing()</code> and <code>recode_strings_missing</code> correspondingly on numeric and string variables. They work the same as <code>recode_missings</code> but accept a vector of class labelled, the class of each of the columns that returns the <code>ess_*</code> functions.</p>
<p>For example</p>
<pre class="r"><code>another_clean_df$tvtot &lt;-
  recode_numeric_missing(
    another_clean_df$tvtot,
    &quot;Don&#39;t know&quot;
    )</code></pre>
<p>works for recoding the “Don’t know” category. By default all missing values are chosen.</p>
<p>Note that both sets of functions <strong>only</strong> work with labelled numeric vectors from the <code>haven</code> package. If you use the <code>ess</code> package that’s taken care of. If you download the data manually, you must read it with the <code>haven</code> package for these functions to work.</p>
<p>There are also two new <code>show_*</code> functions, namely <code>show_themes</code> and <code>show_rounds_country</code>.</p>
<p>The first one returns all available themes…</p>
<pre class="r"><code>show_themes()</code></pre>
<pre><code>##  [1] &quot;Ageism&quot;                            
##  [2] &quot;Citizen involvement&quot;               
##  [3] &quot;Democracy&quot;                         
##  [4] &quot;Economic morality&quot;                 
##  [5] &quot;Family work and well-being&quot;        
##  [6] &quot;Gender, Household&quot;                 
##  [7] &quot;Health and care&quot;                   
##  [8] &quot;Human values&quot;                      
##  [9] &quot;Immigration&quot;                       
## [10] &quot;Justice&quot;                           
## [11] &quot;Media and social trust&quot;            
## [12] &quot;Personal ... well-being&quot;           
## [13] &quot;Politics&quot;                          
## [14] &quot;Public attitudes to climate change&quot;
## [15] &quot;Social inequalities in health&quot;     
## [16] &quot;Socio demographics&quot;                
## [17] &quot;Subjective well-being...&quot;          
## [18] &quot;Timing of life&quot;                    
## [19] &quot;Welfare attitudes&quot;</code></pre>
<p>… but doesn’t haven a corresponding <code>ess_*</code> function. This means that it works purely for descriptive purposes.</p>
<p>Additionaly, <code>show_rounds_country</code> returns all countries that participated in a give round.</p>
<pre class="r"><code>show_rounds_country(rounds = 2)</code></pre>
<pre><code>##  [1] &quot;Austria&quot;        &quot;Belgium&quot;        &quot;Czech Republic&quot; &quot;Denmark&quot;       
##  [5] &quot;Estonia&quot;        &quot;Finland&quot;        &quot;France&quot;         &quot;Germany&quot;       
##  [9] &quot;Greece&quot;         &quot;Hungary&quot;        &quot;Iceland&quot;        &quot;Ireland&quot;       
## [13] &quot;Italy&quot;          &quot;Luxembourg&quot;     &quot;Netherlands&quot;    &quot;Norway&quot;        
## [17] &quot;Poland&quot;         &quot;Portugal&quot;       &quot;Slovakia&quot;       &quot;Slovenia&quot;      
## [21] &quot;Spain&quot;          &quot;Sweden&quot;         &quot;Switzerland&quot;    &quot;Turkey&quot;        
## [25] &quot;Ukraine&quot;        &quot;United Kingdom&quot;</code></pre>
<p>You could use this to see which countries participated in all rounds. For example..</p>
<pre class="r"><code>all_countries &lt;-
  map(show_rounds(), ~ show_rounds_country(.x)) %&gt;%
  reduce(intersect)

all_countries</code></pre>
<pre><code>##  [1] &quot;Belgium&quot;        &quot;Finland&quot;        &quot;France&quot;         &quot;Germany&quot;       
##  [5] &quot;Ireland&quot;        &quot;Netherlands&quot;    &quot;Norway&quot;         &quot;Poland&quot;        
##  [9] &quot;Slovenia&quot;       &quot;Sweden&quot;         &quot;Switzerland&quot;    &quot;United Kingdom&quot;</code></pre>
</div>
<div id="breaking-changes" class="section level2">
<h2>Breaking changes</h2>
<p>Finally, there is one change that breaks backward compatability. All the <code>ess_*</code> functions always used to return a list, regardless of the number of rounds that were requested. Now, <code>ess_*</code> functions return a <code>tibble</code> whenever it is request only one round and a list when more than one round is requested.</p>
<p>For example</p>
<pre class="r"><code>ess_rounds(1, &quot;your_email@gmail.com&quot;)</code></pre>
<p>will return a tibble but…</p>
<pre class="r"><code>ess_rounds(1:3, &quot;your_email@gmail.com&quot;)</code></pre>
<p>…will return a list with each tibble in a slot.</p>
<p>For more concrete examples check out the new website of the ess <a href="https://cimentadaj.github.io/ess/">here</a>. If you have any ideas for features or find a bug, please report <a href="https://github.com/cimentadaj/ess/issues">here</a>.</p>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>What time should I ride my bike?</title>
      <link>/blog/2018-02-12-what-time-should-i-ride-my-bike/what-time-should-i-ride-my-bike/</link>
      <pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-02-12-what-time-should-i-ride-my-bike/what-time-should-i-ride-my-bike/</guid>
      <description><![CDATA[
      


<p>For a few months now I’ve started developing a project on which I download live bicycle usage from the API of Bicing, the public bicycle service from the city of Barcelona. Before I started analyzing the data I wanted to harvest a reasonable amount of data to be able to get a representative sample of bicycle usage.</p>
<ol style="list-style-type: decimal">
<li><p>The first thing I did was to set up my Virtual Private Server (VPS) and set a <code>cron</code> job to email me every day after the scraping of the data is done. Check out a detailed tutorial on how to this <a href="blog/2017-12-01-how-long-should-i-wait-for-my-bike/how-long-should-i-wait-for-my-bike/index.html">here</a></p></li>
<li><p>The second thing I did was to set up a MySQL database in my VPS and develop a program that interacts with the Barcelona Public Bicycle System API and feeds the database on a daily basis. Check out a detailed tutorial on how I did it <a href="blog/2018-01-31-scraping-at-scale-daily-scraping-to-your-database/scraping-at-scale-daily-scraping-to-your-database/index.html">here</a></p></li>
</ol>
<p>I left this program grabing biclycle data of a station close to my house only in the mornings and evenings (moments I used the bicycle) for the last 3 months. This is my first attempt to analyze this data. Please take this as a work in progress as I develop more fine-grained understanding of the data.</p>
<p>Here I load the libraries and connect to the database in my VPS. Note how I hide the IP of the server and the password by grabbing it as environment variables.</p>
<pre class="r"><code>library(DBI)
library(RMySQL)
library(caret)
library(viridis)
library(tidyverse)


password &lt;- Sys.getenv(&quot;password&quot;)
host &lt;- Sys.getenv(&quot;host&quot;)

con &lt;- dbConnect(MySQL(),
                 dbname = &quot;bicing&quot;, # in &quot;&quot; quotes
                 user = &quot;cimentadaj&quot;, # in &quot;&quot; quotes
                 password = password,
                 host = host) # ip of my server</code></pre>
<p>Next, let’s grab the data with a simple query. Let’s get some columns:</p>
<ol style="list-style-type: decimal">
<li><code>slots</code> is the number of available slots in the station</li>
<li><code>bikes</code> is the number of available bikes in the station</li>
</ol>
<p>These two columns are exact opposites. If the station can hold 20 bicycles and there are 8 slots available, then there’s 12 bicycles availables.</p>
<ol start="3" style="list-style-type: decimal">
<li><code>status</code> is the status of the station. Whether <code>OPN</code> or <code>CLOSED</code>.</li>
<li><code>time</code> is the specific date/time at which that row was returned from the API.</li>
</ol>
<p>There’s an additional column named <code>error_msg</code> that has the error message if the API couldn’t retrieve the data. Let’s use only those which were scraped correctly. Let’s write that query and grab the data.</p>
<pre class="r"><code>query &lt;- 
&quot;SELECT slots, bikes, status, time
 FROM bicing_station
 WHERE hour(time) IN (&#39;7&#39;, &#39;8&#39;, &#39;9&#39;, &#39;10&#39;, &#39;18&#39;, &#39;19&#39;, &#39;20&#39;)
 AND error_msg IS NULL;&quot;

bicing &lt;-
  dbGetQuery(con, query) %&gt;%
  as_tibble() %&gt;% 
  mutate(time = lubridate::ymd_hms(time),
         slots = as.numeric(slots),
         bikes = as.numeric(slots))</code></pre>
<p>Awesome. Now we have our data set.</p>
<pre class="r"><code>bicing</code></pre>
<pre><code>## # A tibble: 46,399 x 4
##    slots bikes status time               
##    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dttm&gt;             
##  1   10.   10. OPN    2017-12-11 08:01:16
##  2   10.   10. OPN    2017-12-11 08:02:12
##  3   10.   10. OPN    2017-12-11 08:03:04
##  4   10.   10. OPN    2017-12-11 08:04:04
##  5   10.   10. OPN    2017-12-11 08:05:04
##  6    9.    9. OPN    2017-12-11 08:06:04
##  7    8.    8. OPN    2017-12-11 08:07:04
##  8    8.    8. OPN    2017-12-11 08:08:05
##  9    7.    7. OPN    2017-12-11 08:09:04
## 10    8.    8. OPN    2017-12-11 08:10:04
## # ... with 46,389 more rows</code></pre>
<p>Let’s check if there’s any cases in which the station was not open.</p>
<pre class="r"><code>bicing %&gt;%
  filter(status != &quot;OPN&quot;)</code></pre>
<pre><code>## # A tibble: 0 x 4
## # ... with 4 variables: slots &lt;dbl&gt;, bikes &lt;dbl&gt;, status &lt;chr&gt;,
## #   time &lt;dttm&gt;</code></pre>
<p>Empty rows, alright, station has worked fine.</p>
<p>Let’s explore the number of bikes comparing between mornings/evenings</p>
<pre class="r"><code>summary_time &lt;-
  bicing %&gt;% 
  group_by(hour = as.factor(lubridate::hour(time))) %&gt;% 
  summarize(Average = mean(bikes, na.rm = TRUE),
            Median = median(bikes, na.rm = TRUE)) %&gt;% 
  gather(type, value, -hour)

bicing %&gt;%
  mutate(hour = as.factor(lubridate::hour(time))) %&gt;%
  ggplot(aes(hour, bikes)) +
  geom_jitter(alpha = 1/8) +
  geom_point(data = summary_time,
             aes(y = value, colour = type), size = 3) +
  theme_bw() +
  labs(x = &quot;Hour of the day (24H)&quot;,
       y = &quot;# of available bikes&quot;,
       title = &quot;Mornings have greater bicycle usage than evenings&quot;,
       subtitle = &quot;But number of bikes can vary betwen 0 and 20 in the morning&quot;) +
  scale_colour_manual(name = &quot;Types&quot;, values = c(&#39;Average&#39; = &#39;red&#39;, &#39;Median&#39; = &#39;blue&#39;))</code></pre>
<p><img src="/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>This is a bit revealing. Some take aways:</p>
<ol style="list-style-type: decimal">
<li><p>Mornings have greater number of bikes but they also have high variability. For example, look at the 8 AM category. Even though the average is at around 7 bikes, it’s also very likely that there’s 0 bikes as well as 20 bikes.</p></li>
<li><p>As time passes, more outliers appear in the distribution. We can infer this both from the overall distribution and the average and the mean are farther away from each other.</p></li>
</ol>
<p>This is probably related to how Bicing fills out the stations (a few times a days a truck with bicycles passes by the station and fills them out). I think this is beginning to tell a story although perhaps it’s too early: usage in the mornings is heavy and very dynamic but as the day passes by more a more bikes are taken (either by the Bicing team or by citizens).</p>
<p>This gives no clear clue to the layman citizen: if it’s 8 AM, how likely am I find bikes? Let’s inspect further.</p>
<p>Logically, the next question is: does this differ by day of the week? Bloew I plot the average number of bikes per day/hour combination. In addition we’d also want to plot some sort of uncertainty indicator like the standard deviation. However, because it’s very common for bikes to be close to 7-10 bikes as average and below, I plot the uncertainty as the percentage of times that the station has over 10 bikes.</p>
<pre class="r"><code>summary_time &lt;-
  bicing %&gt;% 
  group_by(hour = as.factor(lubridate::hour(time)),
           day = as.factor(lubridate::wday(time, label = TRUE, week_start = TRUE))) %&gt;% 
  summarize(Average = mean(bikes, na.rm = TRUE),
            Variation = mean(bikes &gt; 10, na.rm = TRUE)) %&gt;% 
  gather(type, value, -hour, -day)

p1 &lt;- 
  summary_time %&gt;% 
  filter(type == &quot;Average&quot;) %&gt;% 
  ggplot(aes(hour, day, fill = value)) + 
  geom_tile() +
  scale_fill_viridis(name = &quot;Avg # of bikes&quot;) +
  labs(x = &#39;Hour of the day (24H)&#39;,
       y = &#39;Day of the week&#39;,
       title = &#39;Average number of bikes has a workin week/end of week divide&#39;,
       subtitle = &#39;Thu and Wed seem to have high peaks at 8, Sun and Sat have peaks at 10&#39;)

p2 &lt;-
  summary_time %&gt;% 
  filter(type == &quot;Variation&quot;) %&gt;% 
  ggplot(aes(hour, day, fill = value)) + 
  geom_tile() +
  scale_fill_viridis(name = &#39;% of times \n station has &gt; 10 bikes&#39;) +
  labs(x = &#39;Hour of the day (24H)&#39;,
       y = &#39;Day of the week&#39;,
       title = &#39;Variability reflects same pattern as average # of bikes&#39;,
       subtitle = &#39;Thu and Wed seem to have &gt; 10 bikes often at 8, Sun and Sat have peaks at 10&#39;)

p1</code></pre>
<p><img src="/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>p2</code></pre>
<p><img src="/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Similarly, we can see whether there’s a clear morning/evening divide by looking at the percentage of bikes for every day in the evening and the morning.</p>
<pre class="r"><code>bicing %&gt;%
  mutate(hour = lubridate::hour(time),
         day = lubridate::wday(time, label = TRUE, week_start = TRUE),
         morning_evening = ifelse(hour &lt;= 10, &quot;Morning&quot;, &quot;Evening&quot;)) %&gt;%
  ggplot(aes(day, bikes, fill = morning_evening)) +
  geom_col(position = &quot;fill&quot;) +
  labs(x = &quot;Day of the week&quot;,
       y = &quot;% of bikes&quot;,
       title = &#39;# of bikes increases linearly through out the week&#39;,
       fill = &#39;Time of day&#39;)</code></pre>
<p><img src="/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Alright, so we got that down. So far:</p>
<ol style="list-style-type: decimal">
<li><p>There’s more taking and droping happening in the first few days of the week than in the rest</p></li>
<li><p>Weekdays and weekends have different patterns in bike usage; namely that bike usage is higher in earlier in the week than in the weekends.</p></li>
<li><p>More bikes are taken in the early days of the weeks than in the latter parts.</p></li>
</ol>
<p>Following the previous conclusion, I had the itching of figuring out the rate at which bicycles are taken out by hour. This we can depart from the total or average number of bikes, to actual rate of picking/droping bikes. This can help to pinpoint specific times at which we should avoid going or droping a bike.</p>
<p>What’s the rate at which bicycles are being taken out by hour? At which time is the station emptying out quicker?</p>
<p>I’ve computed a metric that calculates the percentage of minutes that there’s changes in the station.</p>
<pre class="r"><code>intm_df &lt;-
  bicing %&gt;%
  mutate(hour = as.factor(lubridate::hour(time)),
         day = lubridate::wday(time, label = TRUE, week_start = TRUE)) %&gt;%
  group_by(hour, day) %&gt;%
  mutate(future_bike = lag(bikes)) %&gt;%
  summarize(avg_movement = mean(bikes != future_bike, na.rm = TRUE) * 60) %&gt;%
  ungroup()

intm_df %&gt;% 
  ggplot(aes(hour, avg_movement, colour = day, group = day)) +
  geom_line(size = 1.2) +
  facet_wrap(~ day, ncol = 4) +
  theme_bw() +
  labs(x = &#39;Hour of the day (24H)&#39;,
       y = &quot;Minutes per hour with a bicycle change&quot;,
       title = &#39;Weekdays have much greater bicycle usage than weekends&#39;,
       subtitle = &quot;Wed has the busiest hour of the week at 8AM; There&#39;s activity 25 minutes out of the 60 minutes.&quot;) +
  theme(legend.position = &#39;none&#39;)</code></pre>
<p><img src="/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>This is very interesting! This plot reverses some of the findings from before. First off, we see that there’s high variability between hours: there’s no point in looking at averages within an hour because a lot happens between minutes. For example, at 7AM and 10 AM during working days there’s very little activity regardless of the day. On the contrary, 8AM and 9AM have high bycicle usage through the working days.</p>
<p>This makes sense, it’s the time that people usually go to work. Also as expected, bicycle usage is low on weekend mornings and increases linearly through out the day. All in all, 8/9 AM on working days seems to be the time to avoid bicycles if you can! Eventually, in another post, I plan to investigate whether there’s minute-to-minute patterns for 8/9 AM on working days. For example, is there more activity closer to certain minutes? Like half past the hour or at exactly the hour.</p>
<p>Also, it seems that evenings are busy even on working days, specially on Thursdays but have very little bicycle usage on Fridays! Perhaps Catalans are ready to party and travel on the metro. On my follow up post, I also plan to see whether these patterns hold by season. I would expect summer and winter to have strong seasonal patterns.</p>
<p>To begin the conclusion, when are the moments when the station is empty? This will trigger me to avoid picking bicycles on those specific times.</p>
<pre class="r"><code>bicing %&gt;%
  filter(bikes == 0) %&gt;%
  mutate(time_day = as.numeric(lubridate::hm(str_extract(time, &quot;[0-9]{2}:[0-9]{2}&quot;)))) %&gt;% 
  ggplot(aes(x = time_day)) +
  geom_histogram()</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Bicing people probably prepare very well because it’s mostly empty in the evenings</p>
<p>The next step in the analysis is to start making predictions in waiting time. That will be the topic of my next post, in which I start to develop a modelling approach to predict the time you’ll have to have wait until a bicycle arrives or leaves. As a very simple exercise, I wanted to predict check whether I can predict when the station will be empty? I tried a simple logistic regression just to check.</p>
<pre class="r"><code>empty_bicycle &lt;-
  mutate(bicing,
         empty = ifelse(bikes == 0, 1, 0),
         hour = as.character(lubridate::hour(time)),
         day = lubridate::wday(time),
         day = as.character(day)) %&gt;%
  select(-(1:4))

training_rows &lt;- createDataPartition(empty_bicycle$empty, 1, p = 0.8)[[1]]

training &lt;- empty_bicycle[training_rows, ]
test &lt;- empty_bicycle[-training_rows, ]

mod1 &lt;- glm(empty ~ . + day:hour, data = training, family = &quot;binomial&quot;)

pred1 &lt;- predict(mod1, newdata = test, type = &#39;response&#39;)

pred_empty &lt;- rbinom(length(pred1), 1, prob = pred1)

confusionMatrix(test$empty, pred_empty, positive = &quot;1&quot;)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 6693 1162
##          1 1103  321
##                                          
##                Accuracy : 0.7559         
##                  95% CI : (0.747, 0.7646)
##     No Information Rate : 0.8402         
##     P-Value [Acc &gt; NIR] : 1.000          
##                                          
##                   Kappa : 0.0762         
##  Mcnemar&#39;s Test P-Value : 0.223          
##                                          
##             Sensitivity : 0.21645        
##             Specificity : 0.85852        
##          Pos Pred Value : 0.22542        
##          Neg Pred Value : 0.85207        
##              Prevalence : 0.15982        
##          Detection Rate : 0.03459        
##    Detection Prevalence : 0.15346        
##       Balanced Accuracy : 0.53749        
##                                          
##        &#39;Positive&#39; Class : 1              
## </code></pre>
<p>This model is terrible at predicting the emptyness of the stations as it can only predict 20% of the time. A few strategies I could check out to improve accuracy:</p>
<ul>
<li>Feature engineer when the bicing team picks up bicycles (because they leave them empty)</li>
<li>Add more information on weather and public holidays from public API’s</li>
<li>Because the cell that contains empty stations has very few cases, it might be useful to resample that sample until it reaches a similar sample size as the other cells. This might give greater certainty and I assume that there’s not a lot of variability in the pattern of empty stations, so it should be representative.</li>
</ul>
<p>Finally, other classification models are certainly warranted. One good alternative would be a random forest, as it takes into consideration specific thresholds in the time of day when prunning the trees.</p>
<p>However, we also need to be aware that a model is as good as the data that’s being fit. Perhaps, we just need better data!</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Rewriting duplicated</title>
      <link>/blog/2018-02-06-rewriting-duplicated/rewriting-duplicated/</link>
      <pubDate>Tue, 06 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-02-06-rewriting-duplicated/rewriting-duplicated/</guid>
      <description><![CDATA[
      


<p>Lightning post. I got very confused earlier today on how to use <code>duplicated</code>. Basically, I didn’t know if it was picking only one duplicate or many of the duplicates at the same time. I figure it out but I still was a bit confused so I decided to rewrite the function from scratch. Below you can see it. Please post any other solutions or feedback.</p>
<pre class="r"><code>dupl_identifier &lt;- function(vec, where) {
  intm &lt;- x %in% vec
  pos &lt;- which(intm)
  intm[where(pos)] &lt;- FALSE
  intm
}

my_duplicated &lt;- function(x, fromLast = FALSE) {
  
  where &lt;- ifelse(!fromLast, min, max)
  repeated &lt;- names(which(table(x) &gt; 1))
  
  if (length(repeated) == 0) return(rep(FALSE, length(x)))
  
  val &lt;- lapply(repeated, dupl_identifier, where)
  final &lt;- as.logical(Reduce(`+`, val))
  
  final
}</code></pre>
<pre class="r"><code>x &lt;- sample(1:10, 100, replace = TRUE)

identical(my_duplicated(x),
          duplicated(x))</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>x &lt;- sample(c(1:100, NA), 100, replace = TRUE)

identical(my_duplicated(x),
          duplicated(x))</code></pre>
<pre><code>## [1] TRUE</code></pre>
]]>
      </description>
    </item>
    
    <item>
      <title>Cleaning in-door positioning data</title>
      <link>/blog/2018-02-03-predicting-location-via-indoor-positioning-systems/predicting-location-via-indoor-positioning-systems/</link>
      <pubDate>Sat, 03 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-02-03-predicting-location-via-indoor-positioning-systems/predicting-location-via-indoor-positioning-systems/</guid>
      <description><![CDATA[
      


<p>I’ve just started reading the wonderful book <a href="http://rdatasciencecases.org/">Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving</a>. I’ve just begun the first chapter and I wanted to document some of the things I found interesting. In this post I’ll walkthrough the example on how to transform a text file with GPS locations into a well formatted rectangular dataset. For a detailed explanation see their book, which I highly recommend buying.</p>
<p>Note: When it makes senses/it’s possible, I always try to find an equivalent tidyverse solution to everything they do in the book.</p>
<p>This is the data.</p>
<pre class="r"><code>library(tidyverse)

ex_file &lt;- read_lines(&quot;http://rdatasciencecases.org/Data/offline.final.trace.txt&quot;)
ex_file[1:4]</code></pre>
<pre><code>## [1] &quot;# timestamp=2006-02-11 08:31:58&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                 
## [2] &quot;# usec=250&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                      
## [3] &quot;# minReadings=110&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                               
## [4] &quot;t=1139643118358;id=00:02:2D:21:0F:33;pos=0.0,0.0,0.0;degree=0.0;00:14:bf:b1:97:8a=-38,2437000000,3;00:14:bf:b1:97:90=-56,2427000000,3;00:0f:a3:39:e1:c0=-53,2462000000,3;00:14:bf:b1:97:8d=-65,2442000000,3;00:14:bf:b1:97:81=-65,2422000000,3;00:14:bf:3b:c7:c6=-66,2432000000,3;00:0f:a3:39:dd:cd=-75,2412000000,3;00:0f:a3:39:e0:4b=-78,2462000000,3;00:0f:a3:39:e2:10=-87,2437000000,3;02:64:fb:68:52:e6=-88,2447000000,1;02:00:42:55:31:00=-84,2457000000,1&quot;</code></pre>
<p>Some lines are comments and the 4th line is the actual data. Basically, everything that is <code>something=</code> is the name of the column and columns are separated by a <code>;</code>. Now, within each column there can also be several values like in the column <code>pos</code> where numbers are separated by a comma.</p>
<p>First, let’s separate everything now that we know all of the delimiters.</p>
<pre class="r"><code>tokens &lt;- str_split(ex_file[4], pattern = &quot;[;=,]&quot;)[[1]]</code></pre>
<p>From the documentation we know that the first 4 columns are constant in every line. The remaining columns can vary by each line, which is why they decide to transform the data into stacked/long format. So each unique <code>id</code> will be repeate the number of times that there’s MAC columns (the columns that vary).</p>
<pre class="r"><code>tmp &lt;- matrix(tokens[-(1:10)], ncol = 4, byrow = TRUE)
# We got the MAC in a long format, now we have to get unique id
# of each of the macs (along with time and other vars) to be repeated
# the number of rows that tmp has


# There we go
tmp_two &lt;- matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, byrow = TRUE)

mat &lt;- cbind(tmp_two, tmp)
mat</code></pre>
<pre><code>##       [,1]            [,2]                [,3]  [,4]  [,5]  [,6] 
##  [1,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [2,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [3,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [4,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [5,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [6,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [7,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [8,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [9,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
## [10,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
## [11,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##       [,7]                [,8]  [,9]         [,10]
##  [1,] &quot;00:14:bf:b1:97:8a&quot; &quot;-38&quot; &quot;2437000000&quot; &quot;3&quot;  
##  [2,] &quot;00:14:bf:b1:97:90&quot; &quot;-56&quot; &quot;2427000000&quot; &quot;3&quot;  
##  [3,] &quot;00:0f:a3:39:e1:c0&quot; &quot;-53&quot; &quot;2462000000&quot; &quot;3&quot;  
##  [4,] &quot;00:14:bf:b1:97:8d&quot; &quot;-65&quot; &quot;2442000000&quot; &quot;3&quot;  
##  [5,] &quot;00:14:bf:b1:97:81&quot; &quot;-65&quot; &quot;2422000000&quot; &quot;3&quot;  
##  [6,] &quot;00:14:bf:3b:c7:c6&quot; &quot;-66&quot; &quot;2432000000&quot; &quot;3&quot;  
##  [7,] &quot;00:0f:a3:39:dd:cd&quot; &quot;-75&quot; &quot;2412000000&quot; &quot;3&quot;  
##  [8,] &quot;00:0f:a3:39:e0:4b&quot; &quot;-78&quot; &quot;2462000000&quot; &quot;3&quot;  
##  [9,] &quot;00:0f:a3:39:e2:10&quot; &quot;-87&quot; &quot;2437000000&quot; &quot;3&quot;  
## [10,] &quot;02:64:fb:68:52:e6&quot; &quot;-88&quot; &quot;2447000000&quot; &quot;1&quot;  
## [11,] &quot;02:00:42:55:31:00&quot; &quot;-84&quot; &quot;2457000000&quot; &quot;1&quot;</code></pre>
<p>There we go. We have a stacked matrix with all the variables we need. Let’s wrap the line maker into a function:</p>
<pre class="r"><code>processLine &lt;- function(x) {
  tokens &lt;- str_split(x, pattern = &quot;[;=,]&quot;)[[1]]
  
  # We got the MAC in a long format, now we have to get unique id
  # of each of the macs (along with time and other vars) to be repeated
  # the number of rows that tmp has
  tmp &lt;- matrix(tokens[-(1:10)], ncol = 4, byrow = TRUE)
  
  # There we go
  tmp_two &lt;- matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, byrow = TRUE)
  
  mat &lt;- cbind(tmp_two, tmp)
  mat
}</code></pre>
<p>Let’s apply it to a few sample rows:</p>
<pre class="r"><code>tmp &lt;- map(ex_file[4:20], processLine)

offline &lt;- as.data.frame(do.call(&quot;rbind&quot;, tmp))
head(offline)</code></pre>
<pre><code>##              V1                V2  V3  V4  V5  V6                V7  V8
## 1 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:8a -38
## 2 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:90 -56
## 3 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:0f:a3:39:e1:c0 -53
## 4 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:8d -65
## 5 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:81 -65
## 6 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:3b:c7:c6 -66
##           V9 V10
## 1 2437000000   3
## 2 2427000000   3
## 3 2462000000   3
## 4 2442000000   3
## 5 2422000000   3
## 6 2432000000   3</code></pre>
<p>Good! Now we can apply it to all lines, excluding of course the ones which are commented out!</p>
<pre class="r"><code>tmp &lt;- map(ex_file[!str_sub(ex_file, 1, 1) == &quot;#&quot;], processLine)</code></pre>
<pre><code>## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix</code></pre>
<p>Aha.. so there’s a few warnings? What’s happening? If we ran the previous with <code>options(error, warn = 2)</code> we would see that it looks like there are some anomalous cases where there’s no MAC information. We either fill out those values with NA’s or we simply exclude them. Because working with the MAC’s is of utmost importance for the analysis, we drop it to save memory. We redefine our function so that if there’s only the 10 starting values it returns a NULL.</p>
<pre class="r"><code>processLine &lt;- function(x) {
  tokens &lt;- str_split(x, pattern = &quot;[;=,]&quot;)[[1]]
  
  # We exclude rows where there&#39;s no MAC information
  if (length(tokens) == 10) return(NULL)
  
  # We got the MAC in a long format, now we have to get unique id
  # of each of the macs (along with time and other vars) to be repeated
  # the number of rows that tmp has
  tmp &lt;- matrix(tokens[-(1:10)], ncol = 4, byrow = TRUE)
  
  # There we go
  tmp_two &lt;- matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, byrow = TRUE)
  
  mat &lt;- cbind(tmp_two, tmp)
  mat
}</code></pre>
<p>And apply it now..</p>
<pre class="r"><code>tmp &lt;- map(ex_file[!str_sub(ex_file, 1, 1) == &quot;#&quot;], processLine)

offline &lt;- as_tibble(do.call(&quot;rbind&quot;, tmp))</code></pre>
<p>Good, let’s set warnings back: <code>options(error = recover, warn = 1)</code></p>
<p>To finish off let’s set some names.</p>
<pre class="r"><code>names(offline) &lt;- c(&quot;time&quot;, &quot;scanMac&quot;, &quot;posX&quot;, &quot;posY&quot;, &quot;posZ&quot;,
                    &quot;orientation&quot;, &quot;mac&quot;, &quot;signal&quot;, &quot;channel&quot;, &quot;type&quot;)

offline</code></pre>
<pre><code>## # A tibble: 1,181,628 x 10
##    time   scanMac posX  posY  posZ  orientation mac   signal channel type 
##    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;
##  1 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -38    243700~ 3    
##  2 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -56    242700~ 3    
##  3 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -53    246200~ 3    
##  4 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -65    244200~ 3    
##  5 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -65    242200~ 3    
##  6 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -66    243200~ 3    
##  7 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -75    241200~ 3    
##  8 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -78    246200~ 3    
##  9 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -87    243700~ 3    
## 10 11396~ 00:02:~ 0.0   0.0   0.0   0.0         02:6~ -88    244700~ 1    
## # ... with 1,181,618 more rows</code></pre>
<p>— <strong>BONUS</strong> —</p>
<p>Just wanted to try to get the data in a wide format where each MAC indicator is a column rather than stacked.</p>
<pre class="r"><code># Define the MAC colums as wide. Because each MAC columns
# has three associated values, I stack them up so there should
# be three rows pero every MAC column
right_col &lt;- tokens[-(1:10)]

right_names &lt;- seq(1, length(right_col), by = 4)

mac_tibble &lt;-
  matrix(right_col[-right_names], nrow = 3, ncol = length(right_names),
         dimnames = list(NULL, right_col[right_names])) %&gt;%
  as_tibble() %&gt;%
  add_column(mac_indicators = c(&quot;signal&quot;, &quot;chanel&quot;, &quot;type&quot;),
             .before = 1)

# Define the first four columns
left_col &lt;- tokens[1:10]

left_names &lt;- seq(1, length(left_col), by = 2)

left_tibble &lt;-
  matrix(left_col[-left_names], nrow = 3, ncol = length(left_names), byrow = TRUE,
         dimnames = list(NULL, left_col[left_names])) %&gt;%
  as_tibble()

# Bind both dfs
mat &lt;- bind_cols(left_tibble, mac_tibble)
mat</code></pre>
<pre><code>## # A tibble: 3 x 17
##   t         id         pos   `0.0` degree mac_indicators `00:14:bf:b1:97:~
##   &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;            
## 1 11396431~ 00:02:2D:~ 0.0   0.0   0.0    signal         -38              
## 2 11396431~ 00:02:2D:~ 0.0   0.0   0.0    chanel         2437000000       
## 3 11396431~ 00:02:2D:~ 0.0   0.0   0.0    type           3                
## # ... with 10 more variables: `00:14:bf:b1:97:90` &lt;chr&gt;,
## #   `00:0f:a3:39:e1:c0` &lt;chr&gt;, `00:14:bf:b1:97:8d` &lt;chr&gt;,
## #   `00:14:bf:b1:97:81` &lt;chr&gt;, `00:14:bf:3b:c7:c6` &lt;chr&gt;,
## #   `00:0f:a3:39:dd:cd` &lt;chr&gt;, `00:0f:a3:39:e0:4b` &lt;chr&gt;,
## #   `00:0f:a3:39:e2:10` &lt;chr&gt;, `02:64:fb:68:52:e6` &lt;chr&gt;,
## #   `02:00:42:55:31:00` &lt;chr&gt;</code></pre>
<p>Let’s wrap it into a function excluding those which dont have MAC values.</p>
<pre class="r"><code>processLine &lt;- function(x) {
  tokens &lt;- str_split(x, pattern = &quot;[;=,]&quot;)[[1]]
  
  if (length(tokens) == 10) return(NULL) # exclude non-MAC lines
  
  right_col &lt;- tokens[-(1:10)]
  
  right_names &lt;- seq(1, length(right_col), by = 4)
  
  mac_tibble &lt;-
    matrix(right_col[-right_names], nrow = 3, ncol = length(right_names),
           dimnames = list(NULL, right_col[right_names]))

  # Define the first four columns
  left_col &lt;- tokens[1:10]
  
  left_names &lt;- seq(1, length(left_col), by = 2)
  
  left_tibble &lt;-
    matrix(left_col[-left_names], nrow = 3, ncol = length(left_names), byrow = TRUE,
           dimnames = list(NULL, left_col[left_names]))

  # Bind both dfs
  mat &lt;- cbind(left_tibble, mac_tibble)
  mat
}</code></pre>
<p>Let’s apply it to each line:</p>
<pre class="r"><code>tmp &lt;- map(ex_file[!str_sub(ex_file, 1, 1) == &quot;#&quot;], processLine)

# Interestingly, applying as_tibble instead of as.data.frame is
# very slow. So I opt for data frame and then convert the binded
# df to a tibble
final_data &lt;-
  bind_rows(map(tmp, as.data.frame, stringsAsFactors = FALSE)) %&gt;%
  as_tibble() %&gt;%
  add_column(mac_indicators = rep(c(&quot;signal&quot;, &quot;chanel&quot;, &quot;type&quot;), length(unique(.$t))),
             .after = &quot;degree&quot;)

final_data</code></pre>
<pre><code>## # A tibble: 438,222 x 40
##    t         id        pos   `0.0` degree mac_indicators `00:14:bf:b1:97:~
##    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;            
##  1 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
##  2 11396431~ 00:02:2D~ 0.0   0.0   0.0    chanel         2437000000       
##  3 11396431~ 00:02:2D~ 0.0   0.0   0.0    type           3                
##  4 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
##  5 11396431~ 00:02:2D~ 0.0   0.0   0.0    chanel         2437000000       
##  6 11396431~ 00:02:2D~ 0.0   0.0   0.0    type           3                
##  7 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
##  8 11396431~ 00:02:2D~ 0.0   0.0   0.0    chanel         2437000000       
##  9 11396431~ 00:02:2D~ 0.0   0.0   0.0    type           3                
## 10 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
## # ... with 438,212 more rows, and 33 more variables:
## #   `00:14:bf:b1:97:90` &lt;chr&gt;, `00:0f:a3:39:e1:c0` &lt;chr&gt;,
## #   `00:14:bf:b1:97:8d` &lt;chr&gt;, `00:14:bf:b1:97:81` &lt;chr&gt;,
## #   `00:14:bf:3b:c7:c6` &lt;chr&gt;, `00:0f:a3:39:dd:cd` &lt;chr&gt;,
## #   `00:0f:a3:39:e0:4b` &lt;chr&gt;, `00:0f:a3:39:e2:10` &lt;chr&gt;,
## #   `02:64:fb:68:52:e6` &lt;chr&gt;, `02:00:42:55:31:00` &lt;chr&gt;,
## #   `00:04:0e:5c:23:fc` &lt;chr&gt;, `00:30:bd:f8:7f:c5` &lt;chr&gt;, `1.0` &lt;chr&gt;,
## #   `2.0` &lt;chr&gt;, `3.0` &lt;chr&gt;, `4.0` &lt;chr&gt;, `5.0` &lt;chr&gt;, `6.0` &lt;chr&gt;,
## #   `7.0` &lt;chr&gt;, `8.0` &lt;chr&gt;, `9.0` &lt;chr&gt;, `10.0` &lt;chr&gt;, `11.0` &lt;chr&gt;,
## #   `12.0` &lt;chr&gt;, `13.0` &lt;chr&gt;, `00:e0:63:82:8b:a9` &lt;chr&gt;,
## #   `02:37:fd:3b:54:b5` &lt;chr&gt;, `02:2e:58:22:f1:ac` &lt;chr&gt;,
## #   `02:42:1c:4e:b5:c0` &lt;chr&gt;, `02:0a:3d:06:94:88` &lt;chr&gt;,
## #   `02:5c:e0:50:49:de` &lt;chr&gt;, `02:4f:99:43:30:cd` &lt;chr&gt;,
## #   `02:b7:00:bb:a9:35` &lt;chr&gt;</code></pre>
<p>There we go! It’s a bit refreshing to work on datasets that are not pre-cleaned for you.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Scraping at scale: daily scraping to your database</title>
      <link>/blog/2018-01-31-scraping-at-scale-daily-scraping-to-your-database/scraping-at-scale-daily-scraping-to-your-database/</link>
      <pubDate>Wed, 31 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-01-31-scraping-at-scale-daily-scraping-to-your-database/scraping-at-scale-daily-scraping-to-your-database/</guid>
      <description><![CDATA[
      


<p>I’ve been working on a personal project to gather daily data from public bicycles in Barcelona to create a historical timeline of a few stations. Since the data is only available live, I had to scrape the data and store in a database daily. This is a short tutorial showing the steps I had to take to setup a database on my remote server and connect both from my local computer as well as from my server. I also show the R script that scrapes data, connects to the server and appends the information every day for a certain amouint of time.</p>
<p><em>Note: This worked for my Digital Ocean droplet 512 MB and 20 GB disk with Ubuntu 16.04.3 x64.</em></p>
<p>Let’s get to it. It’s better to do <em>ALL</em> of this as a user in your server but remember to append <code>sudo</code> to everything. Nonetheless, beware of problems like the ones I encountered. For example, when installing R packages that where ran by <code>cron</code> in a script, if installed through a non-root user the packages were said to be <code>'not installed'</code> (when I fact running the script separately was fine). However, when I installed the packages logged in as root the packages were installed successfully.</p>
<div id="setting-up-the-data-base" class="section level2">
<h2>Setting up the data base</h2>
<p>All steps:</p>
<ul>
<li><p><a href="https://www.digitalocean.com/community/tutorials/how-to-install-r-on-ubuntu-16-04-2">Install R</a></p></li>
<li><p><a href="https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-16-04">Install MySQL</a></p></li>
<li><p>Type <code>mysql -u root -p</code> to log in to MySQL</p></li>
<li><p>Follow these steps to create an empty table within a database</p></li>
</ul>
<pre class="sql"><code>CREATE DATABASE bicing;
USE bicing;
CREATE TABLE bicing_station (id VARCHAR(30), slots VARCHAR(30), bikes VARCHAR(30), status VARCHAR(30), time VARCHAR(30), error VARCHAR(30));</code></pre>
<ul>
<li><p><a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-remote-database-to-optimize-site-performance-with-mysql">This</a> is an outdated guide by Digital Ocean which might be helpful. Some of the steps below are taken from that guide.</p></li>
<li><p>Alter <code>sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf</code> and change <code>bind-address</code> to have the ‘0.0.0.0’ This is so your server can listen to IP’s from outside the localhost network.</p></li>
<li><p>Create two users to access the data base: a user from your local computer and a user from your server.</p></li>
</ul>
<pre class="bash"><code>mysql -u root -p # Log in to MySQL. -u stands for user and -p for password</code></pre>
<pre class="sql"><code>/* Create user for local computer. Note that when username and ip are in &#39;&#39; they need to be in those quotes. Also, the ip address you can find easily by writing what&#39;s my ip in Google*/

CREATE USER &#39;username&#39;@&#39;ip_address_of_your_computer&#39; IDENTIFIED BY &#39;password&#39;;
GRANT ALL ON bicing.* TO username@ip_address_of_your_computer;

/* Create user for server. For this user don&#39;t change localhost as that already specifies that it belongs to the same computer. */

CREATE USER &#39;username&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;password&#39;;
GRANT ALL ON bicing.* TO username@localhost;

/* Make sure the privileges are isntalled */
FLUSH PRIVILEGES;

quit /* To quit MySQL*/</code></pre>
<ul>
<li>Test whether the access worked for both users</li>
</ul>
<pre class="bash"><code># Login from your server. Replace username for your username 
# -u stands for user and -p will ask for your password 
mysql -u username -h localhost -p


# Login from your LOCAL computer. Replace username for your username and your_server_ip from the server&#39;s IP
mysql -u username -h your_server_ip -p</code></pre>
<ul>
<li>Now install <code>odbc</code> in your Ubuntu server. I follow <a href="I%20followed%20this:%20https://askubuntu.com/questions/800216/installing-ubuntu-16-04-lts-how-to-install-odbc">this</a></li>
</ul>
<pre class="bash"><code>sudo mkdir mysql &amp;&amp; cd mysql

# Download odbc in mysql folder
sudo wget https://dev.mysql.com/get/Downloads/Connector-ODBC/5.3/mysql-connector-odbc-5.3.9-linux-ubuntu16.04-x86-64bit.tar.gz

# Unzip it and copy it somewhere.
sudo tar -xvf mysql-connector-odbc-5.3.9-linux-ubuntu16.04-x86-64bit.tar.gz 
sudo cp mysql/mysql-connector-odbc-5.3.9-linux-ubuntu16.04-x86-64bit/lib/libmyodbc5a.so /usr/lib/x86_64-linux-gnu/odbc/
# If the odbc folder doesn&#39;t exists, create it with mkdir /usr/lib/x86_64-linux-gnu/odbc/</code></pre>
<p>Note: you might need to change the url’s and directories to a <strong>newer</strong> version of <code>odbc</code> so don’t simply copy and paste the links from below.</p>
<ul>
<li>Create and update the <code>odbc</code> settings.</li>
</ul>
<pre class="bash"><code>sudo touch /etc/odbcinst.ini

sudo nano /etc/odbcinst.ini

# And add

[MySQL Driver]
Description = MySQL
Driver = /usr/lib/x86_64-linux-gnu/odbc/libmyodbc5a.so
Setup = /usr/lib/x86_64-linux-gnu/odbc/libodbcmyS.so
FileUsage = 1

# close the nano
# And continue

sudo touch /etc/odbc.ini

sudo nano /etc/odbc.ini

# and add

[MySQL]
Description           = MySQL connection to database
Driver                = MySQL Driver
Database              = dbname
Server                = 127.0.0.1
User                  = root
Password              = password
Port                  = 3306
Socket                = /var/run/mysqld/mysqld.sock

# Change Database to your database name
# The password to your root password

# Finally, run

sudo ln -s /var/run/mysqld/mysqld.sock /tmp/mysql.sock

# to move the socket to the folder where the DBI pkgs
# search for it

# Finish by

sudo service mysql restart;

# to restart mysql server</code></pre>
</div>
<div id="connecting-to-the-database-locally-and-remotely" class="section level2">
<h2>Connecting to the database locally and remotely</h2>
<p>From my local computer:</p>
<pre class="r"><code>library(DBI)
library(RMySQL)

con &lt;- dbConnect(MySQL(), # If the database changed, change this
                 host = your_server_ip, # in &quot;&quot; quotes.
                 dbname = &quot;bicing&quot;,
                 user = username, # remember to change to your username (in quotes)
                 password = password, # remember to change to your password (in quotes)
                 port = 3306)

dbListTables(con)

bike_stations &lt;- dbReadTable(con, &quot;bicing_station&quot;)</code></pre>
<p>From R in the server</p>
<pre class="r"><code>con &lt;- dbConnect(RMySQL::MySQL(),
                 dbname = &quot;bicing&quot;,
                 user = username, # remember to change to your username (in quotes)
                 password = password, # remember to change to your password (in quotes)
                 port = 3306)

dbListTables(con)

bike_stations &lt;- dbReadTable(con, &quot;bicing_station&quot;)</code></pre>
<p>That did it for me. Now I could connect to the database from R from my local computer and from the server itself.</p>
</div>
<div id="scraping-automatically" class="section level2">
<h2>Scraping automatically</h2>
<p>So far you should have a database in your server which you can connect locally and remotely. I assume you have a working script that can actually add/retrieve information from the remote database. Here I will explain how to set up the scraping to run automatically as a <code>cron</code> job and get a final email with the summary of the scrape.</p>
<ul>
<li><p>Create a text file to save the output of the scraping with <code>sudo touch scrape_log.txt</code></p></li>
<li><p>Write <code>cron -e</code> logged in as your non-root user.</p></li>
<li><p>At the bottom of the interactive <code>cron</code> specify these options:</p></li>
</ul>
<pre class="bash"><code>SHELL=/bin/bash # the path to the predetermined program to run cron jobs. Default bash

PATH=bla/bla/bla # PATH I’m not sure what’s for but I pasted the output of echo $PATH.

HOME= your/dr/ofinteres # Path to the directory where the scripts will be executed (where the script is)

MAILTO=&quot;your@email.com&quot; # Your email to receive emails

# The actual cron jobs. Below each job I explain them
30-59 11 * * * /usr/bin/Rscript scrape_bicing.R &gt;&gt;scrape_log.txt 2&gt;&amp;1

# Run this cron job from 11:30 to 11:59 every day (*), every month (*), every year(*): 30-59 11 * * *

# Use R to run the script: /usr/bin/Rscript
# You can find this directory with which Rscript

# Execute the file scrape_bicing.R (which is looked for in the HOME variable specified above)
# &gt;&gt;scrape_log.txt 2&gt;&amp;1: Save the output to scrape_log.txt (which we created) and DON&#39;T send an email
# because we don&#39;t want to received 29 emails.

00 12 * * * /bin/bash sql_query.sh
# Instead of receiving 29 emails, run a query the minute after the scraping ends
# to filter how many rows were added between 11:30 and 11:59
# By default it will send the result of the query to your email</code></pre>
<p>Great but what does <code>scrape_bicing.R</code> have?</p>
<p>The script should do something along the lines of:</p>
<pre class="r"><code># Load libraries
library(httr)
library(DBI)
library(RMySQL)

# The url of your api
api_url &lt;- &quot;bla/bla&quot;

# Wrap GET so that whenever the request fails it returns an R error
my_GET &lt;- function(x, config = list(), ...) {
  stop_for_status(GET(url = x, config = config, ...))
}

# If it can&#39;t connect to the API will throw an R error
test_bike &lt;- my_GET(api_url)


## Do API calls here
## I assume the result is a data.frame or something like that
## It should have the same column names as the SQL database.

# Establish the connection to the database.
# This script is run within the server, so the connection
# should not specify the server ip, it assumes it&#39;s
# the localhost

con &lt;- dbConnect(MySQL(),
                 dbname = database_name, # in &quot;&quot; quotes
                 user = your_user, # in &quot;&quot; quotes
                 password = your_password, # in &quot;&quot; quotes
                 port = 3306)

# Append the table
write_success &lt;-
  dbWriteTable(conn = con, # connection from above
              &quot;table name&quot;, # name of the table to append (in quotes)
              api output, # data frame from the API output
              append = TRUE, row.names = FALSE) # to append instead of overwrite and ignore row.names

# Write your results to the database. In my API call
# I considered many possible errors and coded the request
# very defensively, running the script many times under certain
# scenarios (no internet, getting different results).
# If you get unexpected results from your API request then this step will
# not succeed.


# If the append was successfull, write_success should be TRUE
if (write_success) print(&quot;Append success&quot;) else print(&quot;No success&quot;)</code></pre>
<p>Something to keep in mind, by default you can connect from the your local computer to the remote DB by port 3306. This port can be closed if you’re in a public internet network or a network connection from a university. If you can’t connect, make you sort this out with the personnel from that network (it happened to me with my university network).</p>
<p>What does <code>sql_query.sh</code> have?</p>
<p>A very simple SQL query:</p>
<pre class="sql"><code>read PASS &lt; pw.txt /* Read the password from a pw.txt file you create with your user pasword*/

mysql -uroot -p$PASS database_name -e &quot;SELECT id, error_msg, COUNT(*) AS count FROM bicing_station WHERE time &gt;= CONCAT(CURDATE(),&#39; &#39;,&#39;11:30:00&#39;) AND time &lt;= CONCAT(CURDATE(),&#39; &#39;,&#39;12:00:00&#39;) GROUP BY id, error_msg;&quot;

/*
mysql: run mysql

-uroot: specify your mysql username (note there are no spaces)

-p$PASS: -p is for password and $PASS is the variable with the password

database_name: is the data base name

-e: is short for -execute a query

The remaining is the query to execute. I would make sure the query
works by running this same line in the server interactively.

What this query means is to get the counts of the id and error messages
where the time is between the scheduele cron of the API request.

This way I get a summary of the error messages and how many lines were
appended between the time the script should&#39;ve started and should&#39;ve ended
*/
</code></pre>
<p>As stated in the first line of the code chunk, create a text file with your password. You can do so with <code>echo &quot;Your SQL username password&quot; &gt;&gt; pw.txt</code>. That should allow PASS to read in the password just fine.</p>
<p>And that should be it! Make sure you run each of these steps separately so that they work on it’s own and you don’get weird errors. This workflow will now run <code>cron</code> jobs at whatever time you set it, return the output to a text file (in case something bad happens and you want to look at the log) and run a query after it finishes so that you only get one email with a summary of API requests.</p>
<p>Hope this was helpful!</p>
<p>PS:</p>
<ul>
<li><p><a href="https://www.digitalocean.com/community/tutorials/a-basic-mysql-tutorial">Basic MySQL tutorial</a></p></li>
<li><p>I use SQL Workbench to run queries from my local computer</p></li>
</ul>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>Brief Analysis of Independent/Unionist Vote in Catalonia</title>
      <link>/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/brief-analysis-of-independent-unionist-vote-in-catalonia/</link>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/brief-analysis-of-independent-unionist-vote-in-catalonia/</guid>
      <description><![CDATA[
      


<div id="catalan-elections" class="section level2">
<h2>Catalan elections</h2>
<p>On a train from Barcelona-Madrid I started working with an <code>R</code> package called <code>ggrides</code>. To my surprise, the package contains one dataset that documents the change in independent/unionist vote for all Catalan municipalities from 1980 to 2015. This is very cool! Needless to say, I left what I was doing and started to dive into the dataset.</p>
<p>The data looks like this:</p>
<pre class="r"><code>library(ggridges)
Catalan_elections</code></pre>
<pre><code>## # A tibble: 20,764 x 4
##    Municipality        Year Option Percent
##    &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;
##  1 Abella de la Conca  1980 Indy      68.4
##  2 Abella de la Conca  1984 Indy      95.7
##  3 Abella de la Conca  1988 Indy      89.4
##  4 Abella de la Conca  1992 Indy      81.7
##  5 Abella de la Conca  1995 Indy      80.0
##  6 Abella de la Conca  1999 Indy      74.7
##  7 Abella de la Conca  2003 Indy      84.4
##  8 Abella de la Conca  2006 Indy      73.2
##  9 Abella de la Conca  2010 Indy      75.9
## 10 Abella de la Conca  2012 Indy      84.0
## # ... with 20,754 more rows</code></pre>
<p>Very straight forward. It’s the ‘Indy’ or Independent vote and the ‘Unionist’ vote from 1980 until 2015. The data is complete for nearly all Municipalities, meaning that the data is available for all years. Only a handful (~ 40) do not have data starting from 1980.</p>
<p>Basically, I wanted to answer one question: has the indepence vote grown over time? This question is of general interest considering that the topic is being hotly debated in the media and next week new Catalan elections will be held in a scenario never seen before; after independent parties proclaimed unilateral independece and the government seized control of Catalunya. The elections are predicted to be very contested with Independent parties losing some votes.</p>
<p>With that said, let’s dive into the data!</p>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<pre class="r"><code># Load my libraries
library(scales)
library(tidyverse)

# Change abbreviated Indy and Unionist to long names
Catalan_elections$Option &lt;- with(Catalan_elections, ifelse(Option == &quot;Indy&quot;, &quot;Independent&quot;, &quot;Unionist&quot;))


# Summarize the median independence/unionist vote for
# all municipalities on the first/last year recorded
avg_pl &lt;-
  Catalan_elections %&gt;%
  group_by(Municipality, Option) %&gt;%
  summarize(first_year = first(Percent, Year),
            last_year = last(Percent, Year)) %&gt;%
  group_by(Option) %&gt;%
  summarize(first_year = median(first_year, na.rm = TRUE),
            last_year = median(last_year, na.rm = TRUE)) %&gt;%
  mutate(id = 1:nrow(.)) %&gt;%
  gather(year, value, -id, -Option)

# Summarize the indy/unionist vote for
# the first/last year for Barcelona
bcn_pl &lt;-
  Catalan_elections %&gt;%
  filter(Municipality == &quot;Barcelona&quot;) %&gt;%
  group_by(Municipality, Option) %&gt;%
  summarize(first_year = first(Percent, Year),
            last_year = last(Percent, Year)) %&gt;%
  mutate(id = 1:nrow(.)) %&gt;%
  gather(year, value, ends_with(&quot;year&quot;))

# Create a base parallel plot with both
# unionist/independence votes pooled
base_plot &lt;-
  Catalan_elections %&gt;%
  group_by(Municipality, Option) %&gt;%
  summarize(first_year = first(Percent, Year),
            last_year = last(Percent, Year)) %&gt;%
  mutate(id = paste0(Municipality, &quot;_&quot;, Option)) %&gt;%
  gather(year, value, ends_with(&quot;year&quot;)) %&gt;%
  ggplot(aes(year, value)) +
  geom_point(alpha = 0.1, size = 2) +
  geom_line(aes(group = id), alpha = 0.1)

# Add the median summary line for both indy/unionist
median_plot &lt;-
  base_plot +
  geom_point(data = avg_pl, aes(year, value),
            colour = &quot;red&quot;, alpha = 0.5, size = 2) +
  geom_line(data = avg_pl, aes(year, value, group = id),
            colour = &quot;red&quot;, alpha = 0.5, size = 2)

# Add the change of Barcelona for both indy/unionist vote
bcn_plot &lt;-
  median_plot +
  geom_point(data = bcn_pl, aes(year, value),
             colour = &quot;blue&quot;, alpha = 0.5, size = 2) +
  geom_line(data = bcn_pl, aes(year, value, group = id),
            colour = &quot;blue&quot;, alpha = 0.5, size = 2)


# Separate the plot for indy/unionist in different
# panels and add pretty options
pretty_plot &lt;-
  bcn_plot +
  scale_x_discrete(name = NULL,
                   labels = c(&quot;1980&quot;,
                              &quot;2015&quot;)) +
  scale_y_continuous(name = &quot;% of votes in favour of:&quot;,
                     breaks = seq(0, 100, 20),
                     labels = percent(seq(0, 1, 0.2))) +
  facet_wrap(~ Option, strip.position = &quot;bottom&quot;) +
  labs(
    title = &quot;Independence/Unionist vote in Catalonia in three decades&quot;,
    subtitle = &quot;Red line is the median change for all municipalities - Blue line is Barcelona&quot;,
    caption = &quot;Data collected by @marcbeldata - Plot and analisis by @cimentadaj&quot;
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, family = &quot;Arial-BoldMT&quot;),
    plot.subtitle = element_text(size = 12, color = &quot;#666666&quot;),
    plot.caption = element_text(size = 12, color = &quot;#666666&quot;),
    strip.text.x = element_text(size = 14),
    axis.title.y = element_text(size = 16),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 14)
  )
# Final plot
pretty_plot</code></pre>
<p><img src="/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/2017-12-14-brief-analysis-of-independent-unionist-vote-in-catalonia_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>I was very surprised with this plot. On the left panel we can see the increase of independence votes in the last 35 years. The red line is the median change for all municipalities. There is a huge average increase from around 49% to over 70%. In fact, it’s not just an artifact of mean/median with big variance. If we look at the bulk of the distribution on the right line and then the left line, we see an upwards shift in the whole distribution.</p>
<p>On the other hand, the unionist vote seems to have decreased! The left/right distributions seem to be very similar (but it looks like the distribution of the right line has some outliers shifting upwards). But remember something: these are all municipalities. Municipalities might have 1000 citizen or even less! Consider the lonely town in a mountain with 50 people voting for independent parties: that’s also a municipality.</p>
<p>It is for this reason that we need to pay attention to places like Barcelona, which have over 1 million residents and definately weigh in more in proportion. And that’s where the interesting thing about this plot arises: the Barcelona change is practically the same. Not only have the votes increased very very similarly for both sides, but they’re also at the same level of support. Both blue lines look pretty much identical.</p>
<p>Don’t forget: small differences <strong>can</strong> make a difference, specially in elections. Perhaps they <strong>are</strong> different but we need to take a closer look.</p>
<p>Let’s plot the independence/unionist evolution only for Barcelona.</p>
<pre class="r"><code># Plot for indy/unionist vote over time only for Barcelona
Catalan_elections %&gt;%
  filter(Municipality == &quot;Barcelona&quot;) %&gt;%
  ggplot(aes(Year, Percent, group = Option, colour = Option)) +
  geom_line(alpha = 0.5, size = 2) +
  scale_x_continuous(name = NULL) +
  scale_colour_discrete(name = NULL) +
  scale_y_continuous(name = &quot;% of votes in favour of:&quot;,
                     lim = c(0, 100),
                     breaks = seq(0, 100, 20),
                     labels = percent(seq(0, 1, 0.2))) +
  labs(
    title = &quot;Overtime votes for independence/unionist in Barcelona&quot;,
    caption = &quot;Data collected by @marcbeldata - Plot and analisis by @cimentadaj&quot;
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, family = &quot;Arial-BoldMT&quot;),
    plot.subtitle = element_text(size = 12, color = &quot;#666666&quot;),
    plot.caption = element_text(size = 12, color = &quot;#666666&quot;),
    legend.position = &quot;top&quot;,
    legend.text = element_text(size = 14),
    strip.text.x = element_text(size = 14),
    axis.title.y = element_text(size = 16),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 14)
  )</code></pre>
<p><img src="/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/2017-12-14-brief-analysis-of-independent-unionist-vote-in-catalonia_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Despite most municipalities are for independence, Barcelona, by only a small margin, has a majority of people voting for unionist parties. Be aware that these votes are not ‘referendums’ carried out every year. These are votes towards independence/unionist parties, which is a different thing. Also note that these are not predictions/forecasts, so they don’t have uncertainty intervals or margins of errors. This is empirical data from voter turnout.</p>
<p>I also tried other big municipalities such as Sabadell and found that unionism trumps over independence much strongly. Yet in others like Lleida, Independence seems to be on top. For a look at specific municipalities, <a href="http://marcbeldata.github.io/ggjoy-Catalan-vote-2015/">check the post by Marc Belzunces</a></p>
</div>
<div id="a-note-on-next-weeks-elections" class="section level2">
<h2>A note on next week’s elections</h2>
<p>This data takes us as far as 2015. Catalonia has suffered dramatic changes since 2015 specially due to the independence movement. These data are most likely not a good representation of what’s gonna happen next week. Big municipalities have usually been majority unionists according to polls, but the differences are tiny and we’ve seen dramatic changes with independence parties proclaiming unilateral independence. There are good attempts at predicting catalan elections (<a href="https://politica.elpais.com/politica/2017/12/07/ratio/1512647178_322229.html">in Spanish</a>) so tune in next week to see what happens.</p>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>How long should I wait for my bike?</title>
      <link>/blog/2017-12-01-how-long-should-i-wait-for-my-bike/how-long-should-i-wait-for-my-bike/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-12-01-how-long-should-i-wait-for-my-bike/how-long-should-i-wait-for-my-bike/</guid>
      <description><![CDATA[
      


<p>I’ve just started a project which I’m very excited about. Everyday I take my bike to work and most days I have one of two problems. First, whenever I get to my station there are no bikes available; no problem, there’s an app that shows the closest stations with bikes available. The problem is that these stations might be far and sometimes I’m relucant to walk that much. I’d love for bicing to give me some time estimation until a new bike arrives.</p>
<p>Second, whenever you’re trying to return a bike the station might not have any parking spaces available. Similarly, it would be very cool if bicing (the public bicycle company) gave me an estimate of how much time I should wait until a new bike will be taken. I started thinking on how I could implement this and started looking for bicing data online. To my surprise, bicing actually releases their <strong>live</strong> data as a json! But for this type of estimation I need historical data. I want to know the pattern usage of the station and use that information for the prediction.</p>
<p>With that idea in mind, I got to work. I needed to set up my Virtual Private Server (VPS) to pull the data from the bicing API everyday. Because this is still a work in progress, I will only describe here how I set my VPS to scrape the bicing API everyday and how I set <code>cron</code> to send me an email after every scrape.</p>
<p>I have a VPS from <a href="https://www.digitalocean.com/">Digital Ocean</a> with an Ubuntu OS and 512 mb of RAM and 2 GB of hard disk. That’s enough for this task because the data should not be very big, even in the long run. In any case you can adjust for your VPS to have more memory/ram without losing information. Assuming you have <a href="https://www.digitalocean.com/community/tutorials/how-to-install-r-on-ubuntu-16-04-2">R installed in your Ubuntu VPS</a> with your favorite packages, then make sure your script works by running <code>Rscript path/to/your/script.R</code>. It might be better to type <code>which Rscript</code> in the terminal and paste the path to the executable, similar to <code>/usr/bin/Rscript path/to/your/script.R</code></p>
<p>My workflow is as follows: I first create an empty dataset saved as <code>.rds</code> and my script reads the data, scrapes the bicing data and then saves the data by appending both the empty and the scraped data. It finishes by saving the same <code>.rds</code> for a later scrape. I tested this very thoroughly to make sure the script wouldn’t fail and I always get the expected data.</p>
<p>All good so far, right? This took me no time. The hard problem came when setting the <code>cron</code> job, which is a way of scheduling tasks in OSx and Ubuntu. For an explanation of how <code>cron</code> works, check out how I set <a href="blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/index.html">my PISA twitter bot</a>.</p>
<p>First, make sure you have <code>cron</code> <a href="https://www.digitalocean.com/community/tutorials/how-to-use-cron-to-automate-tasks-on-a-vps">installed</a>. I followed <strong>a lot</strong> of tutorials and dispered information. What worked for me perhaps does not work for you, but here it is.</p>
<p>Type <code>crontab -e</code> and the cron interface should appear. The lines starting with <code>#</code> are coments, so scroll down until the end of the comments. First we have to set a few environmental variables that <code>cron</code> uses to execute your script. I followed <a href="http://krisjordan.com/essays/timesaving-crontab-tips">these tips</a>.</p>
<p>When I finished my crontab looked like this:</p>
<pre class="bash"><code>SHELL=/bin/bash
PATH=/home/cimentadaj/bin:/home/cimentadaj/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
HOME=/home/cimentadaj/bicycle
MAILTO=my_email # set email here!
15,50 16  * * * /usr/bin/Rscript scrape_bicing.R</code></pre>
<ul>
<li><p>SHELL is the path to the pre-determined program to run on the cron job. Be default I set it to bash (but it could be anything else you want).</p></li>
<li><p>PATH I’m not sure what’s for but I pasted the output of <code>echo $PATH</code>, as the tips suggested.</p></li>
<li><p>HOME is the root directory where the script will be executed, I set it to where the script is (or where your project is at).</p></li>
<li><p>MAILTO is the email where I will get the cron job alert when it finishes.</p></li>
<li><p><code>15,50 16  * * * /usr/bin/Rscript scrape_bicing.R</code> is the schedule, program and script to run. Here I set arbitrary times, so the the script is scheduled to run at <code>16:15</code> and <code>16:50</code> every day, every month and every year. I will run using <code>Rscript</code> and the name of the script to run.</p></li>
</ul>
<p><strong>WARNING:</strong> remember that the <code>cron</code> is set relative to the time of where your server is. Mine did not have the same timezone of where I lived, so I had to set the <code>cron</code> one hour before of my actual time. Use <code>date</code> to print the time of your VPS.</p>
<p>Even after this, the <code>cron</code> job was still not running. Nothing, no email, no log, no change in the data. I then figured out that Ubuntu systems have some <a href="https://serverfault.com/a/754104">pecularities</a> when it comes to <code>cron</code>. So I went to <code>./etc/</code> and renamed every <code>cron.</code> file for <code>cron-</code> with <code>rename 's/cron./cron-/g' *</code>, thanks to this <a href="https://stackoverflow.com/a/20657563/3617958">answer</a>.</p>
<p>Run again and it worked! Great. However, I didn’t receive an email stating that the <code>cron</code> job finished. I looked up many solutions and ended up installing <code>ssmtp</code> which is a library for sending emails from terminal. I won’t bore you with the details. Here are the steps I took:</p>
<ul>
<li>Install <code>ssmtp</code> with <code>sudo apt-get update</code> and <code>sudo apt-get install ssmtp</code>.</li>
<li>Edit <code>ssmtp.conf</code> with <code>sudo nano /etc/ssmtp/ssmtp.conf</code></li>
</ul>
<p>Here’s the config that worked for me using <code>gmail</code>:</p>
<pre class="bash"><code># Config file for sSMTP sendmail
#
# The person who gets all mail for userids &lt; 1000
# Make this empty to disable rewriting.
root=your_email@gmail.com

# The place where the mail goes. The actual machine name is required no 
# MX records are consulted. Commonly mailhosts are named mail.domain.com
mailhub=smtp.gmail.com:587

AuthUser=your_email@gmail.com
AuthPass=your_password
UseTLS=YES
UseSTARTTLS=yes
TLS_CA_FILE=/etc/ssl/certs/ca-certificates.crt

# Where will the mail seem to come from?
#rewriteDomain=gmail.com

# The full hostname
hostname=your_host_name

# Are users allowed to set their own From: address?
# YES - Allow the user to specify their own From: address
# NO - Use the system generated From: address
#FromLineOverride=YES</code></pre>
<p>Three caveats that took me a lot of time to figure out.</p>
<ul>
<li><p>First, <a href="https://www.digitalocean.com/community/tutorials/how-to-use-google-s-smtp-server">some docs</a> say you should use another port in <code>mailhub</code>, but <code>587</code> worked for me.</p></li>
<li><p><code>TLS_CA_FILE</code>: make sure that <a href="https://askubuntu.com/questions/342484/etc-pki-tls-certs-ca-bundle-crt-not-found">this file exists</a>! For Ubuntu/Debian the file is at <code>/etc/ssl/certs/ca-certificates.crt</code> while on other platforms it might be in <code>/etc/pki/tls/certs/ca-bundle.crt</code>. Note the different file names!</p></li>
<li><p><code>hostname</code> should be the result of typing <code>hostname</code> in your server.</p></li>
</ul>
<p>Lastly, I also added the line <code>root:your_EMAIL_@gmail.com:smtp.gmail.com:587</code> with <code>sudo nano /etc/ssmtp/revaliases</code>.</p>
<p>After an entire day figuring out all this information, the <code>cron</code> job worked! I now set my <code>cron</code> job and whenever it finished I receive an email directly showing the log of the script.</p>
<p>I wrote this primarily for me not to forget any of this, but it might be useful for other people.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>An introduction to the ess package</title>
      <link>/blog/2017-11-23-an-introduction-to-the-ess-package/an-introduction-to-the-ess-package/</link>
      <pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-11-23-an-introduction-to-the-ess-package/an-introduction-to-the-ess-package/</guid>
      <description><![CDATA[
      


<p>The <code>ess</code> package is designed to download the ESS data as easily as possible. It has a few helper functions to download rounds, rounds for a selected country and to show which rounds/countries are available. In this tutorial I will walk you through how these functions work.</p>
<p>Before using the package it is necessary for you to sign up at <a href="http://www.europeansocialsurvey.org/" class="uri">http://www.europeansocialsurvey.org/</a>.</p>
<p>Let’s do it together.</p>
<p>When you enter the website go the to topmost left corner and click on <code>Sign in/Register</code>. Under the email box, click on <code>New user?</code> and fill out your personal information. Click on <code>Register</code> and check your email inbox. You should’ve received an email from the ESS with an activation link. Click on that link and voila! We’re ready to go.</p>
<p>We can install and load the package with this code:</p>
<pre class="r"><code>install.packages(&quot;ess&quot;, dependencies = TRUE)
library(ess)</code></pre>
<div id="download-country-rounds" class="section level2">
<h2>Download country rounds</h2>
<p>First things first, do we know if Spain participated in the European Social Survey? <code>ess</code> has <code>show_countries()</code> that automatically searchers for all countries that participated. The nice thing is that these (an all other functions from the package) interactively check this information on the website, so any changes should be also visible immediately in R.</p>
<pre class="r"><code>show_countries()</code></pre>
<p>Spain is there! But which rounds did Spain participate? For that, the usual way would be to visit <a href="http://www.europeansocialsurvey.org/data/country_index.html" class="uri">http://www.europeansocialsurvey.org/data/country_index.html</a> and look for it. <code>ess</code> provides the function <code>show_country_rounds()</code> which returns all the available rounds from that website.</p>
<pre class="r"><code>show_country_rounds(&quot;Spain&quot;)</code></pre>
<p>Remember to type exactly the same name provided by <code>show_countries()</code> because these functions are case sensitive. How do we download this data?</p>
<pre class="r"><code>your_email &lt;- &quot;your email here&quot;

spain_seven &lt;- ess_country(
  country = &quot;Spain&quot;,
  rounds = 7,
  your_email = your_email
)</code></pre>
<p>That easy! Now you have <code>spain_seven</code> with the 7th round for Spain. If you wanted to download more rounds, you can specify them in the rounds section.</p>
<pre class="r"><code>spain_three &lt;- ess_country(
  country = &quot;Spain&quot;,
  rounds = c(1, 3, 5),
  your_email = your_email
)</code></pre>
<p>If you’re interested in downloading all available waves from the start, use <code>ess_all_cntrounds()</code>.</p>
<pre class="r"><code>ess_all_cntrounds(&quot;Spain&quot;, your_email)</code></pre>
</div>
<div id="download-complete-rounds" class="section level2">
<h2>Download complete rounds</h2>
<p>What about specific rounds for all countries? <code>ess</code> provides the same set of functions: <code>show_rounds()</code> for available rounds, <code>ess_rounds()</code> for specific rounds and <code>ess_all_rounds()</code>.</p>
<pre class="r"><code>show_rounds()</code></pre>
<p>Let’s grab the first three rounds, although this might take a bit more time than for country rounds!</p>
<pre class="r"><code>three_rounds &lt;-
  ess_rounds(
  c(1, 3),
  your_email
)

three_rounds[[1]]</code></pre>
<p>Finally, you can download all available rounds with:</p>
<pre class="r"><code>all_rounds &lt;- ess_all_rounds(your_email)</code></pre>
</div>
<div id="download-for-stata" class="section level2">
<h2>Download for Stata</h2>
<p>To download Stata files you can use:</p>
<pre class="r"><code>ess_country(
  &quot;Spain&quot;,
  1:2,
  your_email,
  only_download = TRUE,
  output_dir = &quot;./ess&quot;
)</code></pre>
<p>The <code>only_download</code> argument makes sure that it won’t return anything in R, and <code>output_dir</code> will be where the data is saved. If you supply a non existent directory it will create it on the fly.</p>
<p>rounds can be downloaded in the same way with:</p>
<pre class="r"><code>ess_rounds(
  1:2,
  your_email,
  only_download = TRUE,
  output_dir = &quot;./ess&quot;
)</code></pre>
<p>That easy! <code>ess</code> will continue to evolve in the future and there are some of the features already in the to-do list.</p>
<ul>
<li><p>Add a <code>*_themes()</code> family of function for topics; see <a href="http://www.europeansocialsurvey.org/data/module-index.html">here</a></p></li>
<li><p>Download data in SPSS and SAS format</p></li>
<li><p>Stata files (as well as SPSS and SAS) need to be pre-processed before reading into R (ex: run a do file before reading into R)</p></li>
</ul>
<p>The repository and development version of the package can be found at <a href="https://github.com/cimentadaj/ess" class="uri">https://github.com/cimentadaj/ess</a> and please report any bugs/issues/improvements <a href="https://github.com/cimentadaj/ess/issues">here</a>!</p>
<p>Thanks.</p>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>1..2..3..check!</title>
      <link>/blog/2017-11-18-123check/1-2-3-check/</link>
      <pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-11-18-123check/1-2-3-check/</guid>
      <description><![CDATA[
      


<p>1..2..3..check! This is my first post using <a href="https://cran.r-project.org/web/packages/blogdown/index.html">blogdown</a>. I migrated my website from <code>Jekyll</code> to <code>Hugo</code> and although it took me around 2 days to tweak everything to where I wanted it, the process wasn’t so bad after all. As a celebration, I though of doing a quick analysis!</p>
<p>I live in Barcelona, a city known for sunny weather, great football and for wanting to become an independent state. In fact, just recently there was an unsuccessful attempt to break parts with the Spanish nation. Without delving too much into it, I searched for any question related to nationalism into the European Social Survey and downloaded the last available wave using the <a href="https://cran.r-project.org/web/packages/ess/index.html">ess</a> package.</p>
<pre class="r"><code>library(essurvey)
library(cimentadaj)
library(tidyverse)

spain_df &lt;- import_country(&quot;Spain&quot;, 7, &quot;your_email@gmail.com&quot;)</code></pre>
<p><code>spain_df</code> is now a data frame containing the 7th ESS round. Next we have to recode the autonomous communities which are in a ESS format. We’re interested in two variables, <code>region</code> and <code>fclcntr</code>, the second one asking whether the person feels closer to Spain.</p>
<p><strong>Note: did you notice the comunidades <code>tibble</code> there? I pasted that with no effort with <a href="https://cran.r-project.org/web/packages/datapasta/index.html">datapasta</a>! If you’re using Rstudio, just copy the table from your source and use Shift + CMD + T (on a mac) to paste it as a very nice tibble.</strong></p>
<pre class="r"><code>comunidades &lt;- tibble::tribble(
  ~ round,                      ~ country,
  &quot;ES11&quot;,                      &quot;Galicia&quot;,
  &quot;ES12&quot;,      &quot;Asturias&quot;,
  &quot;ES13&quot;,                   &quot;Cantabria&quot;,
  &quot;ES21&quot;,                  &quot;País Vasco&quot;,
  &quot;ES22&quot;,  &quot;Navarra&quot;,
  &quot;ES23&quot;,                    &quot;La Rioja&quot;,
  &quot;ES24&quot;,                      &quot;Aragón&quot;,
  &quot;ES30&quot;,         &quot;Madrid&quot;,
  &quot;ES41&quot;,             &quot;Castilla y León&quot;,
  &quot;ES42&quot;,          &quot;Castilla-La Mancha&quot;,
  &quot;ES43&quot;,                 &quot;Extremadura&quot;,
  &quot;ES51&quot;,                    &quot;Cataluña&quot;,
  &quot;ES52&quot;,        &quot;Valenciana&quot;,
  &quot;ES53&quot;,               &quot;Illes Balears&quot;,
  &quot;ES61&quot;,                   &quot;Andalucía&quot;,
  &quot;ES62&quot;,            &quot;Región de Murcia&quot;,
  &quot;ES63&quot;,    &quot;Ceuta&quot;,
  &quot;ES64&quot;,  &quot;Melilla&quot;,
  &quot;ES70&quot;,                    &quot;Canarias&quot;
)

var_recode &lt;- reverse_name(attr(spain_df$fclcntr, &quot;labels&quot;))

ready_df &lt;-
  spain_df %&gt;%
  transmute(com_aut = deframe(comunidades)[region],
         close_cnt = factor(var_recode[fclcntr],
                            levels = var_recode[1:4],
                            ordered = TRUE))</code></pre>
<p>Next up we calculate the percentage of respondents within each category and within each region and visualize it.</p>
<pre class="r"><code>perc_table &lt;-
  ready_df %&gt;%
  count(com_aut, close_cnt) %&gt;%
  group_by(com_aut) %&gt;%
  mutate(perc = (n / n())) %&gt;%
  filter(!is.na(com_aut), !is.na(close_cnt))


perc_table %&gt;%
  ggplot(aes(close_cnt, perc)) +
  geom_col() +
  facet_wrap(~ com_aut) +
  labs(
    x = &quot;How close do you feel to Spain?&quot;,
    y = &quot;Percentage&quot;
  ) +
  ggtitle(label = &quot;Closeness to Spain by autonomous communities&quot;) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90),
    plot.title = element_text(size = 16, family = &quot;Arial-BoldMT&quot;),
    plot.subtitle = element_text(size = 14, color = &quot;#666666&quot;),
    plot.caption = element_text(size = 10, color = &quot;#666666&quot;)
  ) +
  coord_flip()</code></pre>
<p><img src="/blog/2017-11-18-123check/2017-11-18-1-2-3-check_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Catalonia does seem to be the region with the highest share of respondents saying that they don’t feel close to Spain, although the vast majority does say they feel very or just close to Spain. On the other hand, Andalucia does comply with stereotypes! They certainly feel very close to the Spanish identity.</p>
<p>My <code>blogdown</code> workflow is very easy:</p>
<ol style="list-style-type: decimal">
<li><p>Create a post with my function <code>cimentadaj::my_new_post</code> which is a wrapper around <code>blogdown::new_post</code></p></li>
<li><p>Run <code>blogdown::serve_site</code> to have a realtime visual of how my blog post is being rendered</p></li>
<li><p>Write blogpost</p></li>
<li><p>Run <code>blogdown::build_site</code>. This can take long if you posts that takea long time to compile</p></li>
<li><p>Push to github (although this is more complicated because I have two branches, one for developing content and the other for pushing to the website. Maybe I’ll write a post about this once)</p></li>
</ol>
<p>Bloggin with <code>blogdown</code> was so easy that I think I’m gonna start bloggin more now…</p>
]]>
      </description>
    </item>
    
    <item>
      <title>PhD thesis template with Sweave and knitr</title>
      <link>/blog/2017-10-24-phd-thesis-template-with-sweave-and-knitr/phd-thesis-template-with-sweave-and-knitr/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-10-24-phd-thesis-template-with-sweave-and-knitr/phd-thesis-template-with-sweave-and-knitr/</guid>
      <description><![CDATA[
      


<p>Writing my thesis with <code>Sweave</code> and <code>knitr</code> was very nice at the beginning, but then I began running into problems when I wanted to combine different chapters into one single document. Most of the problems were related to having each chapter be compilable on its own with separate bibliographies, among other things. I wrote a detailed guide on how I did it and you can read it <a href="https://cimentadaj.github.io/phd_thesis/thesis_template_example/2017-10-24-thesis-template.html">here</a>. I’d love some feedback as workflow is still very rudimentary. You can post a comment on this post or email me at <a href="mailto:cimentadaj@gmail.com">cimentadaj@gmail.com</a>.</p>
<p>Hope it’s useful.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Scraping and visualizing How I Met Your Mother</title>
      <link>/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</guid>
      <description><![CDATA[
      


<p>How I Met Your Mother (HIMYM from here after) is a television series very similar to the classical ‘Friends’ series from the 90’s. Following the release of the <a href="http://tidytextmining.com/">tidy text</a> book I was looking for a project in which I could apply these skills. I decided I would scrape all the transcripts from HIMYM and analyze patterns between characters. This post really took me to the limit in terms of web scraping and pattern matching, which was specifically what I wanted to improve in the first place. Let’s begin!</p>
<p>My first task was whether there was any consistency in the URL’s that stored the transcripts. If you ever watched HIMYM, we know there’s around nine seasons, each one with about 22 episodes. This makes about 200 episodes give or take. It would be a big pain to manually write down 200 complicated URL’s. Luckily, there is a way of finding the 200 links without writing them down manually.</p>
<p>First, we create the links for the 9 websites that contain all episodes (1 through season 9)</p>
<pre class="r"><code>library(rvest)
library(tidyverse)
library(stringr)
library(tidytext)

main_url &lt;- &quot;http://transcripts.foreverdreaming.org&quot;
all_pages &lt;- paste0(&quot;http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=&quot;, seq(0, 200, 25))
characters &lt;- c(&quot;ted&quot;, &quot;lily&quot;, &quot;marshall&quot;, &quot;barney&quot;, &quot;robin&quot;)</code></pre>
<p>Each of the URL’s of <code>all_pages</code> contains all episodes for that season (so around 22 URL’s). I also picked the characters we’re gonna concentrate for now. From here the job is very easy. We create a function that reads each link and parses the section containing all links for that season. We can do that using <a href="http://selectorgadget.com/.">SelectorGadget</a> to find the section we’re interested in. We then search for the <code>href</code> attribute to grab all links in that attribute and finally create a tibble with each episode together with it’s link.</p>
<pre class="r"><code>episode_getter &lt;- function(link) {
  title_reference &lt;-
    link %&gt;%
    read_html() %&gt;%
    html_nodes(&quot;.topictitle&quot;) # Get the html node name with &#39;selector gadget&#39;
  
  episode_links &lt;-
    title_reference %&gt;%
    html_attr(&quot;href&quot;) %&gt;%
    gsub(&quot;^.&quot;, &quot;&quot;, .) %&gt;%
    paste0(main_url, .) %&gt;%
    setNames(title_reference %&gt;% html_text()) %&gt;%
    enframe(name = &quot;episode_name&quot;, value = &quot;link&quot;)
  
  episode_links
}

all_episodes &lt;- map_df(all_pages, episode_getter) # loop over all seasons and get all episode links
all_episodes$id &lt;- 1:nrow(all_episodes)</code></pre>
<p>There we go! Now we have a very organized <code>tibble</code>.</p>
<pre class="r"><code>all_episodes
# # A tibble: 208 x 3
#    episode_name                   link                                  id
#    &lt;chr&gt;                          &lt;chr&gt;                              &lt;int&gt;
#  1 01x01 - Pilot                  http://transcripts.foreverdreamin~     1
#  2 01x02 - Purple Giraffe         http://transcripts.foreverdreamin~     2
#  3 01x03 - Sweet Taste of Liberty http://transcripts.foreverdreamin~     3
#  4 01x04 - Return of the Shirt    http://transcripts.foreverdreamin~     4
#  5 01x05 - Okay Awesome           http://transcripts.foreverdreamin~     5
#  6 01x06 - Slutty Pumpkin         http://transcripts.foreverdreamin~     6
#  7 01x07 - Matchmaker             http://transcripts.foreverdreamin~     7
#  8 01x08 - The Duel               http://transcripts.foreverdreamin~     8
#  9 01x09 - Belly Full of Turkey   http://transcripts.foreverdreamin~     9
# 10 01x10 - The Pineapple Incident http://transcripts.foreverdreamin~    10
# # ... with 198 more rows</code></pre>
<p>The remaining part is to actually scrape the text from each episode. We can work that out for a single episode and then turn that into a function and apply for all episodes.</p>
<pre class="r"><code>episode_fun &lt;- function(file) {
  
  file %&gt;%
    read_html() %&gt;%
    html_nodes(&quot;.postbody&quot;) %&gt;%
    html_text() %&gt;%
    str_split(&quot;\n|\t&quot;) %&gt;%
    .[[1]] %&gt;%
    data_frame(text = .) %&gt;%
    filter(str_detect(text, &quot;&quot;), # Lots of empty spaces
           !str_detect(text, &quot;^\\t&quot;), # Lots of lines with \t to delete
           !str_detect(text, &quot;^\\[.*\\]$&quot;), # Text that start with brackets
           !str_detect(text, &quot;^\\(.*\\)$&quot;), # Text that starts with parenthesis
           str_detect(text, &quot;^*.:&quot;), # I want only lines with start with dialogue (:)
           !str_detect(text, &quot;^ad&quot;)) # Remove lines that start with ad (for &#39;ads&#39;, the link of google ads)
}</code></pre>
<p>The above function reads each episode, turns the html text into a data frame and organizes it clearly for text analysis. For example:</p>
<pre class="r"><code>episode_fun(all_episodes$link[15])
# # A tibble: 195 x 1
#    text                                                                   
#    &lt;chr&gt;                                                                  
#  1 Ted from 2030: Kids, something you might not know about your Uncle Mar~
#  2 &quot;Ted: You don&#39;t have to shout out \&quot;poker\&quot; when you win.&quot;             
#  3 Marshall: I know. It&#39;s just fun to say.                                
#  4 &quot;Ted from 2030: We all finally agreed Marshall should be running our g~
#  5 &quot;Marshall: It&#39;s called \&quot;Marsh-gammon.\&quot; It combines all the best feat~
#  6 Robin: Backgammon, obviously.                                          
#  7 &quot;Marshall: No. Backgammon sucks. I took the only good part of backgamm~
#  8 Lily: I&#39;m so excited Victoria&#39;s coming.                                
#  9 Robin: I&#39;m going to go get another round.                              
# 10 Ted: Okay, I want to lay down some ground rules for tonight. Barney, I~
# # ... with 185 more rows</code></pre>
<p>We now have a data frame with only dialogue for each character. We need to apply that function to each episode and <code>bind</code> everything together. We first apply the function to every episode.</p>
<pre class="r"><code>all_episodes$text &lt;- map(all_episodes$link, episode_fun)</code></pre>
<p>The <code>text</code> list-column is an organized list with text for each episode. However, manual inspection of some episodes actually denotes a small error that limits our analysis greatly. Among the main interests of this document is to study relationships and presence between characters. For that, we need each line of text to be accompanied by the character who said it. Unfortunately, some of these scripts don’t have that.</p>
<p>For example, check any episode from season <a href="http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=175">8</a> and <a href="http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=200">9</a>. The writer didn’t write the dialogue and just rewrote the lines. There’s nothing we can do so far to improve that and we’ll be excluding these episodes. This pattern is also present in random episodes like in season 4 or season 6. We can exclude chapters based on the number of lines we parsed. On average, each of these episodes has about 200 lines of dialogue. Anything significantly lower, like 30 or 50 lines, is an episode which doesn’t have a lot of dialogue.</p>
<pre class="r"><code>all_episodes$count &lt;- map_dbl(all_episodes$text, nrow)</code></pre>
<p>We can extend the previous <code>tibble</code> to be a bit more organized by separating the episode-season column into separate season and episo numbers.</p>
<pre class="r"><code>all_episodes &lt;-
  all_episodes %&gt;%
  separate(episode_name, c(&quot;season&quot;, &quot;episode&quot;), &quot;-&quot;, extra = &quot;merge&quot;) %&gt;%
  separate(season, c(&quot;season&quot;, &quot;episode_number&quot;), sep = &quot;x&quot;)</code></pre>
<p>Great! We now have a very organized <code>tibble</code> with all the information we need. Next step is to actually break down the lines into words and start looking for general patterns. We can do that by looping through all episodes that have over 100 lines (just an arbitrary threshold) and unnesting each line for each <strong>valid</strong> character.</p>
<pre class="r"><code>lines_characters &lt;-
  map(filter(all_episodes, count &gt; 100) %&gt;% pull(text), ~ { 
    # only loop over episodes that have over 100 lines
    .x %&gt;%
      separate(text, c(&quot;character&quot;, &quot;text&quot;), sep = &quot;:&quot;, extra = &#39;merge&#39;) %&gt;%
      # separate character dialogue from actual dialogo
      unnest_tokens(character, character) %&gt;%
      filter(str_detect(character, paste0(paste0(&quot;^&quot;, characters, &quot;$&quot;), collapse = &quot;|&quot;))) %&gt;%
      # only count the lines of our chosen characters
      mutate(episode_lines_id = 1:nrow(.))
  }) %&gt;%
  setNames(filter(all_episodes, count &gt; 100) %&gt;% # name according to episode
             unite(season_episode, season, episode_number, sep = &quot;x&quot;) %&gt;%
             pull(season_episode)) %&gt;%
  enframe() %&gt;%
  unnest() %&gt;%
  mutate(all_lines_id = 1:nrow(.))</code></pre>
<p>Ok, our text is sort of ready. Let’s remove some bad words.</p>
<pre class="r"><code>words_per_character &lt;-
  lines_characters %&gt;%
  unnest_tokens(word, text) %&gt;% # expand all sentences into words
  anti_join(stop_words) %&gt;% # remove bad words
  filter(!word %in% characters) %&gt;% # only select characters we&#39;re interested
  arrange(name) %&gt;%
  separate(name, c(&quot;season&quot;, &quot;episode&quot;), sep = &quot;x&quot;, remove = FALSE) %&gt;%
  mutate(name = factor(name, ordered = TRUE),
         season = factor(season, ordered = TRUE),
         episode = factor(episode, ordered = TRUE)) %&gt;%
  filter(season != &quot;07&quot;)</code></pre>
<p>Just to make sure, let’s look at the <code>tibble</code>.</p>
<pre class="r"><code>words_per_character
# # A tibble: 88,174 x 7
#    name     season episode character episode_lines_id all_lines_id word   
#    &lt;ord&gt;    &lt;ord&gt;  &lt;ord&gt;   &lt;chr&gt;                &lt;int&gt;        &lt;int&gt; &lt;chr&gt;  
#  1 &quot;01x01 &quot; 01     &quot;01 &quot;   marshall                 1            1 ring   
#  2 &quot;01x01 &quot; 01     &quot;01 &quot;   marshall                 1            1 marry  
#  3 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 perfect
#  4 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 engaged
#  5 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 pop    
#  6 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 champa~
#  7 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 drink  
#  8 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 toast  
#  9 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 kitchen
# 10 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 floor  
# # ... with 88,164 more rows</code></pre>
<p>Perfect! One row per word, per character, per episode with the id of the line of the word.</p>
<p>Alright, let’s get our hands dirty. First, let visualize the presence of each character in terms of words over time.</p>
<pre class="r"><code># Filtering position of first episode of all seasons to
# position the X axis in the next plot.
first_episodes &lt;-
  all_episodes %&gt;%
  filter(count &gt; 100, episode_number == &quot;01 &quot;) %&gt;%
  pull(id)

words_per_character %&gt;%
  split(.$name) %&gt;%
  setNames(1:length(.)) %&gt;%
  enframe(name = &quot;episode_id&quot;) %&gt;%
  unnest() %&gt;%
  count(episode_id, character) %&gt;%
  group_by(episode_id) %&gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&gt;%
  ggplot(aes(as.numeric(episode_id), perc, group = character, colour = character)) +
  geom_line() +
  geom_smooth(method = &quot;lm&quot;) +
  scale_colour_discrete(guide = FALSE) +
  scale_x_continuous(name = &quot;Seasons&quot;,
                     breaks = first_episodes, labels = paste0(&quot;S&quot;, 1:7)) +
  scale_y_continuous(name = &quot;Percentage of words per episode&quot;) +
  theme_minimal() +
  facet_wrap(~ character, ncol = 3)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-13-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Ted is clearly the character with the highest number of words per episode followed by Barney. Lily and Robin, the only two women have very low presence compared to the men. In fact, if one looks closely, Lily seemed to have decreased slightly over time, having an all time low in season 4. Marshall, Lily’s partner in the show, does have much lower presence than both Barney and Ted but he has been catching up over time.</p>
<p>We also see an interesting pattern where Barney has a lot of peaks, suggesting that in some specific episodes he gains predominance, where Ted has an overall higher level of words per episode. And when Ted has peaks, it’s usually below its trend-line.</p>
<p>Looking at the distribution:</p>
<pre class="r"><code># devtools::install_github(&quot;clauswilke/ggjoy&quot;)
library(ggjoy)

words_per_character %&gt;%
  split(.$name) %&gt;%
  setNames(1:length(.)) %&gt;%
  enframe(name = &quot;episode_id&quot;) %&gt;%
  unnest() %&gt;%
  count(season, episode_id, character) %&gt;%
  group_by(episode_id) %&gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&gt;%
  ggplot(aes(x = perc, y = character, fill = character)) +
  geom_joy(scale = 0.85) +
  scale_fill_discrete(guide = F) +
  scale_y_discrete(name = NULL, expand=c(0.01, 0)) +
  scale_x_continuous(name = &quot;Percentage of words&quot;, expand=c(0.01, 0)) +
  ggtitle(&quot;Percentage of words per season&quot;) +
  facet_wrap(~ season, ncol = 7) +
  theme_minimal()</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>we see the differences much clearer. For example, we see Barney’s peaks through out every season with Season 6 seeing a clear peak of 40%. On the other hand, we see that their distributions don’t change that much over time! Suggesting that the presence of each character is very similar in all seasons. Don’t get me wrong, there are differences like Lily in Season 2 and then in Season 6, but in overall terms the previous plot suggests no increase over seasons, and this plot suggests that between seasons, there’s not a lot of change in their distributions that affects the overall mean.</p>
<p>If you’ve watched the TV series, you’ll remember Barney always repeating one similar trademark word: legendary! Although it is a bit cumbersome for us to count the number of occurrences of that sentence once we unnested each sentence, we can at least count the number of words per character and see whether some characters have particular words.</p>
<pre class="r"><code>count_words &lt;-
  words_per_character %&gt;%
  filter(!word %in% characters) %&gt;%
  count(character, word, sort = TRUE)

count_words %&gt;%
  group_by(character) %&gt;%
  top_n(20) %&gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &quot;free_y&quot;)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Here we see that a lot of the words we capture are actually nouns or expressions which are common to everyone, such as ‘yeah’, ‘hey’ or ‘time’. We can weight down commonly used words for other words which are important but don’t get repeated a lot. We can exclude those words using <code>bind_tf_idf()</code>, which for each character decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection or corpus of documents (see 3.3 in <a href="http://tidytextmining.com/tfidf.html" class="uri">http://tidytextmining.com/tfidf.html</a>).</p>
<pre class="r"><code>count_words %&gt;%
  bind_tf_idf(word, character, n) %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  group_by(character) %&gt;%
  top_n(20) %&gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &quot;free_y&quot;)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Now Barney has a very distinctive word usage, one particularly sexist with words such as couger, bang and tits. Also, we see the word legendary as the thirdly repeated word, something we were expecting! On the other hand, we see Ted with things like professor (him), aunt (because of aunt Lily and such).</p>
<p>Knowing that Ted is the main character in the series is no surprise. To finish off, we’re interested in knowing which characters are related to each other. First, let’s turn the data frame into a suitable format.</p>
<p>Here we turn all lines to lower case and check which characters are present in the text of each dialogue. The loop will return a vector of logicals whether there was a mention of any of the characters. For simplicity I exclude all lines where there is more than 1 mention of a character, that is, 2 or more characters.</p>
<pre class="r"><code>lines_characters &lt;-
  lines_characters %&gt;%
  mutate(text = str_to_lower(text))

rows_fil &lt;-
  map(characters, ~ str_detect(lines_characters$text, .x)) %&gt;%
  reduce(`+`) %&gt;%
  ifelse(. &gt;= 2, 0, .) # excluding sentences which have 2 or more mentions for now
  # ideally we would want to choose to count the number of mentions
  # per line or randomly choose another a person that was mentioned.</code></pre>
<p>Now that we have the rows that have a mention of another character, we subset only those rows. Then we want know which character was mentioned in which line. I loop through each line and test which character is present in that specific dialogue line. The loop returns the actual character name for each dialogue. Because we already filtered lines that <strong>have</strong> a character name mentioned, the loop should return a vector of the same length.</p>
<pre class="r"><code>character_relation &lt;-
  lines_characters %&gt;%
  filter(as.logical(rows_fil)) %&gt;%
  mutate(who_said_what =
           map_chr(.$text, ~ { # loop over all each line
             who_said_what &lt;- map_lgl(characters, function(.y) str_detect(.x, .y))
             # loop over each character and check whether he/she was mentioned
             # in that line
             characters[who_said_what]
             # subset the character that matched
           }))
</code></pre>
<p>Finally, we plot the relationship using the <code>ggraph</code> package.</p>
<pre class="r"><code>library(ggraph)
library(igraph)

character_relation %&gt;%
  count(character, who_said_what) %&gt;%
  graph_from_data_frame() %&gt;%
  ggraph(layout = &quot;linear&quot;, circular = TRUE) +
  geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                width = 2.5, show.legend = FALSE) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>A very clear pattern emerges. There is a strong relationship between Robin and Barney towards Ted. In fact, their direct relationship is very weak, but both are very well connected to Ted. On the other hand, Marshall and Lily are also reasonably connected to Ted but with a weaker link. Both of them are indeed very connected, as should be expected since they were a couple in the TV series.</p>
<p>We also see that the weakest members of the group are Robin and Barney with only strong bonds toward Ted but no strong relationship with the other from the group. Overall, there seems to be a division: Marshall and Lily hold a somewhat close relationship with each other and towards Ted and Barney and Robin tend to be related to Ted but no one else.</p>
<p>As a follow-up question, is this pattern of relationships the same across all seasons? We can do that very quickly by filtering each season using the previous plot.</p>
<pre class="r"><code>library(cowplot)

# Loop through each season
seasons &lt;- paste0(0, 1:7)

all_season_plots &lt;- lapply(seasons, function(season_num) {

  set.seed(2131)
  
  character_relation %&gt;%
    # Extract the season number from the `name` column
    mutate(season = str_replace_all(character_relation$name, &quot;x(.*)$&quot;, &quot;&quot;)) %&gt;%
    filter(season == season_num) %&gt;%
    count(character, who_said_what) %&gt;%
    graph_from_data_frame() %&gt;%
    ggraph(layout = &quot;linear&quot;, circular = TRUE) +
    geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                  width = 2.5, show.legend = FALSE) +
    geom_node_text(aes(label = name), repel = TRUE) +
    theme_void()
})

# Plot all graphs side-by-side
cowplot::plot_grid(plotlist = all_season_plots, labels = seasons)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-20-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>There are reasonable changes for all non-Ted relationship! For example, for season 2 the relationship Marshall-Lily-Ted becomes much stronger and it disappears in season 3. Let’s remember that these results might be affected by the fact that I excluded some episodes because of low number of dialogue lines. Keeping that in mind, we also see that for season 7 the Robin-Barney relationship became much stronger (is this the season the started dating?). All in all, the relationships don’t look dramatically different from the previous plot. Everyone seems to be strongly related to Ted. The main difference is the changes in relationship between the other members of the cast.</p>
<p>This dataset has a lot of potential and I’m sure I’ve scratched the surface of what one can do with this data. I encourage anyone interested in the topic to use the code to analyze the data further. One idea I might explore in the future is to build a model that attempts to predict who said what for all dialogue lines that didn’t have a character member. This can be done by extracting features from all sentences and using these patterns try to classify which. Any feedback is welcome, so feel free to message me at <a href="mailto:cimentadaj@gmail.com">cimentadaj@gmail.com</a></p>
]]>
      </description>
    </item>
    
    <item>
      <title>The LOO and the Bootstrap</title>
      <link>/blog/2017-09-07-the-loo-and-the-bootstrap/the-loo-and-the-bootstrap/</link>
      <pubDate>Thu, 07 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-09-07-the-loo-and-the-bootstrap/the-loo-and-the-bootstrap/</guid>
      <description><![CDATA[
      


<p>This is the second entry, and probably the last, on model validation methods. These posts are inspired by the work of Kohavi (1995), which I totally recommend reading. This post will talk talk about the Leave-One-Out Cross Validation (LOOCV), which is the extreme version of the K-Fold Cross Validation and the Bootstrap for model assessment.</p>
<p>Let’s dive in!</p>
<div id="the-leave-one-out-cv-method" class="section level2">
<h2>The Leave-One-Out CV method</h2>
<p>The LOOCV is actually a very intuitive idea if you know how the K-Fold CV works.</p>
<ul>
<li>LOOCV: Let’s imagine a data set with 30 rows. We separate the 1st row to be the test data and the remaining 29 rows to be the training data. We fit the model on the training data and then predict the one observation we left out. We record the model accuracy and then repeat but predicting the 2nd row from training the model on row 1 and 3:30. We repeat until every row has been predicted.</li>
</ul>
<p>This is surprisingly easy to implement in R.</p>
<pre class="r"><code>library(tidyverse)

set.seed(21341)
loo_result &lt;-
  map_lgl(1:nrow(mtcars), ~ {
  test &lt;- mtcars[.x, ] # Pick the .x row of the iteration to be the test
  train &lt;- mtcars[-.x, ] # Let the training be all the data EXCEPT that row
  
  train_model &lt;- glm(am ~ mpg + cyl + disp, family = binomial(), data = train) # Fit any model
  
  # Since the prediction is in probabilities, pass the probability
  # to generate either a 1 or 0 based on the probability
  prediction &lt;- predict(train_model, newdata = test, type = &quot;response&quot;) %&gt;% rbinom(1, 1, .)
  
  test$am == prediction # compare whether the prediction matches the actual value
})

summary(loo_result %&gt;% as.numeric) # percentage of accurate results
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#  0.0000  0.0000  1.0000  0.5938  1.0000  1.0000</code></pre>
<p>It looks like our model had nearly 60% accuracy, not very good. But not entirely bad given our very low sample size.</p>
<p>Advantages:</p>
<ul>
<li><p>Just as with the K-Fold CV, this approach is useful because it uses all the data. At some point, every rows gets to be the test set and training set, maximizing information.</p></li>
<li><p>In fact, it uses almost ALL the data as the original data set as the training set is just N - 1 (this method uses even more than the K-Fold CV).</p></li>
</ul>
<p>Disadvantage:</p>
<ul>
<li><p>This approach is very heavy on your computer. We need to refit de model N times (although there is a shortcut for linear regreesion, see <a href="https://gerardnico.com/wiki/lang/r/cross_validation">here</a>).</p></li>
<li><p>Given that the test set is of only 1 observation, there might be a lot of variance in the prediction, making the accuracy test more unreliable (that is, relative to K-Fold CV)</p></li>
</ul>
</div>
<div id="the-bootstrap-method" class="section level2">
<h2>The Bootstrap method</h2>
<p>The bootstrap method is a bit different. Maybe you’ve heard about the bootstrap for estimating standard errors, and in fact for model assessment it’s very similar.</p>
<ul>
<li>Boostrap method: Take the data from before with 30 rows. Suppose we resample this dataset with replacement. That is, the dataset will have the same 30 rows, but row 1 might be repeated 3 times, row 2 might be repeated 4 times, row 3 might not be in the dataset anymore, and so on. Now, take this resampled data and use it to train the model. Now test your predictions on the actual data (the one with 30 unique rows) and calculate the model accuracy. Repeat N times.</li>
</ul>
<p>Again, the R implementation is very straightforward.</p>
<pre class="r"><code>
set.seed(21314)
bootstrap &lt;-
  map_dbl(1:500, ~ {
  train &lt;- mtcars[sample(nrow(mtcars), replace = T), ] # randomly sample rows with replacement
  test &lt;- mtcars
  
  train_model &lt;- glm(am ~ mpg + cyl + disp, family = binomial(), data = train) # fit any model
  
  # Get predicted probabilities and assign a 1 or 0 based on the probability
  prediction &lt;- predict(train_model, newdata = test, type = &quot;response&quot;) %&gt;% rbinom(nrow(mtcars), 1, .)
  accuracy &lt;- test$am == prediction # compare whether the prediction matches the actual value
  
  mean(accuracy) # get the proportion of correct predictions
})

summary(bootstrap)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#  0.4375  0.6875  0.7500  0.7468  0.8125  0.9375</code></pre>
<p>We got a better accuracy with the bootstrap (probably biased, see below) and a range of possible values going from 0.43 to 0.93. Note that if you run these models you’ll get a bunch of warnings like <code>glm.fit: fitted probabilities numerically 0 or 1 occurred</code> because we just have too few observations to be including covariates, resulting in a lot of overfitting.</p>
<p>Advantages:</p>
<ul>
<li>Variance is small considering both train and test have the same number of rows.</li>
</ul>
<p>Disadvantages</p>
<ul>
<li>It gives more biased results than the CV methods because it repeats data, rather than keep unique observations for training and testing.</li>
</ul>
<p>In the end, it’s a trade-off against what you’re looking for. In some instances, it’s alright to have a slightly biased estimate (either pessimistic or optimistic) as long as its reliable (bootstrap). On other instances, it’s better to have a very exact prediction but that is less unreliable (CV methods).</p>
<p>Some rule of thumbs:</p>
<ul>
<li><p>For large sample sizes, the variance issues become less important and the computational part is more of an issues. I still would stick by repeated CV for small and large sample sizes. See <a href="https://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio">here</a></p></li>
<li><p>Cross validation is a good tool when deciding on the model – it helps you avoid fooling yourself into thinking that you have a good model when in fact you are overfitting. When your model is fixed, then using the bootstrap makes more sense to assess accuracy (to me at least). See again <a href="https://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio">here</a></p></li>
</ul>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Again, this is a very crude approach, and the whole idea is to understand the inner workings of these algorithms in practice. For more thorough approaches I suggest using the <code>cv</code> functions from the <code>boot</code> package or <code>caret</code> or <code>modelr</code>. I hope this was useful. I will try to keep doing these things as they help me understand these techniques better.</p>
<ul>
<li>Kohavi, Ron. “A study of cross-validation and bootstrap for accuracy estimation and model selection.” Ijcai. Vol. 14. No. 2. 1995.</li>
</ul>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>Holdout and cross-validation</title>
      <link>/blog/2017-09-06-holdout-and-crossvalidation/holdout-and-crossvalidation/</link>
      <pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-09-06-holdout-and-crossvalidation/holdout-and-crossvalidation/</guid>
      <description><![CDATA[
      


<p>In a recent attempt to bring a bit of discipline into my life, I’ve been forcing myself to read papers after lunch, specifically concentrated on data science topics. The whole idea is to educated myself every day, but if I find something cool that I can implement in R, I’ll do it right away.</p>
<p>This blogpost is the first of a series of entries I plan to post explaining the main concepts of Kohavi (1995), which compares cross-validation methods and bootstrap methods for model selection. This first post will implement a K-Fold cross validation from scratch in order to understand more deeply what’s going on behind the scenes.</p>
<p>Before we explain the concept of K-Fold cross validation, we need to define what the ‘Holdout’ method is.</p>
<div id="holdout-method" class="section level2">
<h2>Holdout method</h2>
<ul>
<li>Holdout method: Imagine we have a dataset with house prices as the dependent variable and two independent variables showing the square footage of the house and the number of rooms. Now, imagine this dataset has <code>30</code> rows. The whole idea is that you build a model that can predict house prices accurately. To ‘train’ your model, or see how well it performs, we randomly subset 20 of those rows and fit the model. The second step is to predict the values of those 10 rows that we excluded and measure how well our predictions were. As a rule of thumb, experts suggest to randomly sample 80% of the data into the training set and 20% into the test set.</li>
</ul>
<p>A very quick example:</p>
<pre class="r"><code>library(tidyverse)
library(modelr)

holdout &lt;- function(repeat_times) { # empty argument for later
  n &lt;- nrow(mtcars)
  eighty_percent &lt;- (n * 0.8) %&gt;% floor
  train_sample &lt;- sample(1:n, eighty_percent) # randomly pick 80% of the rows
  test_sample &lt;- setdiff(1:n, train_sample) # get the remaining 20% of the rows
  
  train &lt;- mtcars[train_sample, ] # subset the 80% of the rows
  test &lt;- mtcars[test_sample, ] # subset 20% of the rows
  
  train_model &lt;- lm(mpg ~ ., data = train)
  
  test %&gt;%
    add_predictions(train_model) %&gt;% # add the predicted mpg values to the test data
    summarize(average_error = (pred - mpg) %&gt;% mean %&gt;% round(2)) %&gt;%
    pull(average_error)
  # calculate the average difference of the predicition from the actual value
}

set.seed(2131)
holdout()
# [1] 3.59</code></pre>
<p>We can see that on average the training set over predicts the actual values by about 3.6 points. An even more complex approach is what Kohavi (1995) calls “random subsampling”.</p>
</div>
<div id="random-subsampling" class="section level2">
<h2>Random subsampling</h2>
<p>In a nutshell, repeat the previous <code>N</code> times and calculate the average and standard deviation of your metric of interest.</p>
<pre class="r"><code>set.seed(2134)

random_subsampling &lt;- map_dbl(1:500, holdout)
summary(random_subsampling)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# -7.3100 -0.7525  0.4000  0.4255  1.5550  9.1500</code></pre>
<p>We get a mean error of 0.42, a maximum of 9.15 and a minimum of -7.31. Quite some variation, eh? It is precisely for this reason that Kohavi (1995) highlights that random subsampling has an important problem.</p>
<ul>
<li><p>Each time we resample, some observations might’ve been in the previous resample, leading to non-independence and making the training dataset unrepresentative of the original dataset.</p></li>
<li><p>What happens when you try to predict Y from an unrepresented X, 500 times? What we just saw before.</p></li>
</ul>
</div>
<div id="k-fold-cross-validation" class="section level2">
<h2>K-Fold cross validation</h2>
<p>Let’s move on to cross validation. K-Fold cross validation is a bit trickier, but here is a simple explanation.</p>
<ul>
<li>K-Fold cross validation: Take the house prices dataset from the previous example, divide the dataset into 10 parts of equal size, so if the data is 30 rows long, you’ll have 10 datasets of 3 rows each. Each split contains unique rows not present in other splits. In the first iteration, take the first dataset as the test dataset and merge the remaining 9 datasets as the train dataset. Fit the model on the training data, predict on the test data and record model accuracy. Repeat a new iteration where dataset 2 is the test set and data set 1 and 3:10 merged is the training set. Repeat for all K slices.</li>
</ul>
<p>We can implement this in R.</p>
<pre class="r"><code>k_slicer &lt;- function(data, slices) {
  stopifnot(nrow(data) &gt; slices) # the number of rows must be bigger than the K slices
  slice_size &lt;- (nrow(data) / slices) %&gt;% floor
  
  rows &lt;- 1:nrow(data)
  data_list &lt;- rep(list(list()), slices) # create empty list of N slices

  # Randomly sample slice_size from the rows available, but exclude these rows
  # from the next sample of rows. This makes sure each slice has unique rows.
  for (k in 1:slices) {
    specific_rows &lt;- sample(rows, slice_size) # sample unique rows for K slice
    rows &lt;- setdiff(rows, specific_rows) # exclue those rows
    data_list[[k]] &lt;- data[specific_rows, ] # sample the K slice and save in empty list
  }
  
  data_list
}

mtcars_sliced &lt;- k_slicer(mtcars, slices = 5) # data sliced in K slices</code></pre>
<p>All good so far? We took a dataset and split it into K mutually exclusive datasets. The next step is to run the modeling on <code>K = 2:10</code> and test on <code>K = 1</code>, and then repeat on <code>K = c(1, 3:10)</code> as training and test on <code>K = 2</code>, and repeat for al <code>K’s</code>. Below we implement it in R.</p>
<pre class="r"><code>
k_fold_cv &lt;-
  map_dbl(seq_along(mtcars_sliced), ~ {
  test &lt;- mtcars_sliced[[.x]] # Take the K fold
  
  # Note the -.x, for excluding that K
  train &lt;- mtcars_sliced[-.x] %&gt;% reduce(bind_rows) # bind row all remaining K&#39;s
  
  lm(mpg ~ ., data = train) %&gt;%
    rmse(test) # calculate the root mean square error of predicting the test set
})

k_fold_cv %&gt;%
  summary
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#   3.192   3.746   3.993   3.957   4.279   4.574</code></pre>
<p>And we get a summary of the root mean square error, a metric we decided to use now, instead of predictions. We can asses how accurate our model is this way and compare several specification of models and choose the one which better fits the data.</p>
<p>The main advantage of this approach:</p>
<ul>
<li>We maximize the use of data because all data is used, at some point, as test and training.</li>
</ul>
<p>This is very interesting in contrast to the holdout method in which we can’t maximize our data! Take data out of the test set and the predictions will have wider uncertainty intervals, take data out of the train set and get biased predictions.</p>
<p>This approach, as any other, has disadvantages.</p>
<ul>
<li><p>It is computationally intensive, given that we have to run the model K-1 times. In this setting it’s trivial, but in more complex modeling this can be quite costly.</p></li>
<li><p>If in any of the K iterations the predictions are bad, the overall accuracy will be bad, considering that other K iterations will also likely be bad. In other words, predictions need to be stable across all K iterations.</p></li>
<li><p>Building on the previous point, once the model is stable, increasing the number of folds (5, 10, 20, 25…) generates little change considering that the accuracy will be similar (and the variance of different K-folds will be similar as well).</p></li>
<li><p>Finally, if Y consists of categories, and one of these categories is very minimal, the best K-Fold CV can do is predict the class with more observations. If an observation of this minimal class gets to be in the test set in one of the iterations, then the training model will have very little accuracy for that category. See Kohavi (1995) page 3, example 1 for a detailed example.</p></li>
</ul>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>This was my first attempt at manually implementing the Holdout method and the K-Fold CV. These examples are certainly flawed, like rounding the decimal number of rows correct for the unique number of rows in each K-Fold slice. If anyone is interested in correcting thes, please do send a pull request. For those interested in using more reliable approaches, take a look at the <code>caret</code> and the <code>modelr</code> package. In the next entry I will implement the LOO method and the bootstrap (and maybe the stratified K-Fold CV)</p>
<ul>
<li>Kohavi, Ron. “A study of cross-validation and bootstrap for accuracy estimation and model selection.” Ijcai. Vol. 14. No. 2. 1995.</li>
</ul>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>perccalc package</title>
      <link>/blog/2017-08-01-perccalc-package/perccalc-package/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-08-01-perccalc-package/perccalc-package/</guid>
      <description><![CDATA[
      


<p>Reardon (2011) introduced a very interesting concept in which he calculates percentile differences from ordered categorical variables. He explains his procedure very much in detail in the appendix of the book chapter but no formal implementation has been yet available on the web. With this package I introduce a function that applies the procedure, following a step-by-step Stata script that Sean Reardon kindly sent me.</p>
<p>In this vignette I show you how to use the function and match the results to the Stata code provided by Reardon himself.</p>
<p>For this example, we’ll use a real world data set, one I’m very much familiar with: PISA. We’ll use the PISA 2012 wave for Germany because it asked parents about their income category. For this example we’ll need the packages below.</p>
<pre class="r"><code># install.packages(c(&quot;devtools&quot;, &quot;matrixStats&quot;, &quot;tidyverse&quot;))
# devtools::install_github(&quot;pbiecek/PISA2012lite&quot;)

library(matrixStats)
library(tidyverse)
library(haven)
library(PISA2012lite)</code></pre>
<p>If you haven’t installed any of the packages above, uncomment the first two lines to install them. Beware that the <code>PISA2012lite</code> package contains the PISA 2012 data and takes a while to download.</p>
<p>Let’s prepare the data. Below we filter only German students, select only the math test results and calculate the median of all math plausible values to get one single math score. Finally, we match each student with their corresponding income data from their parents data and their sample weights.</p>
<pre class="r"><code>ger_student &lt;- student2012 %&gt;%
  filter(CNT == &quot;Germany&quot;) %&gt;%
  select(CNT, STIDSTD, matches(&quot;^PV*.MATH$&quot;)) %&gt;%
  transmute(CNT, STIDSTD,
            avg_score = rowMeans(student2012[student2012$CNT == &quot;Germany&quot;, paste0(&quot;PV&quot;, 1:5, &quot;MATH&quot;)]))

ger_parent &lt;-
  parent2012 %&gt;%
  filter(CNT == &quot;Germany&quot;) %&gt;%
  select(CNT, STIDSTD, PA07Q01)

ger_weights &lt;-
  student2012weights %&gt;%
  filter(CNT == &quot;Germany&quot;) %&gt;%
  select(CNT, STIDSTD, W_FSTUWT)

dataset_ready &lt;-
  ger_student %&gt;%
  left_join(ger_parent, by = c(&quot;CNT&quot;, &quot;STIDSTD&quot;)) %&gt;%
  left_join(ger_weights, by = c(&quot;CNT&quot;, &quot;STIDSTD&quot;)) %&gt;%
  as_tibble() %&gt;%
  rename(income = PA07Q01,
         score = avg_score,
         wt = W_FSTUWT) %&gt;%
  select(-CNT, -STIDSTD)</code></pre>
<p>The final results is this dataset:</p>
<pre><code>## # A tibble: 10 x 3
##   score income            wt
##   &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;
## 1  440. Less than &lt;$A&gt;  137.
## 2  523. Less than &lt;$A&gt;  170.
## 3  291. Less than &lt;$A&gt;  162.
## 4  437. Less than &lt;$A&gt;  162.
## 5  367. Less than &lt;$A&gt;  115.
## # ... with 5 more rows</code></pre>
<p>This is the minimum dataset that the function will accept. This means that it needs to have at least a categorical variable and a continuous variable (the vector of weights is optional).</p>
<p>The package is called <code>perccalc</code>, short for percentile calculator and we can install and load it with this code:</p>
<pre class="r"><code>install.packages(&quot;perccalc&quot;, repo = &quot;https://cran.rediris.es/&quot;)
## package &#39;perccalc&#39; successfully unpacked and MD5 sums checked
## 
## The downloaded binary packages are in
##  C:\Users\cimentadaj\AppData\Local\Temp\RtmpYFnDzN\downloaded_packages
library(perccalc)</code></pre>
<p>The package has two functions, which I’ll show some examples. The first one is called <code>perc_diff</code> and it’s very easy to use, we just specify the data, the name of the categorical and continuous variable and the percentile difference we want.</p>
<p>Let’s put it to use!</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, percentiles = c(90, 10))
## Error: is_ordered_fct is not TRUE</code></pre>
<p>I generated that error on purpose to raise a very important requirement of the function. The categorical variable needs to be an ordered factor (categorical). It is very important because otherwise we could be calculating percentile differences of categorical variables such as married, single and widowed, which doesn’t make a lot of sense.</p>
<p>We can turn it into an ordered factor with the code below.</p>
<pre class="r"><code>dataset_ready &lt;-
  dataset_ready %&gt;%
  mutate(income = factor(income, ordered = TRUE))</code></pre>
<p>Now it’ll work.</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, percentiles = c(90, 10))
## difference         se 
##   97.00706    8.74790</code></pre>
<p>We can play around with other percentiles</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, percentiles = c(50, 10))
## difference         se 
##  58.776200   8.291083</code></pre>
<p>And we can add a vector of weights</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902</code></pre>
<p>Now, how are we sure that these estimates are as accurate as the Reardon (2011) implementation? We can compare the Stata ouput using this data set.</p>
<pre class="r"><code># Saving the dataset to a path
dataset_ready %&gt;%
  write_dta(path = &quot;/Users/cimentadaj/Downloads/pisa_income.dta&quot;, version = 13)</code></pre>
<p>Running the code below using the <code>pisa_income.dta</code>..</p>
<pre class="r"><code>*--------
use &quot;/Users/cimentadaj/Downloads/pisa_income.dta&quot;, clear

tab income, gen(inc)
*--------

/*-----------------------
    Making a data set that has 
    one observation per income category
    and has mean and se(mean) in each category
    and percent of population in the category
------------------------*/

tempname memhold
tempfile results
postfile `memhold&#39; income mean se_mean per using `results&#39;

forv i = 1/6 {
    qui sum inc`i&#39; [aw=wt]
    loc per`i&#39; = r(mean)    
                                
    qui sum score if inc`i&#39;==1 
                            
    if `r(N)&#39;&gt;0 {
        qui regress score if inc`i&#39;==1 [aw=wt]
        post `memhold&#39; (`i&#39;) (_b[_cons]) (_se[_cons]) (`per`i&#39;&#39;)
                            
    }               
}
postclose `memhold&#39; 

/*-----------------------
    Making income categories
    into percentiles
------------------------*/


    use `results&#39;, clear

    sort income
    gen cathi = sum(per)
    gen catlo = cathi[_n-1]
    replace catlo = 0 if income==1
    gen catmid = (catlo+cathi)/2
    
    /*-----------------------
        Calculate income 
        achievement gaps
    ------------------------*/

    sort income
    
    g x1 = catmid
    g x2 = catmid^2 + ((cathi-catlo)^2)/12
    g x3 = catmid^3 + ((cathi-catlo)^2)/4

    g cimnhi = mean + 1.96*se_mean
    g cimnlo = mean - 1.96*se_mean

    reg mean x1 x2 x3 [aw=1/se_mean^2] 

    twoway (rcap cimnhi cimnlo catmid) (scatter mean catmid) ///
        (function y = _b[_cons] + _b[x1]*x + _b[x2]*x^2 + _b[x3]*x^3, ran(0 1)) 
    
    loc hi_p = 90
    loc lo_p = 10

    loc d1 = [`hi_p&#39; - `lo_p&#39;]/100
    loc d2 = [(`hi_p&#39;)^2 - (`lo_p&#39;)^2]/(100^2)
    loc d3 = [(`hi_p&#39;)^3 - (`lo_p&#39;)^3]/(100^3)

    lincom `d1&#39;*x1 + `d2&#39;*x2 + `d3&#39;*x3
    loc diff`hi_p&#39;`lo_p&#39; = r(estimate)
    loc se`hi_p&#39;`lo_p&#39; = r(se)
    
    di &quot;`hi_p&#39;-`lo_p&#39; gap:     `diff`hi_p&#39;`lo_p&#39;&#39;&quot;
    di &quot;se(`hi_p&#39;-`lo_p&#39; gap): `se`hi_p&#39;`lo_p&#39;&#39;&quot;</code></pre>
<p>I get that the 90/10 difference is <code>95.22</code> with a standard error of <code>8.45</code>. Does it sound familiar?</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902</code></pre>
<p>The second function of the package is called <code>perc_dist</code> and instead of calculating the difference of two percentiles, it returns the score and standard error of every percentile. The arguments of the function are exactly the same but without the <code>percentiles</code> argument, because this will return the whole set of percentiles.</p>
<pre class="r"><code>perc_dist(dataset_ready, income, score)
## # A tibble: 100 x 3
##   percentile estimate std.error
##        &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1          1     3.69      1.33
## 2          2     7.28      2.59
## 3          3    10.8       3.79
## 4          4    14.1       4.93
## 5          5    17.4       6.01
## # ... with 95 more rows</code></pre>
<p>We can also add the optional set of weights and graph it:</p>
<pre class="r"><code>perc_dist(dataset_ready, income, score, wt) %&gt;%
  mutate(ci_low = estimate - 1.96 * std.error,
         ci_hi = estimate + 1.96 * std.error) %&gt;%
  ggplot(aes(percentile, estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_hi))</code></pre>
<p><img src="/blog/2017-08-01-perccalc-package/2017-08-01-perccalc-package_files/figure-html/unnamed-chunk-15-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Please note that for calculating the difference between two percentiles it is more accurate to use the <code>perc_diff</code> function. The <code>perc_diff</code> calculates the difference through a linear combination of coefficients resulting in a different standard error.</p>
<p>For example:</p>
<pre class="r"><code>perc_dist(dataset_ready, income, score, wt) %&gt;%
  filter(percentile %in% c(90, 10)) %&gt;%
  summarize(diff = diff(estimate),
            se_diff = diff(std.error))
## # A tibble: 1 x 2
##    diff se_diff
##   &lt;dbl&gt;   &lt;dbl&gt;
## 1  95.2    5.68</code></pre>
<p>compared to</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902</code></pre>
<p>They both have the same point estimate but a different standard error.</p>
<p>I hope this was a convincing example, I know this will be useful for me. All the intelectual ideas come from Sean Reardon and the Stata code was written by Sean Reardon, Ximena Portilla, and Jenna Finch. The R implemention is my own work.</p>
<p>You can find the package repository <a href="https://github.com/cimentadaj/perccalc">here</a>.</p>
<ul>
<li>Reardon, Sean F. “The widening academic achievement gap between the rich and the poor: New evidence and possible explanations.” Whither opportunity (2011): 91-116.</li>
</ul>
]]>
      </description>
    </item>
    
    <item>
      <title>Replicating Dupriez and Dumay (2006)</title>
      <link>/blog/2017-04-13-replicating-dupriez-and-dumay-2006/replicating-dupriez-and-dumay-2006/</link>
      <pubDate>Thu, 13 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-04-13-replicating-dupriez-and-dumay-2006/replicating-dupriez-and-dumay-2006/</guid>
      <description><![CDATA[
      


<p>So, I was bored for two days and decided I’d replicate a paper I had read. The paper is called ‘Inequalities in school systems: effect of school structure or of society structure?’ written by Vincent Dupriez and Xavier Dumay which you can read from <a href="http://www.tandfonline.com/doi/full/10.1080/03050060600628074?scroll=top&amp;needAccess=true">here</a>. I made available two version, one in <a href="https://github.com/cimentadaj/Inequality_schools_replication/raw/master/replication_tracking_pisa.pdf">pdf</a> and another in <a href="https://cimentadaj.github.io/Inequality_schools_replication/replication_tracking_pisa.html">html</a>.</p>
<p>If you’re interested in looking at the code or suggesting some changes, do send me an email at <code>cimentadaj@gmail.com</code> or check the <a href="https://github.com/cimentadaj/Inequality_schools_replication">Github repository</a>.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>My PISA twitter bot</title>
      <link>/blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/</guid>
      <description><![CDATA[
      


<p>I’ve long wanted to prepare a project with R related to education. I knew I’d found the idea when I read Thomas Lumley’s <a href="http://notstatschat.tumblr.com/post/156007757906/a-bus-watching-bot">attempt to create a Twitter bot in which he tweeted bus arrivals in New Zealand</a>. Quoting him, “Is it really hard to write a bot? No. Even I can do it. And I’m old.”</p>
<p>So I said to myself, alright, you have to create a Twitter bot but it has to be related to education. It’s an easy project which shouldn’t take a lot of your time. I then came up with this idea: what if you could randomly sample questions from the <a href="http://www.oecd.org/pisa/aboutpisa/">PISA databases</a> and create a sort of random facts generator. The result would be one graph a day, showing a question for some random sample of countries. I figured, why not prepare a post (both for me to remember how I did it but also so others can contribute to the project) where I explained step-by-step how I did it?</p>
<p>The repository for the project is <a href="https://github.com/cimentadaj/PISAfacts_twitterBot">here</a>, so feel free to drop any comments or improvements. The idea is to load the <a href="http://vs-web-fs-1.oecd.org/pisa/PUF_SPSS_COMBINED_CMB_STU_QQQ.zip">PISA 2015 data</a>, randomly pick a question that doesn’t have a lot of labels (because then it’s very difficult to plot it nicely), and based on the type of question create an appropriate graph. Of course, all of this needs to be done on the fly, without human assistance. You can follow this twitter account at <span class="citation">@DailyPISA_Facts</span>. Let’s start!</p>
<div id="data-wrangling" class="section level2">
<h2>Data wrangling</h2>
<p>First we load some of the packages we’ll use and read the PISA 2015 student data.</p>
<pre class="r"><code>library(tidyverse)
library(forcats)
library(haven)
library(intsvy) # For correct estimation of PISA estimates
library(countrycode) # For countrycodes
library(cimentadaj) # devtools::install_github(&quot;cimentadaj/cimentadaj&quot;)
library(lazyeval)
library(ggthemes) # devtools::install_github(&quot;jrnold/ggthemes&quot;)</code></pre>
<pre class="r"><code>file_name &lt;- file.path(tempdir(), &quot;pisa.zip&quot;)

download.file(
  &quot;http://vs-web-fs-1.oecd.org/pisa/PUF_SPSS_COMBINED_CMB_STU_QQQ.zip&quot;,
  destfile = file_name
)

unzip(file_name, exdir = tempdir())

pisa_2015 &lt;- read_spss(file.path(tempdir(), &quot;CY6_MS_CMB_STU_QQQ.sav&quot;))</code></pre>
<p>Downloading the data takes a bit but make sure to download the zip file and unzip it as I’ve just outlined.</p>
<p>The idea is to generate a script that can be used with all PISA datasets, so at some point we should be able not only to randomly pick question but also randomly pick PISA surveys (PISA has been implemented since the year 2000 in three year intervals). We create some places holders for the variable country name, the format of the country names and the missing labels we want to ignore for each question (I think these labels should be the same across all surveys).</p>
<pre class="r"><code>country_var &lt;- &quot;cnt&quot;
country_types &lt;- &quot;iso3c&quot;

missing_labels &lt;- c(&quot;Valid Skip&quot;,
                    &quot;Not Reached&quot;,
                    &quot;Not Applicable&quot;,
                    &quot;Invalid&quot;,
                    &quot;No Response&quot;)

int_data &lt;- pisa_2015 # Create a safe copy of the data, since it takes about 2 mins to read.</code></pre>
<p>After this, I started doing some basic data manipulation. Each line is followed by a comment on why I did it.</p>
<pre class="r"><code>names(int_data) &lt;- tolower(names(int_data)) # It&#39;s easier to write variable names as lower case
int_data$region &lt;- countrycode(int_data[[country_var]], country_types, &quot;continent&quot;)
# Create a region variable to add regional colours to plots at some point.</code></pre>
<p>Most PISA datasets are in SPSS format, where the variable’s question has been written as a label. If you’ve used SPSS or SAS you know that labels are very common; they basically outline the question of that variable. In R, this didn’t properly exists until the <code>foreign</code> and <code>haven</code> package. With <code>read_spss()</code>, each variable has now two important attributes called <code>label</code> and <code>labels</code>. Respectively, the first one contains the question, while the second contains the value labels (assuming the file to be read has these labels). This information will be vital to our PISA bot. In fact, this script works only if the data has these two attributes. If you’re feeling particularly adventurous, you can fork this repository and make the script work also with metadata!</p>
<p>Have a look at the country names in <code>int_data[[country_var]][1:10]</code>. They’re all written as 3-letter country codes. But to our luck, the <code>labels</code> attribute has the correct names with the 3-letter equivalent. We can save these attributes and recode the 3-letter country name to long names.</p>
<pre class="r"><code># Saving country names to change 3 letter country name to long country names
country_labels &lt;- attr(int_data[[country_var]], &quot;labels&quot;)

# Reversing the 3-letter code to names so I can search for countries
# in a lookup table
country_names &lt;- reverse_name(country_labels)

# Lookup 3-letter code and change them for long country names
int_data[, country_var] &lt;- country_names[int_data[[country_var]]]
attr(int_data[[country_var]], &quot;labels&quot;) &lt;- country_labels</code></pre>
<p>Next thing I’d like to do is check which variables will be valid, i.e. those which have a <code>labels</code> attribute, have 2 or more <code>labels</code> aside from the <code>missing</code> category of labels and are not either characters or factors (remember that all variables should be numeric with an attribute that contains the labels; character columns are actually invalid here). This will give me the list of variables that I’ll be able to use.</p>
<pre class="r"><code>subset_vars &lt;- 
  int_data %&gt;%
  map_lgl(function(x)
    !is.null(attr(x, &quot;labels&quot;)) &amp;&amp;
    length(setdiff(names(attr(x, &quot;labels&quot;)), missing_labels)) &gt;= 2 &amp;&amp;
    !typeof(x) %in% c(&quot;character&quot;, &quot;factor&quot;)) %&gt;%
  which()</code></pre>
<p>Great, we have our vector of valid columns.</p>
<p>The next steps are fairly straight forward. I randomply sample one of those indexes (which have the variale name as a <code>names</code> attribute, check <code>subset_vars</code>), together with the <code>cnt</code> and <code>region</code> variables.</p>
<pre class="r"><code>valid_df_fun &lt;- function(data, vars_select) {
  data %&gt;%
  select_(&quot;cnt&quot;, &quot;region&quot;, sample(names(vars_select), 1)) %&gt;%
  as.data.frame()
}

valid_df &lt;- valid_df_fun(int_data, subset_vars)
random_countries &lt;- unique(valid_df$cnt) # To sample unique countries later on</code></pre>
<p>We also need to check how many labels we have, aside from the <code>missing</code> labels. In any case, if those unique labels have more than 5, we need to resample a new variable. It’s difficult to understand a plot with that many labels. We need to make our plots as simple and straightforward as possible.</p>
<pre class="r"><code>var_labels &lt;- attr(valid_df[[names(valid_df)[3]]], &#39;labels&#39;) # Get labels

# Get unique labels
valid_labels &lt;- function(variable_label, miss) {
  variable_label %&gt;%
    names() %&gt;%
    setdiff(miss)
}

len_labels &lt;- length(valid_labels(var_labels, missing_labels)) # length of unique labels

# While the length of the of the labels is &gt; 4, sample a new variable.
while (len_labels &gt; 4) {
  valid_df &lt;- valid_df_fun(int_data, subset_vars)
  var_labels &lt;- attr(valid_df[[names(valid_df)[3]]], &#39;labels&#39;) # Get labels
  len_labels &lt;- length(valid_labels(var_labels, missing_labels))
}

# Make 100% sure we get the results:
stopifnot(len_labels &lt;= 4)

(labels &lt;- reverse_name(var_labels)) 
# Reverse vector names to objects and viceversa for 
# later recoding.

var_name &lt;- names(valid_df)[3]</code></pre>
<p>Before estimating the <code>PISA</code> proportions, I want to create a record of all variables that have been used. Whenever a graph has something wrong we wanna know which variable it was, so we can reproduce the problem and fix it later in the future.</p>
<pre class="r"><code>new_var &lt;- paste(var_name, Sys.Date(), sep = &quot; - &quot;)
write_lines(new_var, path = &quot;./all_variables.txt&quot;, append = T) 
# I create an empty .txt file to write the vars</code></pre>
<p>Now comes the estimation section. Using the <code>pisa.table</code> function from the package <code>intsvy</code> we can correctly estimate the population proportions of any variable for any valid country. This table will be the core data behind our plot.</p>
<pre class="r"><code>try_df &lt;-
  valid_df %&gt;%
  filter(!is.na(region)) %&gt;%
  pisa.table(var_name, data = ., by = &quot;cnt&quot;) %&gt;%
  filter(complete.cases(.))</code></pre>
<p>Let’s check out the contents of <code>try_df</code>:</p>
<pre><code>##                   cnt pa039q01ta Freq Percentage Std.err.
## 1             Belgium          1 3972      85.16        0
## 2             Belgium          2  692      14.84        0
## 3               Chile          1 6062      96.88        0
## 4               Chile          2  195       3.12        0
## 5             Croatia          1 4353      81.17        0
## 6             Croatia          2 1010      18.83        0
## 7  Dominican Republic          1 4335      98.37        0
## 8  Dominican Republic          2   72       1.63        0
## 9              France          1 4463      84.46        0
## 10             France          2  821      15.54        0</code></pre>
<p>Great! To finish with the data, we simply need one more thing: to recode the value labels with the <code>labels</code> vector.</p>
<pre class="r"><code>try_df[var_name] &lt;- labels[try_df[, var_name]]</code></pre>
<p>Awesome. We have the data ready, more or less. Let’s produce a dirty plot to check how long the title is.</p>
<pre class="r"><code>title_question &lt;- attr(valid_df[, var_name], &#39;label&#39;)

ggplot(try_df, aes_string(names(try_df)[2], &quot;Percentage&quot;)) +
  geom_col() +
  xlab(title_question)</code></pre>
<p><img src="/blog/2017-03-08-my-pisa-twitter-bot/2017-03-08-my-pisa-twitter-bot_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><em>Note: Because the question is randomly sampled, you might be getting a short title. Rerun the script and eventually you’ll get a long one.</em></p>
<p>So, the question <em>might</em> have two problems. The wording is a bit confusing (something we can’t really do anything about because that’s how it’s written in the questionnaire) and it’s too long. For the second problem I created a function that cuts the title in an arbitrary cutoff point (based on experimental tests on how many letters fit into a ggplot coordinate plane) but it makes sure that the cutoff is not in the middle of a word, i.e. it searches for the closest end of a word.</p>
<pre class="r"><code>## Section: Get the title
cut &lt;- 60 # Arbitrary cutoff

# This function accepts a sentence (or better, a title) and cuts it between
# the start and cutoff arguments (just as substr).
# But if the cutoff is not an empty space it will search +-1 index by
# index from the cutoff point until it reaches
# the closest empty space. It will return from start to the new cutoff
sentence_cut &lt;- function(sentence, start, cutoff) {
  
  if (nchar(sentence) &lt;= cutoff) return(substr(sentence, start, cutoff))
  
  excerpt &lt;- substr(sentence, start, cutoff)
  actual_val &lt;- cutoff
  neg_val &lt;- pos_val &lt;- actual_val
  
  if (!substr(excerpt, actual_val, actual_val) == &quot; &quot;) {
    
    expr &lt;- c(substr(sentence, neg_val, neg_val) == &quot; &quot;, substr(sentence, pos_val, pos_val) == &quot; &quot;)
    
    while (!any(expr)) {
      neg_val &lt;- neg_val - 1
      pos_val &lt;- pos_val + 1
      
      expr &lt;- c(substr(sentence, neg_val, neg_val) == &quot; &quot;, substr(sentence, pos_val, pos_val) == &quot; &quot;)
    }
    
    cutoff &lt;- ifelse(which(expr) == 1, neg_val, pos_val)
    excerpt &lt;- substr(sentence, start, cutoff)
    return(excerpt)
    
  } else {
    
    return(excerpt)
    
  }
}

# How many lines in ggplot2 should this new title have? Based on the cut off
sentence_vecs &lt;- ceiling(nchar(title_question) / cut)

# Create an empty list with the length of `lines` of the title.
# In this list I&#39;ll paste the divided question and later paste them together
list_excerpts &lt;- replicate(sentence_vecs, vector(&quot;character&quot;, 0))</code></pre>
<p>Just to make sure our function works, let’s do some quick tests. Let’s create the sentence <code>This is my new sentence</code> and subset from index <code>1</code> to index <code>17</code>. Index <code>17</code> is the letter <code>e</code> from the word <code>sentence</code>, so we should cut the sentence to the closest space, in our case, <code>This is my new</code>.</p>
<pre class="r"><code>sentence_cut(&quot;This is my new sentence&quot;, 1, 17)</code></pre>
<pre><code>## [1] &quot;This is my new &quot;</code></pre>
<p>A more complicated test using <code>I want my sentence to be cut where no word is still running</code>. Let’s pick from index <code>19</code>, which is the space between <code>sentence</code> and <code>to</code>, the index <code>27</code>, which is the <code>u</code> of <code>cut</code>. Because the length to a space <code>-1 and +1</code> is the same both ways, the function always picks the shortest length as a defensive mechanism to long titles.</p>
<pre class="r"><code>sentence_cut(&quot;I want my sentence to be cut where no word is still running&quot;, 19, 27)</code></pre>
<pre><code>## [1] &quot; to be &quot;</code></pre>
<p>Now that we have the function ready, we have to automate the process so that the first line is cut, then the second line should start where the first line left off and so on.</p>
<pre class="r"><code>for (list_index in seq_along(list_excerpts)) {
  
  non_empty_list &lt;- Filter(f = function(x) !(is_empty(x)), list_excerpts)
  
  # If this is the first line, the start should 1, otherwise the sum of all characters
  # of previous lines
  start &lt;- ifelse(list_index == 1, 1, sum(map_dbl(non_empty_list, nchar)))
  
  # Because start gets updated every iteration, simply cut from start to start + cut
  # The appropriate exceptions are added when its the first line of the plot.
  list_excerpts[[list_index]] &lt;-
    sentence_cut(title_question, start, ifelse(list_index == 1, cut, start + cut))
}

final_title &lt;- paste(list_excerpts, collapse = &quot;\n&quot;)</code></pre>
<p>The above loop gives you a list with the title separate into N lines based on the cutoff point. For the ggplot title, we finish by collapsing the separate titles with the <code>\n</code> as the separator.</p>
<p>So, I wrapped all of this into this function:</p>
<pre class="r"><code>label_cutter &lt;- function(variable_labels, cut) {
  
  variable_label &lt;- unname(variable_labels)
  
  # This function accepts a sentence (or better, a title) and cuts it between
  # the start and cutoff arguments ( just as substr). But if the cutoff is not an empty space
  # it will search +-1 index by index from the cutoff point until it reaches
  # the closest empty space. It will return from start to the new cutoff
  sentence_cut &lt;- function(sentence, start, cutoff) {
    
    if (nchar(sentence) &lt;= cutoff) return(substr(sentence, start, cutoff))
    
    excerpt &lt;- substr(sentence, start, cutoff)
    actual_val &lt;- cutoff
    neg_val &lt;- pos_val &lt;- actual_val
    
    if (!substr(excerpt, actual_val, actual_val) == &quot; &quot;) {
      
      expr &lt;- c(substr(sentence, neg_val, neg_val) == &quot; &quot;, substr(sentence, pos_val, pos_val) == &quot; &quot;)
      
      while (!any(expr)) {
        neg_val &lt;- neg_val - 1
        pos_val &lt;- pos_val + 1
        
        expr &lt;- c(substr(sentence, neg_val, neg_val) == &quot; &quot;, substr(sentence, pos_val, pos_val) == &quot; &quot;)
      }
      
      cutoff &lt;- ifelse(which(expr) == 1, neg_val, pos_val)
      excerpt &lt;- substr(sentence, start, cutoff)
      return(excerpt)
      
    } else {
      
      return(excerpt)
      
    }
  }
  
  # How many lines should this new title have? Based on the cut off
  sentence_vecs &lt;- ceiling(nchar(variable_label) / cut)
  
  # Create an empty list with the amount of lines for the excerpts
  # to be stored.
  list_excerpts &lt;- replicate(sentence_vecs, vector(&quot;character&quot;, 0))
  
  for (list_index in seq_along(list_excerpts)) {
    
    non_empty_list &lt;- Filter(f = function(x) !(is_empty(x)), list_excerpts)
    
    # If this is the first line, the start should 1, otherwise the sum of all characters
    # of previous lines
    start &lt;- ifelse(list_index == 1, 1, sum(map_dbl(non_empty_list, nchar)))
    
    # Because start gets updated every iteration, simply cut from start to start + cut
    # The appropriate exceptions are added when its the first line of the plot.
    list_excerpts[[list_index]] &lt;-
      sentence_cut(variable_label, start, ifelse(list_index == 1, cut, start + cut))
  }
  
  final_title &lt;- paste(list_excerpts, collapse = &quot;\n&quot;)
  final_title
}</code></pre>
<p>The function accepts a string and a cut off point. It will automatically create new lines if needed and return the separated title based on the cutoff point. We apply this function over the title and the labels, to make sure everything is clean.</p>
<pre class="r"><code>final_title &lt;- label_cutter(title_question, 60)
labels &lt;- map_chr(labels, label_cutter, 35)</code></pre>
<p>Finally, as I’ve outlined above, each question should have less then four labels. I though that it might be a good idea if I created different graphs for different number of labels. For example, for the two label questions, I thought a simple dot plot might be a good idea —— the space between the dots will sum up to one making it quite intuitive. However, for three and four labels, I though of a cutomized dotplot.</p>
<p>At the time I was writing this bot I was learning object oriented programming, so I said to myself, why not create a generic function that generates different plots for different labels? First, I need to assign the data frame the appropriate class.</p>
<pre class="r"><code>label_class &lt;-
  c(&quot;2&quot; = &quot;labeltwo&quot;, &#39;3&#39; = &quot;labelthree&quot;, &#39;4&#39; = &quot;labelfour&quot;)[as.character(len_labels)]

class(try_df) &lt;- c(class(try_df), label_class)</code></pre>
<p>The generic function, together with its cousin functions, are located in the <code>ggplot_funs.R</code> script in the PISA bot repository linked in the beginning.</p>
<p>The idea is simple. Create a generic function that dispatches based on the class of the object.</p>
<pre class="r"><code>pisa_graph &lt;- function(data, y_title, fill_var) UseMethod(&quot;pisa_graph&quot;)</code></pre>
<pre class="r"><code>pisa_graph.labeltwo &lt;- function(data, y_title, fill_var) {
  
  dots &lt;- setNames(list(interp(~ fct_reorder2(x, y, z),
                               x = quote(cnt),
                               y = as.name(fill_var),
                               z = quote(Percentage))), &quot;cnt&quot;)
  # To make sure we can randomly sample a number lower than the length
  unique_cnt &lt;- length(unique(data$cnt))
  
  data %&gt;%
    filter(cnt %in% sample(unique(cnt), ifelse(unique_cnt &gt;= 15, 15, 10))) %&gt;%
    mutate_(.dots = dots) %&gt;%
    ggplot(aes(cnt, Percentage)) +
    geom_point(aes_string(colour = fill_var)) +
    labs(y = y_title, x = NULL) +
    scale_colour_discrete(name = NULL) +
    guides(colour = guide_legend(nrow = 1)) +
    scale_y_continuous(limits = c(0, 100),
                       breaks = seq(0, 100, 20),
                       labels = paste0(seq(0, 100, 20), &quot;%&quot;)) +
    coord_flip() +
    theme_minimal() +
    theme(legend.position = &quot;top&quot;)
}</code></pre>
<p>This is the graph for the <code>labeltwo</code> class. Using a work around for non-standard evaluation, I reorder the <code>x</code> axis. This took me some time to understand but it’s very easy once you’ve written two or three expressions. Create a list with the formula (this might be for <code>mutate</code>, <code>filter</code> or whatever <code>tidyverse</code> function) and <strong>rename</strong> the placeholders in the formula with the appropriate names. Make sure to name that list object with the new variable name you want for this variable. So, for my example, we’re creating a new variable called <code>cnt</code> that will be the same variable reordered by the <code>fill_var</code> and the <code>Percentage</code> variable.</p>
<p>After this, I just built a usual <code>ggplot2</code> object (although notice that I used <code>mutate_</code> instead of <code>mutate</code> for the non-standard evaluation).</p>
<p>If you’re interested in learning more about standard and non-standard evaluation, I found these resources very useful (<a href="http://www.carlboettiger.info/2015/02/06/fun-standardizing-non-standard-evaluation.html">here</a>, <a href="http://adv-r.had.co.nz/Computing-on-the-language.html">here</a> and <a href="https://cran.r-project.org/web/packages/lazyeval/vignettes/lazyeval.html">here</a>)</p>
<p>The generic for <code>labelthree</code> and <code>labelfour</code> are pretty much the same as the previous plot but using a slightly different <code>geom</code>. Have a look at the original file <a href="https://raw.githubusercontent.com/cimentadaj/PISAfacts_twitterBot/master/ggplot_funs.R">here</a></p>
<p>We’ll, we’re almost there. After this, we simply, <code>source</code> the <code>ggplot_funs.R</code> script and produce the plot.</p>
<pre class="r"><code>source(&quot;https://raw.githubusercontent.com/cimentadaj/PISAfacts_twitterBot/master/ggplot_funs.R&quot;)
pisa_graph(data = try_df,
             y_title = final_title,
             fill_var = var_name)</code></pre>
<p><img src="/blog/2017-03-08-my-pisa-twitter-bot/2017-03-08-my-pisa-twitter-bot_files/figure-html/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>file &lt;- tempfile()
ggsave(file, device = &quot;png&quot;)</code></pre>
</div>
<div id="setting-the-twitter-bot" class="section level2">
<h2>Setting the twitter bot</h2>
<p>The final part is automating the twitter bot. I followed <a href="https://www.r-bloggers.com/programming-a-twitter-bot-and-the-rescue-from-procrastination/">this</a> and <a href="http://www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/">this</a>. I won’t go into the specifics because I probably wouldn’t do justice to the the second post, but you have to create your account on Twitter, this will give you some keys that make sure you’re the right person. <em>You need to write these key-value pairs as environment variables</em> (follow the second post) and then delete them from your R script (they’re secret! You shouldn’t keep them on your script but on some folder on your computer). Finally, make sure you identify your twitter account and make your first tweet!</p>
<pre class="r"><code>library(twitteR) # devtools::install_github(&quot;geoffjentry/twitteR&quot;)
setwd(&quot;./folder_with_my_credentials/&quot;)

api_key             &lt;- Sys.getenv(&quot;twitter_api_key&quot;)
api_secret          &lt;- Sys.getenv(&quot;twitter_api_secret&quot;)
access_token        &lt;- Sys.getenv(&quot;twitter_access_token&quot;)
access_token_secret &lt;- Sys.getenv(&quot;twitter_access_token_secret&quot;)
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)

tweet(&quot;&quot;, mediaPath = file)
unlink(file)</code></pre>
<p>That’s it! The last line should create the<code>tweet</code>.</p>
</div>
<div id="automating-the-bot" class="section level2">
<h2>Automating the bot</h2>
<p>The only thing left to do is automate this to run every day. I’ll explain how I did it for OSx by following <a href="http://www.techradar.com/how-to/computing/apple/terminal-101-creating-cron-jobs-1305651">this</a> tutorial. You can find a Windows explanation in step 3 <a href="https://www.r-bloggers.com/programming-a-twitter-bot-and-the-rescue-from-procrastination/">here</a>.</p>
<p>First, we need to figure out the specific time we want to schedule the script. We define the time by filling out five stars:</p>
<p><code>*****</code></p>
<ul>
<li>The first asterisk is for specifying the minute of the run (0-59)</li>
<li>The second asterisk is for specifying the hour of the run (0-23)</li>
<li>The third asterisk is for specifying the day of the month for the run (1-31)</li>
<li>The fourth asterisk is for specifying the month of the run (1-12)</li>
<li>The fifth asterisk is for specifying the day of the week (where Sunday is equal to 0, up to Saturday is equal to 6)</li>
</ul>
<p>Taken from <a href="http://www.techradar.com/how-to/computing/apple/terminal-101-creating-cron-jobs-1305651">here</a></p>
<p>For example, let’s say we wanted to schedule the script for <code>3:00 pm</code> every day, then the combination would be <code>0 15 * * *</code>. If we wanted something every <code>15</code> minutes, then <code>15 * * * *</code> would do it. If we wanted to schedule the script for Mondays and Wednesdays at <code>15:00</code> and <code>17:00</code> respectively, then we would write <code>0 15,17 * * 1,3</code>. In this last example the <code>* *</code> are the placeholders for day of the month and month.</p>
<p>In my example, I want the script to run every weekday at <code>9:30</code> am, so my equivalent would be <code>30 9 * * 1-5</code>.</p>
<p>To begin, we type <code>env EDITOR=nano crontab -e</code> in the <code>terminal</code> to initiate the <code>cron</code> file that will run the script. Next, type our time schedule followed by the command that will run the script in R. The command is <code>RScript</code>. However, because your terminal might not know where <code>RScript</code> is we need to type the directory to where RScript is. Type <code>which RScript</code> in the terminal and you shall get something like <code>/usr/local/bin/RScript</code>. Then the expression would be something like <code>30 9 * * 1-5 /usr/local/bin/RScript path_to_your/script.R</code>. See <a href="https://support.rstudio.com/hc/en-us/articles/218012917-How-to-run-R-scripts-from-the-command-line">here</a> for the <code>RScript</code> explanation.</p>
<p>The whole sequence would be like this:</p>
<pre class="bash"><code>env EDITOR=nano crontab -e
30 9 * * 1-5 /usr/local/bin/RScript path_to_your/script.R</code></pre>
<p>To save the file, press Control + O (to write out the file), then enter to accept the file name, then
press Control + X (to exit nano). If all went well, then you should see “crontab: installing new crontab” without anything after that.</p>
<p>Aaaaaand that’s it! You now have a working script that will be run from Monday to Friday at 9:30 am. This script will read the PISA data, pick a random variable, make a graph and tweet it. You can follow this twitter account at <span class="citation">[@DailyPISA_Facts]</span>(<a href="https://twitter.com/DailyPISA_Facts" class="uri">https://twitter.com/DailyPISA_Facts</a>).</p>
<p>Hope this was useful!</p>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>Cognitive inequality around the world – Shiny app</title>
      <link>/blog/2016-12-12-cognitive-inequality-around-the-world-shiny-app/cognitive-inequality-around-the-world-shiny-app/</link>
      <pubDate>Mon, 12 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-12-12-cognitive-inequality-around-the-world-shiny-app/cognitive-inequality-around-the-world-shiny-app/</guid>
      <description><![CDATA[
      


<p>For the last month I’ve been working on this massive dataset that combines all PISA, TIMSS and PIRLS surveys into one major database. It has over 3 million students and over 2,000 variables, including student background and school and teacher information. I started playing around with it and ending up doing this: <a href="https://cimentadaj.shinyapps.io/shiny/" class="uri">https://cimentadaj.shinyapps.io/shiny/</a>. Feel free to check it out and drop any comments below.</p>
<p>If you want to contribute, <a href="https://github.com/cimentadaj/Inequality_Shinyapp">this</a> is the Github repository. I plan to keep adding some stuff to the app, including new surveys and automatic plot downloading, so don’t forget to check it out.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Fitting the wrong model</title>
      <link>/blog/2016-11-10-fitting-the-wrong-model/fitting-the-wrong-model/</link>
      <pubDate>Thu, 10 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-11-10-fitting-the-wrong-model/fitting-the-wrong-model/</guid>
      <description><![CDATA[
      


<p>These exercises are from the book <a href="http://www.stat.columbia.edu/~gelman/arm/">Data Analysis Using Regression and Multilevel/Hierarchical Models</a>. I’ve really gotten into completing these exercises and I guess that by posting them I’ve found an excuse to keep doing it. This time I went back to chapter 8 which deals with simulations. I picked the first exercise, page 165 exercise 8.6.1, which says:</p>
<blockquote>
<p>Fitting the wrong model: suppose you have 100 data points that arose from the following model: y = 3 + 0.1×1 + 0.5×2 + error, with errors having a t distribution with mean 0, scale 5, and 4 degrees of freedom.We shall explore the implications of fitting a standard linear regression to these data.</p>
</blockquote>
<p>The (a) section of the exercises says as follows:</p>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Simulate data from this model. For simplicity, suppose the values of x1 are simply the integers from 1 to 100, and that the values of x2 are random and equally likely to be 0 or 1. Fit a linear regression (with normal errors) to these data and see if the 68% confidence intervals for the regression coefficients (for each, the estimates ±1 standard error) cover the true values.</li>
</ol>
</blockquote>
<p>This is simple enough. Simulate some linear model but change the error term to be t distributed with a set of characteristics. Here’s the code:</p>
<pre class="r"><code>suppressWarnings(suppressMessages({
  library(arm)
  library(broom)
  library(hett)
  }))

set.seed(2131)
x1 &lt;- 1:100
x2 &lt;- rbinom(100, 1, 0.5)
error1 &lt;- rt(100, df=4)*sqrt(5 * (4-2)/4) + 0 # t distributed errors
                                              # with df 4, mean 0 and var 5

y = 3 + 0.1*x1 + 0.5*x2 + error1

display(lm(y ~ x1 + x2))</code></pre>
<pre><code>## lm(formula = y ~ x1 + x2)
##             coef.est coef.se
## (Intercept) 3.30     0.43   
## x1          0.10     0.01   
## x2          0.33     0.40   
## ---
## n = 100, k = 3
## residual sd = 1.96, R-Squared = 0.67</code></pre>
<p>It looks like the true slope of x1 is contained in the 68% CI’s.</p>
<pre class="r"><code>c(upper = 0.10 + (1 * 0.01), lower = 0.10 + (-1 * 0.01))</code></pre>
<pre><code>## upper lower 
##  0.11  0.09</code></pre>
<p>For x2 it’s contained but the uncertainty is too high making the CI’s too wide.</p>
<pre class="r"><code>c(upper = 0.33 + (1 * 0.40), lower = 0.33 + (-1 * 0.40))</code></pre>
<pre><code>## upper lower 
##  0.73 -0.07</code></pre>
<hr />
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Put the above step in a loop and repeat 1000 times. Calculate the confidence coverage for the 68% intervals for each of the three coefficients in the model.</li>
</ol>
</blockquote>
<pre class="r"><code>coefs &lt;- matrix(NA, nrow = 3, ncol = 1000)
se &lt;- matrix(NA, nrow = 3, ncol = 1000)

# Naturally, these estimates will be different for anyone who runs this code
# even if specifying set seed because the loop will loop new numbers each time.

for (i in 1:ncol(coefs)) {
  x1 &lt;- 1:100
  x2 &lt;- rbinom(100, 1, 0.5)
  error1 &lt;- rt(100, df=4)*sqrt(5 * (4-2)/4) + 0 # t distributed errors
                                                # with df 4 and mean 0
  y = 3 + 0.1*x1 + 0.5*x2 + error1
  
  mod1 &lt;- summary(lm(y ~ x1 + x2))
  coefs[1,i] &lt;- tidy(mod1)[1,2, drop = TRUE]
  coefs[2,i] &lt;- tidy(mod1)[2,2, drop = TRUE]
  coefs[3,i] &lt;- tidy(mod1)[3,2, drop = TRUE]
  
  se[1,i] &lt;- tidy(mod1)[1,3, drop = TRUE]
  se[2,i] &lt;- tidy(mod1)[2,3, drop = TRUE]
  se[3,i] &lt;- tidy(mod1)[3,3, drop = TRUE]
}

repl_coef &lt;- rowMeans(coefs)
repl_se &lt;- rowMeans(se)

cbind(repl_coef + (-1 * repl_se), repl_coef + (1 * repl_se))</code></pre>
<pre><code>##            [,1]      [,2]
## [1,] 2.48215326 3.4808804
## [2,] 0.09255549 0.1079026
## [3,] 0.05919238 0.9495994</code></pre>
<p>Going back to previous block of code which contains the true parameters, the 68% interval for the intercept does contain 3, the 68% interval for x1 does contain 0.10 and both CI’s are quite precise. Finally, the confidence interval for x2 does contain 0.5 but the uncertainty is huge. What does this mean? The estimation of the slope for x2 does contain the true parameter but given that our error is too big and the normal distribution of <code>lm</code> does not account for that, it presents much more uncertainty in the estimation of the slope. If we ran a t distributed <code>lm</code> then it will certainly be more precise.</p>
<p>The last section of the exercise asks you to do exactly that. Repeat the previous loop but instead of using <code>lm</code>, use <code>tlm</code> from the hett package which accounts for a t-distributed error term. Compare the CI’s and coefficients. Let’s do it:</p>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Repeat this simulation, but instead fit the model using t errors (see Exercise 6.6). The only change here is defining error1 as a t distribution instead of normally distributed</li>
</ol>
</blockquote>
<pre class="r"><code>coefs &lt;- matrix(NA, nrow = 3, ncol = 1000)
se &lt;- matrix(NA, nrow = 3, ncol = 1000)

for (i in 1:ncol(coefs)) {
  x1 &lt;- 1:100
  x2 &lt;- rbinom(100, 1, 0.5)
  error1 &lt;- rt(100, df=4)*sqrt(5 * (4-2)/4) + 0 # t distributed errors
  y = 3 + 0.1*x1 + 0.5*x2 + error1
  
  mod1 &lt;- summary(tlm(y ~ x1 + x2))
  coefs[1,i] &lt;- mod1$loc.summary$coefficients[1,1]
  coefs[2,i] &lt;- mod1$loc.summary$coefficients[2,1]
  coefs[3,i] &lt;- mod1$loc.summary$coefficients[3,1]
  
  se[1,i] &lt;- mod1$loc.summary$coefficients[1,2]
  se[2,i] &lt;- mod1$loc.summary$coefficients[2,2]
  se[3,i] &lt;- mod1$loc.summary$coefficients[3,2]
}

repl_coef &lt;- rowMeans(coefs)
repl_se &lt;- rowMeans(se)

cbind(repl_coef + (-1 * repl_se), repl_coef + (1 * repl_se))</code></pre>
<pre><code>##            [,1]      [,2]
## [1,] 2.61212284 3.4265738
## [2,] 0.09335461 0.1058854
## [3,] 0.14971691 0.8769205</code></pre>
<p>Accounting for the t-distributed error (so the tails are much wider), the intervals for the intercept and x1 are quite similar (but narrower) and for x2 they’re certainly much more narrow. Note that the CI is still pretty big, reflecting the variance in the error term. But whenever this variance exceeds what a normal distribution can capture, we should account for it: it might help to reduce the uncertainty in the estimation. Note that, if you reran both simulations and compare the coefficients in <code>repl_coef</code>, they’re practically the same. So the different estimations don’t affect the parameters, but rather the uncertainty with which we trust them.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Multilevel modeling – Part 1</title>
      <link>/blog/2016-11-06-multilevel-modeling-part-1/multilevel-modeling-part-1/</link>
      <pubDate>Sun, 06 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-11-06-multilevel-modeling-part-1/multilevel-modeling-part-1/</guid>
      <description><![CDATA[
      


<p>I’ve been reading Andrew Gelman’s and Jennifer Hill’s book again but this time concentrating on the multilevel section of the book. I finished the first chapter (chapter 12) and got fixed on the exercises 12.2, 12.3 and 12.4. I finally completed them and I thought I’d share the three exercises in two posts, mostly for me to come back to these in the future. The first exercise goes as follows:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Write a model predicting CD4 percentage as a function of time with varying intercepts across children. Fit using lmer() and interpret the coefficient for time.</p></li>
<li><p>Extend the model in (a) to include child-level predictors (that is, group-level predictors) for treatment and age at baseline. Fit using lmer() and interpret the coefficients on time, treatment, and age at baseline.</p></li>
<li><p>Investigate the change in partial pooling from (a) to (b) both graphically and numerically.</p></li>
<li><p>Compare results in (b) to those obtained in part (c).</p></li>
</ol>
<p>The data set they’re referring is called ‘CD4’ and as they authors explain in the book it measures ‘… CD4 percentages for a set of young children with HIV who were measured several times over a period of two years. The dataset also includes the ages of the children at each measurement..’. I’m not sure what CD4 means, but that shouldn’t stop us from at least interpreting the results and answering the questions. Let’s start with the exercises:</p>
<hr />
<ol style="list-style-type: lower-alpha">
<li>Write a model predicting CD4 percentage as a function of time with
varying intercepts across children. Fit using lmer() and interpret the coefficient for time. The data argument is excluding some NA’s because the next model is to be compared with this model and we need to have the same number of observations</li>
</ol>
<pre class="r"><code>suppressWarnings(suppressMessages(library(arm)))
cd4 &lt;- read.csv(&quot;http://www.stat.columbia.edu/~gelman/arm/examples/cd4/allvar.csv&quot;)

head(cd4)</code></pre>
<pre><code>##   VISIT newpid       VDATE CD4PCT arv   visage treatmnt CD4CNT baseage
## 1     1      1  6/29/1988      18   0 3.910000        1    323    3.91
## 2     4      1  1/19/1989      37   0 4.468333        1    610    3.91
## 3     7      1  4/13/1989      13   0 4.698333        1    324    3.91
## 4    10      1                 NA   0 5.005000        1     NA    3.91
## 5    13      1 11/30/1989      13   0 5.330833        1    626    3.91
## 6    16      1                 NA  NA       NA        1    220    3.91</code></pre>
<pre class="r"><code># Let&#39;s transform the VDATE variable into date format
cd4$VDATE &lt;- as.Date(cd4$VDATE, format = &quot;%m/%d/%Y&quot;)

mod1 &lt;- lmer(CD4PCT ~
               VDATE +
               (1 | newpid),
             data = subset(cd4, !is.na(treatmnt) &amp; !is.na(baseage)))

display(mod1)</code></pre>
<pre><code>## lmer(formula = CD4PCT ~ VDATE + (1 | newpid), data = subset(cd4, 
##     !is.na(treatmnt) &amp; !is.na(baseage)))
##             coef.est coef.se
## (Intercept) 66.04     9.48  
## VDATE       -0.01     0.00  
## 
## Error terms:
##  Groups   Name        Std.Dev.
##  newpid   (Intercept) 11.65   
##  Residual              7.31   
## ---
## number of obs: 1072, groups: newpid, 250
## AIC = 7914, DIC = 7885.8
## deviance = 7895.9</code></pre>
<p>The time coefficient simply means that as time increases the percentage of CD4 decreases by 0.01 percent for each child. The effect size is really small, although significant. We can also see that most of the variation in CD4 is between children rather than within children (that is between time because that’s the variation within each child)</p>
<hr />
<ol start="2" style="list-style-type: lower-alpha">
<li>Extend the model in (a) to include child-level predictors (that is, group-level predictors) for treatment and age at baseline. Fit using lmer() and interpret the coefficients on time, treatment, and age at baseline.</li>
</ol>
<pre class="r"><code>mod2 &lt;- lmer(CD4PCT ~
               VDATE +
               treatmnt +
               baseage +
               (1 | newpid),
             data = cd4)

display(mod2)</code></pre>
<pre><code>## lmer(formula = CD4PCT ~ VDATE + treatmnt + baseage + (1 | newpid), 
##     data = cd4)
##             coef.est coef.se
## (Intercept) 67.28     9.82  
## VDATE       -0.01     0.00  
## treatmnt     1.26     1.54  
## baseage     -1.00     0.34  
## 
## Error terms:
##  Groups   Name        Std.Dev.
##  newpid   (Intercept) 11.45   
##  Residual              7.32   
## ---
## number of obs: 1072, groups: newpid, 250
## AIC = 7906.3, DIC = 7878.8
## deviance = 7886.5</code></pre>
<p>The time coefficients is exactly the same so neither the treatment or the base age is correlated with the date in which the students were measured. Those who were treated have on average about 1.26% more CD4 than the non-treated. And finally, children which were older at the base measure have about 1% less CD4 than younger children at base. The between-child variance went down from 11.65 to 11.45, so either treatment, baseage or both explained some of the differences between children. The within child variation is practically the same.</p>
<p>The next exercises uses a term called ‘partial pooling’. This term took me some time to understand but it basically means that we’re neither running a regression ignoring any multilevel structure (complete pooling of the groups) or running a regression for each group separately (complete no-pooling). Running a partially pooled model means being able to have single parameters (like in a completely-pooled model), but estimated from separate regression models for each group(like in a complete-no-pooled model).</p>
<p>How we can investigate the changes in partial pooling? A completely pooled model runs perfectly when you have little to no variation between groups. Whenever a set of predictors shrinks the between group variation, we’re getting closer to a model with less and less between group variation ( so completely pooled). How can we measure this? In our case, because we’re modeling a varying intercept, we can compare the confidence intervals of the intercept of each group intercept and see if the estimation has become more certain. Numerically, we can check whether the between group variation has decreased, becoming closer to a completely-pooled model.</p>
<hr />
<ol start="3" style="list-style-type: lower-alpha">
<li>Investigate the change in partial pooling from (a) to (b) both graphically and numerically.</li>
</ol>
<pre class="r"><code>suppressMessages(suppressWarnings(library(ggplot2)))
# Change in standard errors

# First and second model intercepts
df1 &lt;- coef(mod1)$newpid[,1 , drop = F]
df2 &lt;- coef(mod2)$newpid[,1 , drop = F]
names(df1) &lt;- c(&quot;int&quot;)
names(df2) &lt;- c(&quot;int&quot;)

# Confidence intervals for each intercept for both moels
df1$ci_bottom &lt;- df1$int + (-2 * se.ranef(mod1)$newpid[,1])
df1$ci_upper &lt;- df1$int + (2 * se.ranef(mod1)$newpid[,1])

df2$ci_bottom &lt;- df2$int + (-2 * se.ranef(mod2)$newpid[,1])
df2$ci_upper &lt;- df2$int + (2 * se.ranef(mod2)$newpid[,1])

# Now we need to compare whether the CI&#39;s shrunk from
# the first to the second model

# Calculate difference
df1$diff &lt;- df1$ci_upper - df1$ci_bottom
df2$dff &lt;- df2$ci_upper - df2$ci_bottom

# Create a df with both differences
df3 &lt;- data.frame(cbind(df1$diff, df2$dff))

# Create a difference out of that
df3$diff &lt;- df3$X1 - df3$X2

# Graph it
ggplot(df3, aes(diff)) + geom_histogram(bins = 100) +
  xlim(0, 0.2)</code></pre>
<p><img src="/blog/2016-11-06-multilevel-modeling--part-1/2016-11-06-multilevel-modeling--part-1_files/figure-html/fig.-1.png" width="672" /></p>
<p>It looks like the difference is always higher than zero which means that in the second model the difference between the upper and lower CI is smaller than in the first model. This suggests we have greater certainty of our estimation by including the two predictors in the model.</p>
<pre class="r"><code># Numerically, the between-child variance in the first
# model was:
display(mod1)</code></pre>
<pre><code>## lmer(formula = CD4PCT ~ VDATE + (1 | newpid), data = subset(cd4, 
##     !is.na(treatmnt) &amp; !is.na(baseage)))
##             coef.est coef.se
## (Intercept) 66.04     9.48  
## VDATE       -0.01     0.00  
## 
## Error terms:
##  Groups   Name        Std.Dev.
##  newpid   (Intercept) 11.65   
##  Residual              7.31   
## ---
## number of obs: 1072, groups: newpid, 250
## AIC = 7914, DIC = 7885.8
## deviance = 7895.9</code></pre>
<pre class="r"><code>11.65 / (11.65 + 7.31)</code></pre>
<pre><code>## [1] 0.6144515</code></pre>
<pre class="r"><code># For the second model
display(mod2)</code></pre>
<pre><code>## lmer(formula = CD4PCT ~ VDATE + treatmnt + baseage + (1 | newpid), 
##     data = cd4)
##             coef.est coef.se
## (Intercept) 67.28     9.82  
## VDATE       -0.01     0.00  
## treatmnt     1.26     1.54  
## baseage     -1.00     0.34  
## 
## Error terms:
##  Groups   Name        Std.Dev.
##  newpid   (Intercept) 11.45   
##  Residual              7.32   
## ---
## number of obs: 1072, groups: newpid, 250
## AIC = 7906.3, DIC = 7878.8
## deviance = 7886.5</code></pre>
<pre class="r"><code>11.45 / (11.45 + 7.32)</code></pre>
<pre><code>## [1] 0.610016</code></pre>
<p>The between variance went down JUST a little, in line with the tiny reduction in the standard errors of the intercept.</p>
<hr />
<p>The last question asks:</p>
<p>Compare results in (b) to those obtained in part (c).</p>
<p>It looks like from the results in (c) the second model a bit more certain in making estimations because it shrinks the partial pooling closer to complete-pooling. As Gelman and Hill explain in page 270, multilevel modeling is most effective when closest to complete-pooling because the estimation of individual group parameters can be done much more precisely, specially for groups with a small amount of observations.</p>
<p>In the next post we’ll cover exercises 12.3 and 12.4 which build on the models outlined here.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Multilevel modeling – Part 2</title>
      <link>/blog/2016-11-06-multilevel-modeling-part-2/multilevel-modeling-part-2/</link>
      <pubDate>Sun, 06 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-11-06-multilevel-modeling-part-2/multilevel-modeling-part-2/</guid>
      <description><![CDATA[
      


<p>This is a continuation of the multilevel exercise 12.2 from chapter 12 of Data Analysis Using Regression and Multilevel/Hierarchical Models. In the last post (link to previous post) we explored the basic interpretation of multilevel modeling with a varying intercept and delved into comparing models and understanding partial pooling.</p>
<p>This was all done by completing the exercise 12.2 from chapter 12 of the book Data Analysis Using Regression and Multilevel/Hierarchical Models. Exercises 12.3 and 12.4 continue using the same dataset and the same models, so I figured I’d post them here. Let’s start.</p>
<p>Exercise 12.3 asks:</p>
<ol start="3" style="list-style-type: decimal">
<li>Predictions for new observations and new groups:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li><p>Use the model fit from Exercise 12.2 (b) to generate simulation of predicted CD4 percentages for each child in the dataset at a hypothetical next time point.</p></li>
<li><p>Use the same model fit to generate simulations of CD4 percentages at each of the time periods for a new child who was 4 years old at baseline.</p></li>
</ol>
<p>Let’s start:</p>
<pre class="r"><code>suppressWarnings(suppressMessages(library(arm)))
cd4 &lt;- read.csv(&quot;http://www.stat.columbia.edu/~gelman/arm/examples/cd4/allvar.csv&quot;)

# Let&#39;s recreate the model from 12.2 (b)
cd4$VDATE &lt;- as.Date(cd4$VDATE, format = &quot;%m/%d/%Y&quot;)
mod2 &lt;- lmer(CD4PCT ~ VDATE + treatmnt + baseage + (1 | newpid), data = cd4)</code></pre>
<ol style="list-style-type: lower-alpha">
<li>Use the model fit from Exercise 12.2 (b) to generate simulation of predicted CD4 percentages for each child in the dataset at a hypothetical next time point.</li>
</ol>
<p>For this we have to create a hypothetical next time point. Let’s choose:</p>
<pre class="r"><code>pred_data &lt;- subset(cd4, !is.na(treatmnt) &amp; !is.na(baseage))
pred_data$VDATE &lt;- as.Date(&quot;1989-01-01&quot;)


# Let&#39;s select only the independent variables from the model
pred_data &lt;- pred_data[, -c(1, 4, 5, 6, 8)]

# Now we have a dataset with a fixed new date for each child
# but with the original values for all other variables.

# Let&#39;s estimate the predicted CD4 percentage for each child:
newpred &lt;- predict(mod2, newdata = pred_data)

# That it! We have the predicted percentage of CD4 for a fixed date for every child.
# Let&#39;s look at the distribution
hist(newpred)</code></pre>
<p><img src="/blog/2016-11-06-multilevel-modeling--part-2/2016-11-06-multilevel-modeling--part-2_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>There’s quite some variation going from 0% to even 60%. It’d be nice to know what CD4 is.</p>
<hr />
<p>Question (b) is pretty similar but instead of asking for a new time point for each child, it asks to predict the CD4 percentage for a new child who was 4 years old at the baseline with a set of hypothetical time periods.</p>
<ol start="2" style="list-style-type: lower-alpha">
<li>Use the same model fit to generate simulations of CD4 percentages at each of the time periods for a new child who was 4 years old at baseline.</li>
</ol>
<p>For that we have to first create the dataset with the hypothetical child:</p>
<pre class="r"><code># I&#39;ll assume 7 hypothetical dates
hyp_child &lt;- data.frame(newpid = 120,
                        VDATE = sample(cd4$VDATE[!is.na(cd4$VDATE)], 7),
                        baseage = 4,
                        treatmnt = 1)</code></pre>
<p>The exercise doesn’t say anything about the treatment variable but I have to assign a value to it because it’s present in the previous model. I’ll assume this new child was in the treatment.</p>
<pre class="r"><code>year_pred &lt;- predict(mod2, newdata = hyp_child)</code></pre>
<p>That’s it! Now we have the predicted CD4 percentage for different time points
for a hypothetical child.</p>
<p>Finally, exercise 12.4 posits this question:</p>
<blockquote>
<p>Posterior predictive checking: continuing the previous exercise, use the fitted model from Exercise 12.2 (b) to simulate a new dataset of CD4 percentages (with the same sample size and ages of the original dataset) for the final time point of the study, and record the average CD4 percentage in this sample. Repeat this process 1000 times and compare the simulated distribution to the observed CD4 percentage at the final time point for the actual data.</p>
</blockquote>
<p>This problem was a bit confusing because I don’t understand what they mean by the ‘final time point of the study’. Is it the last date in the VDATE variable? But that couldn’t be it because the sample size would be 1. I think what they mean is to take the original data and replace the VDATE variable with the final date available. Predict CD4 percentages for each child with this date (using the original ages of the data set) and calculate the mean of these predictions. After that, do the 1000 replications and compare the distribution with the actual CD4 percentage of the specific date. Let’s start.</p>
<pre class="r"><code># Make the data similar to the model in mod2
finaltime_data &lt;- subset(cd4, !is.na(treatmnt) &amp; !is.na(baseage))

# Assign the final date to the date variable
finaltime_data$VDATE &lt;- as.Date(max(finaltime_data$VDATE, na.rm = T))

# Only select the variables present in the model
finaltime_data &lt;- finaltime_data[, -c(1, 4, 5, 6, 8)]

# Calculate the mean predicted CD4 percentage and it&#39;s standard deviation
mean_cd4 &lt;- mean(predict(mod2, newdata = finaltime_data), na.rm = T)
sd_cd4 &lt;- sd(predict(mod2, newdata = finaltime_data), na.rm = T)

# Simulate 1000 observations considering the uncertainty
# and look at the distribution.
set.seed(421)

hist(rnorm(1000, mean_cd4, sd = sd_cd4))</code></pre>
<p><img src="/blog/2016-11-06-multilevel-modeling--part-2/2016-11-06-multilevel-modeling--part-2_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>It’s incredibly wide and it even includes negative numbers, something impossible with percentages. But whats the actual CD4 percentage of that date?</p>
<pre class="r"><code>cd4 &lt;- subset(cd4, !is.na(VDATE))
cd4[cd4$VDATE == max(cd4$VDATE, na.rm = T), ]</code></pre>
<pre><code>##      VISIT newpid      VDATE CD4PCT arv   visage treatmnt CD4CNT  baseage
## 483     16     99 1991-01-14   39.0   1 1.497500        2    526 0.347500
## 1208    19    237 1991-01-14   21.2   0 2.510833        2   1036 1.073333</code></pre>
<p>And the mean is:</p>
<pre class="r"><code>mean(c(39.0, 21.2))</code></pre>
<pre><code>## [1] 30.1</code></pre>
<p>So our predictions are extremely uncertain. Over half our simulations are underestimating the date and a handful are overestimating the results. It looks like our model is terrible at predicting that final time point.</p>
<hr />
<p>Well that’s it. Hope you enjoyed it. I’ll be completing more exercises in the future so remember to check back.
```</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Los pobres no son los peores pero los ricos sí son los mejores: desigualdad educativa</title>
      <link>/blog/2016-09-24-los-pobres-no-son-los-peores-pero-los-ricos-s%C3%AD-son-los-mejores/los-pobres-no-son-los-peores-pero-los-ricos-s%C3%AD-son-los-mejores-desigualdad-educativa/</link>
      <pubDate>Sat, 24 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-09-24-los-pobres-no-son-los-peores-pero-los-ricos-s%C3%AD-son-los-mejores/los-pobres-no-son-los-peores-pero-los-ricos-s%C3%AD-son-los-mejores-desigualdad-educativa/</guid>
      <description><![CDATA[
      


<p>Hace poco escribí un artículo sobre la desigualdad educativa en países Latinoamericanos y describí como la República Dominicana no parece estar tan mal como creemos. Ofrecí dos posibles explicaciones de porque esto pudiera ser bueno o malo, dependiendo del enfoque con que se ve (missing link after the ‘,’ with this: <a href="http://www.jorgecimentada.com/?p=137" class="uri">http://www.jorgecimentada.com/?p=137</a>). Luego de escribirlo me quedé pensando en algo que di por sentado: ¿realmente los ‘peores’ estudiantes son los estudiantes pobres? ¿son los estudiantes pobres los ‘peores’ en las escuelas públicas y privadas? Y a raíz del artículo de <a href="http://acento.com.do/2016/opinion/8377420-desigualdad-educativa-interpela-desafia/">Dinorah Garcia</a>, ¿existe desigualdad entre las zonas rurales y urbanas?</p>
<p>Estas preguntas no solo ayudan a diagnosticar el estado actual de la desigualdad, más que eso, permiten inferir de donde pudieran provenir las fuentes de desigualdad. Si encontramos una diferencia bastante amplia entre los ‘mejores’ y ‘peores’ estudiantes, pero la diferencia es menor entre escuelas públicas y privada, pues la causa no es necesariamente las diferencias en calidad entre las tipos de escuelas. Sin embargo, si existen diferencias similares entre los estudiantes de escuelas rurales y urbanas, pues la razón puede ser precisamente en temas como la calidad educativa entre sectores pobres y ricos. Lo interesante de estas preguntas es que son todas preguntas empíricas, y se pueden responder con bases de datos como las de TERCE.</p>
<div id="realmente-los-peores-estudiantes-son-los-estudiantes-pobres" class="section level2">
<h2>1. ¿realmente los ‘peores’ estudiantes son los estudiantes pobres?</h2>
<!-- ![1stplot](img/inquality_grade_topic.png) -->
<p><img src="img/inquality_grade_topic.png" alt="Drawing" style="width: 600px;"/></p>
<p>Este gráfico presente claramente que no lo son. Tanto en matemáticas y lenguaje, los estudiantes más pobres (barra azul) suelen tener una mejor puntuación que los ‘peores’ estudiantes (barra roja). Esto parecer ser el caso sin importar que sean de la zona rural o urbana. Sería informativo comparar estos resultados desagregados por tipo de escuela, es decir, privada o pública, para identificar si los ‘peores’ estudiantes son los de las escuelas públicas.</p>
<p>En contraste, los estudiantes ricos (barra morada) si tienen la misma puntuación que los ‘mejores’ estudiantes (barra verde). De hecho, en zonas rurales los estudiantes ricos se desempeñan tan bien que obtienen una puntuación más alta que los ‘mejores’ estudiantes. Este gráfico cuenta una historia muy interesante. En resumen, los estudiantes pobres no reflejan ser los ‘peores’ del sistema educativo. Y por otro lado, los estudiantes ricos sí son los mejores estudiantes en sus respectivas materias.</p>
<p>Desde un punto de vista más general, la distribución entre la zona rural y urbana es muy similar, sugiriendo que no existe una desigualdad muy marcada.</p>
</div>
<div id="son-los-estudiantes-pobres-los-peores-en-las-escuelas-publicas-y-privadas" class="section level2">
<h2>2. ¿son los estudiantes pobres los ‘peores’ en las escuelas públicas y privadas?</h2>
<!-- ![2ndplot](img/inquality_public_topic.png) -->
<p><img src="img/inquality_public_topic.png" alt="Drawing" style="width: 650px;"/></p>
<p>En este gráfico el panorama es un poco diferente. Los estudiantes pobres (barra verde) sí se desempeñan peor que los estudiantes promedios (barra roja) en las escuelas privadas. Esto no significa que son los ‘peores’ ya que el punto de comparación es el estudiante promedio y no los ‘peores’ estudiantes de las escuelas privadas. No obstante, si se nota una estratificación bastante clara: los estudiantes pobres tienen puntuación más baja que los estudiantes promedio, mientras que los estudiantes ricos (barra azul) tienen mayor puntuación que el estudiante average.</p>
<p>Viendo la otra cara de la moneda, las escuelas públicas, los estudiantes pobres si tienen el mismo desempeño que el estudiante promedio. Esto no significa que han mejorado en comparación con la escuela privada. Todo lo contrario. Los estudiantes pobres tienen un desempeño similar tanto en las escuelas públicas y privadas; lo que pasa es que el estudiante promedio de la escuela pública (barra roja) tienen una puntuación mucho más baja que el mismo estudiante en la escuela privada. Esto sugiere que, en promedio, los estudiantes de escuelas públicas tienen un peor desempeño que los de las escuelas privadas. De hecho, los ricos son igual de buenos sin importar el tipo de escuela. Los pobres, por otro lado, tienen una puntuación similar entre escuelas públicas y privadas, pero tienen un desempeño ligeramente menor en las escuelas públicas.</p>
</div>
<div id="existe-desigualdad-entre-las-zonas-rurales-y-urbanas" class="section level2">
<h2>3. ¿existe desigualdad entre las zonas rurales y urbanas?</h2>
<!-- ![3rdplot](img/inquality_ruralidad_topic.png) -->
<p><img src="img/inquality_ruralidad_topic.png" alt="Drawing" style="width: 650px;"/></p>
<p>Este gráfico presenta que tanta desigualdad existe entre los estudiantes en 3ro y 6to de primaria en matemática y lengua para las zonas rurales y urbanas. Empecemos con la lengua española, la sección con las puntuaciones en color rojo. Para los estudiantes de 3ro de primaria, resulta que hay una desigualdad mucho más grande en la zona urbana que en la zona rural. Pero esta desigualdad es en gran parte debido a que los ‘mejores’ estudiantes de la zona urbana son mucho mejor que los de la zona rural, mientras que los ‘peores’ estudiantes son prácticamente iguales. ¿Será este el efecto escuela privada? Para 6to de primaria la desigualdad se ha doblado entre ambas zonas. Esto es consistente con la teoría de capital humano propuesta por James Heckman, que sugiere que mientras más pasa el tiempo, más difícil se hace cerrar la brecha entre los ‘mejores’ y ‘peores’ estudiantes. Aquí se nota la gran brecha que existe entre las zonas urbanas y zonas rurales. En la sección de matemáticas (puntuaciones en color azul), la diferencia entre la zona rural y urbana es más pequeña, aunque se nota un patron claro: los estudiantes urbanos son mejores que los rurales. Cualquier persona que <a href="http://linkis.com/www.listindiario.com/AG90F">sabe los pocos recursos que se le asignan a las provincias fuera de Santiago y Santo Domingo</a> no se sorprendería en encontrar estos resultados. No obstante, siempre es bueno confirmar las especulaciones con datos empíricos.</p>
<div id="que-significa-todo-esto" class="section level4">
<h4>¿Que significa todo esto?</h4>
<p>Realmente, toda la información presentada aquí es pura descripción. No podemos concluir que vivir en las zonas rurales o ir a una escuela pública hace que tengas un peor desempeño, por más que quisiera. La ciencia no funciona de esta manera. Aparentemente existen brechas importantes entre las zonas urbanas y rurales, y tal y como sugiere Ceara Hatton, la distribución del presupuesto nacional abarca en su mayoría a Santo Domingo y Santiago. Se podría empezar por priorizar el gasto educativo a los que más lo necesitan, que suelen ser las zonas rurales. Esto tampoco es una solución inmediata; hay que priorizar la calidad del gasto y no el monto.</p>
<p>Por otro lado, podemos ver que los estudiantes pobres si tienen un peor desempeño que los estudiantes ricos, tanto en las escuelas privadas como en las públicas. Sugerir que se debe mejorar la calidad educativa pública ya se sabe, y si esta fuera homogénea a lo largo de las escuelas esperaríamos que los estudiantes ricos tuvieran la misma puntuación que los pobres, algo que no es el caso. Me parece que una de las explicaciones puede estar en el hecho de que los pobres tienen una ‘racha’ o acumulación de malas experiencias educativas a lo largo de la vida. Al momento de llegar a 3ro o 6to de primaria ya están muy por detrás de sus compañeros de mayores recursos. <a href="http://www.diariolibre.com/opinion/dialogo-libre/el-problema-es-que-la-economia-crece-pero-no-se-distribuye-la-riqueza-EE5262978">El acceso a educación temprana, de muy alta calidad, de forma universal, puede ayudar a remediar este problema de inmovilidad social</a>. Esto es algo que muchas personas ya han pedido en otros países y no es nuevo.</p>
<p>Para terminar en una nota buena, no nos debemos sentir tan mal por estos resultados: como dije aquí(poner link <a href="http://www.jorgecimentada.com/?p=137" class="uri">http://www.jorgecimentada.com/?p=137</a>), en comparación con cualquier otro país latinoamericano, tenemos mucha menor desigualdad, lo importante sería asegurarnos que no crezca, porque es muy difícil cerrarlo después.</p>
<hr />
<p>Notas:</p>
<ul>
<li><p>La pobreza en este caso se mide como los estudiantes que vienen de familias donde sus padres no se graduaron de la escuela secundaria. Opuestamente, los ricos son los estudiantes que vienen de familias donde los padres han obtenido un grado universitario o más.</p></li>
<li><p>El promedio de los ‘peores’ estudiantes es el promedio de todos los niños que están en el 25% de la parte inferior de la distribución, mientras que el promedio de los ‘mejores’ estudiantes es el promedio del 25% de la parte superior de la distribución.</p></li>
<li><p>Cualquier persona interesada en reproducir estos análisis, <a href="https://github.com/cimentadaj/blog-articles/tree/master/private_public_schools">aquí</a> está el código</p></li>
</ul>
</div>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>Obtaining robust standard errors and odds ratios for logistic regression in R</title>
      <link>/blog/2016-09-19-obtaining-robust-standard-errors-and-odds-ratios/obtaining-robust-standard-errors-and-odds-ratios-for-logistic-regression-in-r/</link>
      <pubDate>Mon, 19 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-09-19-obtaining-robust-standard-errors-and-odds-ratios/obtaining-robust-standard-errors-and-odds-ratios-for-logistic-regression-in-r/</guid>
      <description><![CDATA[
      


<p>I’ve always found it frustrating how it’s so easy to produce robust standard errors in Stata and in R it’s so complicated. First, we have to estimate the standard errors separately and then replace the previous standard errors with the new ones. Second, if you want to estimate odds ratios instead of logit coefficients, then the robust standard errors need to be scaled. All of that is as simple as adding robust or in the Stata logit command.</p>
<p>I decided to make it as simple in R.</p>
<p>First, I have to give credit to <a href="http://stackoverflow.com/users/4233642/achim-zeileis">Achim Zeileis</a> in this <a href="http://stackoverflow.com/questions/27367974/different-robust-standard-errors-of-logit-regression-in-stata-and-r">question</a> because he provided part of code to generate the robust standard errors.</p>
<p>The function accepts a glm object and can return logit coefficients with robust standard errors, odd ratios with adjusted robust standard errors or probability scaled coefficients with adjusted robust standard errors.</p>
<p>Here’s the function:</p>
<pre class="r"><code># This function estimates robust standad error for glm objects and
# returns coefficients as either logit, odd ratios or probabilities.
# logits are default
# argument x must be glm model.


# Credit to Achim here:
# http://stackoverflow.com/questions/27367974/
# different-robust-standard-errors-of-logit-regression-in-stata-and-r
# for the code in line 14 and 15

robustse &lt;- function(x, coef = c(&quot;logit&quot;, &quot;odd.ratio&quot;, &quot;probs&quot;)) {
suppressMessages(suppressWarnings(library(lmtest)))
suppressMessages(suppressWarnings(library(sandwich)))

    sandwich1 &lt;- function(object, ...) sandwich(object) *
                                       nobs(object) / (nobs(object) - 1)
    # Function calculates SE&#39;s
    mod1 &lt;- coeftest(x, vcov = sandwich1) 
    # apply the function over the variance-covariance matrix
    
    if (coef == &quot;logit&quot;) {
    return(mod1) # return logit with robust SE&#39;s
    } else if (coef == &quot;odd.ratio&quot;) {
    mod1[, 1] &lt;- exp(mod1[, 1]) # return odd ratios with robust SE&#39;s
    mod1[, 2] &lt;- mod1[, 1] * mod1[, 2]
    return(mod1)
    } else {
    mod1[, 1] &lt;- (mod1[, 1]/4) # return probabilites with robust SE&#39;s
    mod1[, 2] &lt;- mod1[, 2]/4
    return(mod1)
    }
}</code></pre>
<p>Now let’s give it a try. Let’s estimate two models, one with logit coefficients and robust SE’s and the same for odds ratios. Just to make sure, let’s compare it with the Stata output.</p>
<pre class="r"><code># In R for logit coefficients and robust standard errors:
suppressMessages(suppressWarnings(library(haven)))

dat &lt;- read_dta(&quot;http://www.stata-press.com/data/r9/quad1.dta&quot;)
mod1 &lt;- glm(z ~ x1 + x2 + x3, dat, family = binomial)
robustse(mod1, coef = &quot;logit&quot;)</code></pre>
<pre><code>## 
## z test of coefficients:
## 
##             Estimate Std. Error z value  Pr(&gt;|z|)    
## (Intercept) 0.091740   0.025869  3.5464 0.0003905 ***
## x1          0.024050   0.025869  0.9297 0.3525395    
## x2          0.157143   0.089715  1.7516 0.0798448 .  
## x3          0.190162   0.089418  2.1267 0.0334490 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code># In Stata for logit coefficients and robust standard errors:
use http://www.stata-press.com/data/r9/quad1.dta, clear
logit z x1 x2 x3, robust

# ------------------------------------------------------------------------------
#              |               Robust
#            z |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
# -------------+----------------------------------------------------------------
#           x1 |     .02405   .0258693     0.93   0.353    -.0266528    .0747529
#           x2 |   .1571428   .0897145     1.75   0.080    -.0186944      .33298
#           x3 |    .190162   .0894185     2.13   0.033      .014905    .3654189
#        _cons |     .09174   .0258686     3.55   0.000     .0410386    .1424415
# ------------------------------------------------------------------------------</code></pre>
<pre class="r"><code># In R for odd ratios with adjusted standard errors
robustse(mod1, coef = &quot;odd.ratio&quot;)</code></pre>
<pre><code>## 
## z test of coefficients:
## 
##             Estimate Std. Error z value  Pr(&gt;|z|)    
## (Intercept) 1.096080   0.028354  3.5464 0.0003905 ***
## x1          1.024342   0.026499  0.9297 0.3525395    
## x2          1.170163   0.104981  1.7516 0.0798448 .  
## x3          1.209445   0.108147  2.1267 0.0334490 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code># In Stata for odd ratios with adjusted standard errors
logit z x1 x2 x3, robust or

# ------------------------------------------------------------------------------
#              |               Robust
#            z | Odds Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
# -------------+----------------------------------------------------------------
#           x1 |   1.024342    .026499     0.93   0.353     .9736992    1.077618
#           x2 |   1.170163   .1049806     1.75   0.080     .9814792    1.395119
#           x3 |   1.209445   .1081467     2.13   0.033     1.015017    1.441118
#        _cons |    1.09608    .028354     3.55   0.000     1.041892    1.153086
# ------------------------------------------------------------------------------</code></pre>
<p>You can also use the <code>stargazer</code> package to produce nicely formatted tables with these new estimates and it should be exactly the same.</p>
<hr />
<p>UPDATE: I’ve included this function my personal package which you can install with <code>devtools::install_github(&quot;cimentadaj/cimentadaj&quot;)</code>. Feel free to make any pull requests in the github repo.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Simulations and model predictions in R</title>
      <link>/blog/2016-09-13-simulations-and-model-predictions-in-r/simulations-and-model-predictions-in-r/</link>
      <pubDate>Tue, 13 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-09-13-simulations-and-model-predictions-in-r/simulations-and-model-predictions-in-r/</guid>
      <description><![CDATA[
      


<p>I was on a flight from Asturias to Barcelona yesterday and I finally had some free time to open Gelman and Hill’s book and submerge in some studying. After finishing the chapter on simulations, I tried doing the first exercise and enjoyed it very much.</p>
<p>The exercise goes as follows:</p>
<p><em>Discrete probability simulation: suppose that a basketball player has a 60% chance of making a shot, and he keeps taking shots until he misses two in a row. Also assume his shots are independent (so that each shot has 60% probability of success, no matter what happened before).</em></p>
<ol style="list-style-type: lower-alpha">
<li><p>Write an R function to simulate this process.</p></li>
<li><p>Put the R function in a loop to simulate the process 1000 times. Use the simulation to estimate the mean, standard deviation, and distribution of the total number of shots that the player will take.</p></li>
<li><p>Using your simulations, make a scatterplot of the number of shots the player will take and the proportion of shots that are successes.</p></li>
</ol>
<p>Below you can find my answer with some comments on how I did it:</p>
<pre class="r"><code># a)
# The probs argument sets the probability of making a shot. In this case it&#39;ll be 0.60
thrower &lt;- function(probs) {
  vec &lt;- replicate(2, rbinom(1, 1, probs)) 
  # create a vector with two random numbers of either 1 or 0,
  # with a probability of probs for 1
  
  # While the sum of the last and the second-last element is not 0
  while ((vec[length(vec)] + vec[length(vec) - 1]) != 0) { 
    
      vec &lt;- c(vec, rbinom(1, 1, probs))
      # keep adding random shots with a probability of probs
  }
return(vec)
}
# The loop works because whenever the sum of the last two elements is 0,
# then the last two elements must be 0 meaning that the player missed two
# shots in a row.</code></pre>
<pre class="r"><code># test
thrower(0.6)</code></pre>
<pre><code>##  [1] 1 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0</code></pre>
<pre class="r"><code># 0 1 0 1 0 0
# Last two elements are always zero</code></pre>
<pre class="r"><code># b)
attempts &lt;- replicate(1000, thrower(0.60))
mean(sapply(attempts, length)) </code></pre>
<pre><code>## [1] 8.501</code></pre>
<pre class="r"><code># mean number of shots until two shots are missed in a row</code></pre>
<pre class="r"><code>sd(sapply(attempts, length)) </code></pre>
<pre><code>## [1] 7.158019</code></pre>
<pre class="r"><code># standard deviation of shots made
# until two shots are missed in a row</code></pre>
<pre class="r"><code>hist(sapply(attempts, length)) # distribution of shots made</code></pre>
<p><img src="/blog/2016-09-13-simulations-and-model-predictions-in-r/2016-09-13-simulations-and-model-predictions-in-r_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code># c)
df &lt;- cbind(sapply(attempts, mean), sapply(attempts, length)) 
# data frame with % of shots made and number of shots thrown
plot(df[, 2], df[, 1])</code></pre>
<p><img src="/blog/2016-09-13-simulations-and-model-predictions-in-r/2016-09-13-simulations-and-model-predictions-in-r_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>That was fun. I think the key take away here is that you can use these type of simulations to asses the accuracy of model predictions, for example. If you have the probability of being in either 1 or 0 in any dependent variable, then simulation can help determine its reliability by looking at the distribution of the replications.</p>
<p>Whenever I have some free time I’ll go back to the next exercises.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Children: public or private goods?</title>
      <link>/blog/2016-08-29-children-public-or-private-goods/children-public-or-private-goods/</link>
      <pubDate>Mon, 29 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-08-29-children-public-or-private-goods/children-public-or-private-goods/</guid>
      <description><![CDATA[
      


<p>The role of the welfare state, as agreed by many scholars, is to de-commodify its citizens. Some argue that this should be the way that the generosity of the state should be measured; instead of net public and private social expenditure, it should be the extent to which citizens are de-commodified by the services and transfers of the welfare state.</p>
<p>The aim of educational investment can be seen from different lenses, but today we’ll focus on the economic and social returns to education. An important share of the state budget is aimed at the formation and education of citizens through public education. In countries where there is a substantial investment, human capital, innovation and production are maximized creating a highly productive working population that can sustain the young and the elderly through pensions, health and child care; note that in some scenarios an uneducated workforce can sustain those who depend on them but it will possible be with more quantity of work (hours) and less quality [1]. Following this logic, it is arguable that because most of today’s workers will benefit from pensions, retirement schemes, care assistance and healthcare, they are to take care of their young population, the next generation’s workers. Covering kids’ education through higher taxes could be seen as loan: A young worker covers the expenses of a child, and that child, eventually, will cover the pension of that worker; this is a simplified version of the argument.</p>
<p>The scheme I just outlined is virtually widespread across western European countries. But it is important to note that in other countries, due to the intrinsic organization of the state, it is unimaginable to do so. In the counterfactual situation where you don’t have a strong welfare state, maybe it’s unfair to tax those who are not benefiting from the goods that the taxes are being spent on. In places like the United States, where the generosity of the welfare system is minimal, some people don’t find it reasonable to pay for everyone’s kids, even more, when you’re paying already, for instance, a private school for your own.</p>
<p>There’s also another important case: childless couples. Perhaps the exact reason why they didn’t have children was not to have to spend a big proportion of their lifelong income. So, why should they pay? According to their position it seems very unfair. Their long held argument can be synthesized in one sentence: those who want children should be the ones who pay for them.</p>
<p>After presenting very briefly the arguments of the positions in the debate, it is interesting then to posit the important question concerning it: Should children be a public or private good? I presume this question is easy to answer but not easy to apply; adjusting a state to this premise can take decades and it’s an expensive endeavor. So, evaluating this question must be accompanied by hard scientific thinking.</p>
<p>I will start off by stating my position on this dilemma. I support the notion of kids being a public good. I support it not just for the fact that they will pay or not for my pension schemes but because everyone deserves the same opportunities. And if we don’t provide them with equal opportunities, then the gap we see between poor and rich will be as it is today: very sizable. Despite that I sometimes question myself: can they entirely be a public good? Adopting the notion that the family is a private sphere, and at least the states considers it because it is reluctant to interfere within it, then children should not just be considered as a public good. If we pay attention to authors like McDonald (2000) and Lewis &amp; Astrom (1992), we constantly see that the way the state promotes gender equality in the family is not through hands-on, direct implementation of intrusive policies like reeducating couples and telling them what’s right and what’s not. Instead, most of the family policies try to promote gender equality by integrating women into the labor force, reducing the gender wage gap and promoting female and male parental leave. Kids then can never be entirely public goods.</p>
<p>Nevertheless, do they contribute to society, meaning, are the interrelated with the “public”?</p>
<p>I think this question is rooted in the intrinsic dynamics of society. We are social animals and it is unfashionable to think that we are independent of each other. The most independent citizen living within society is never unattached of its surroundings. Everyone’s paths cross indirectly and directly with the decisions and actions of other people. It might be even irresponsible to think that one’s actions only affect themselves. The principal idea to take from this question is that those who think of themselves as independent actors of society are committing a fallacy that could be the exact reason why the levels and indexes of inequalities are so high nowadays.</p>
<p>The Nobel laureate James Heckman has been arguing for the past decade one of his most controversial findings: the first years of a child are the most important ones. Contrary to the long held thought of concentrating the bigger part of the educational budget on tertiary education, he finds that the return of investment of human beings are the highest in the early years and it decreases with time. This means that the first years should be the ones where the highest investment should be: be it monetary and emotional. With the evaluation of the Perry preschool program and the Abecedarian project, Heckman finds that you can trace inequalities between children as early as 22 months old. The principal reason being that some kids get better early childhood education than others. Among others, I think this an important reason to reflect why children should be a public good.</p>
<p>Inequalities are being reproduced from an early age, leading to a more polarized society where you have lucky and unlucky kids. Those that receive the better education will at the end, have better jobs, less odds of committing crime, more prone of a stable union (possibly due to the fact that they will have a stable job). Furthermore, and I think this is a key argument against those that don’t want to contribute, these educated children will in most of the cases, help society better than the ones who aren’t educated. This help can be manifested in direct and indirect ways: respecting traffic lights, avoiding corruption in public institutions, being fair, meritocratic, reporting robberies, not stealing, respecting others, among an endless number of thing. This point makes an important argument: the focus on which some of the parents and scholars are viewing the debate might be wrongly specified. As I have succinctly showed, the benefits of having an educated population are not only personal, but also public. I think that the principal reason why there is a strong opposition to this idea is because they evaluate it as taking care of someone else’s children when nobody is taking care of theirs; that is a strong point, but I urge everyone to see it with another lense. If you turned the coin around and you start seeing this as an investment towards societal problems, which in the short and long run affects us all, then it becomes a tax deduction, just as any other one.</p>
<p>As I argued before, we are all interrelated and it is our business what our fellow citizens do. So, someone who doesn’t have children, still participates in society, interacts within it, and is affected by criminality, public services, and private services. The problem with providing this argument for a country with a weak welfare state is that you won’t receive quality elderly care from this children when you retire. Conversely, in strong-welfare countries, the argument is the main (pragmatic) reason to do so. However, the second idea, that we should do it for everyone to have the same opportunities, is more than enough to warrant the changes. This would be an even more noble reason because you won’t be contributing to receive something but because everyone deserves the same opportunities. For the U.S, this is a reason to tackle one of the most debatable topics of the last 40 years: inequalities.</p>
<p>Now, if we were to delve more into the inequality problem, which shows itself to be the major issue when comparing educational opportunities, it is also important to think of the struggles of having children. Not particularly of the difficulties of childrearing, which are a consequence of the decision to child bear, but the fact that it is too hard to combine work and childrearing now a days. This is quite notorious in the lean welfare state countries and southern European ones.</p>
<p>The demands of the work space and the schedules that are imposed to parents and workers make it virtually impossible for men or women to dedicated high quality time to their children. The most common solution that we’ve seen is for women to quit their jobs or to sacrifice leisure time to take care of the child. So, in the end, this deems either the education of the future population or the ambitions and life course perspective of a mother.</p>
<p>Catherine Hakim’s preference theory, posits that women now have the preference of having a child but this was not entirely the case in the past where the bargaining power of the man was higher than hers. But a woman who has professional ambitions might also want to have a baby. Does this mean that she has to renounce her ambitions? This will certainly harm the tax system (less taxes paid) but it will help society because she will dedicate more time to the child (And if you acknowledge this, then you are implicitly arguing that children are indeed a public good). But as one of Esping-Andersen’s chapter from his book the Incomplete Revolution shows, permitting the woman to go back to work after parental leave, by providing free high quality childcare, will in the long run pay the costs of this childcare service and even produce a surplus for the welfare state. And that is why I think this is not an easy question to answer, conversely to what Paula England and Nancy Folbre imply. The implication of implementing this idea entails a complete reorganization and readjustment of the tax and welfare system, if there is one [2]. And also, what would be the consequences of this new regime besides the public childcare? Will the problem of inequalities be solved or others problems will arise as side effects?</p>
<p>And here I will talk about something that is intrinsically related to having healthy children: the labor perspectives of the mother. The Incomplete Revolution, book by Esping-Andersen, takes on the job of explaining the problem with many of today’s countries: the poor family policies that they implement (if any) when concerning women’s labor market inclusion and combination of working and childrearing. Specifically in two chapters he argues that children should be a public good because children benefit everyone, but furthermore, an important step is to include women in the labor market given that it will help the income prospects of the household and the egalitarianism in the family. How can this be related to children as being a public good, one may ask? This is a complex relation, on which everything is potentially endogenous, but all present evidence shows that if the mother goes back to work after the parental leave, it does not have an adverse effect on the development of the child. This is, assuming that the child is in early high quality childcare. This has important implications! It means that the inclusion of mothers into the labor force will increase the bargaining power of the women in the family, it will heighten the income of the family household, it will expand the human capital of the women; in short, it will not impose the opportunity costs that she will otherwise would’ve had to bear for the remaining part of her life if she would’ve stayed home. This will increase the chances of being in a more egalitarian family, thus the children will be exposed to more equal surroundings.</p>
<p>From the point of view that I see it, it’s about heightening the bar of educational attainment. Instead of having the two extremes that we see in the liberal welfare state regimes (excluding UK), extremely poor and uneducated groups and highly rich and educated group, you will set a bar of minimum education. By doing this you will inevitable give secureness to families, opportunities, possibly change the economic structure of the country by increasing the odds of turning it to a knowledge based economy (although this is way more complex then I suggest).</p>
<p>As a conclusion, I shall finish off with another question, which I think is as important as the one which initiated this essay. I have tried to argue that children are faring under the destiny that is imposed to them. There has been, since after the World War II, persistent inequalities between classes and all the evidence shows that they are not shrinking. In some countries, namely European ones, the decision to help children might be an economic one or an altruistic one. This is worrisome. Are children being taken care of in Europe because they are the payers of the pensioners or because of the noble argument of equal opportunities? I really don’t know the reason for which Europe or the U.S is doing it for. But should everyone take care of children for the economic benefits or for the equal opportunity argument?</p>
<hr />
<p>[1] Let’s not get into the major changes that the quality of job production will have if the population is highly educated. For a book with interesting ideas, read The Race between Education and Technology by Claudia Goldin &amp; Lawrence Katz.</p>
<p>[2] In the case where there is no welfare state, then this is another story.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Producing stargazer tables with odds ratios and standard errors in R</title>
      <link>/blog/2016-08-22-producing-stargazer-tables-with-odds-ratios-and-standard-errors-in-r/producing-stargazer-tables-with-odds-ratios-and-standard-errors-in-r/</link>
      <pubDate>Mon, 22 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-08-22-producing-stargazer-tables-with-odds-ratios-and-standard-errors-in-r/producing-stargazer-tables-with-odds-ratios-and-standard-errors-in-r/</guid>
      <description><![CDATA[
      


<p>Whoa, what a day. I’ve been using the stargazer package for producing my (beautiful) regression tables in R for a while now. Among all the arguments of its main function (<code>stargazer()</code> ) are <code>apply.coef</code>, <code>apply.se</code>, <code>apply.ci</code>, … and so on for all the other statistics of a regression output. Each of these arguments, if specified, applies a function over the specified statistic. So, for calculating the odds ratios I would simply apply the <code>exp()</code> function over the set of log odds. It turns out that if you apply any function over the coefficients (or any other statistic), stargazer automatically recalculates t values with the new coefficients! This means that the significance of my model will depend on the new values and we surely wouldn’t want that.</p>
<p>Let’s show a reproducible example:</p>
<pre class="r"><code># install.packages(&quot;stargazer&quot;) # in case you don&#39;t have this package
suppressMessages(library(stargazer))

m1 &lt;- glm(mtcars$vs ~ mtcars$hp + mtcars$mpg)

stargazer(m1, type = &quot;text&quot;) # Our standard log odds</code></pre>
<pre><code>## 
## =============================================
##                       Dependent variable:    
##                   ---------------------------
##                               vs             
## ---------------------------------------------
## hp                         -0.004**          
##                             (0.001)          
##                                              
## mpg                          0.022           
##                             (0.017)          
##                                              
## Constant                     0.566           
##                             (0.519)          
##                                              
## ---------------------------------------------
## Observations                  32             
## Log Likelihood              -11.217          
## Akaike Inf. Crit.           28.434           
## =============================================
## Note:             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<pre class="r"><code>stargazer(m1, apply.coef = exp, type = &quot;text&quot;)</code></pre>
<pre><code>## 
## =============================================
##                       Dependent variable:    
##                   ---------------------------
##                               vs             
## ---------------------------------------------
## hp                         0.996***          
##                             (0.001)          
##                                              
## mpg                        1.022***          
##                             (0.017)          
##                                              
## Constant                   1.762***          
##                             (0.519)          
##                                              
## ---------------------------------------------
## Observations                  32             
## Log Likelihood              -11.217          
## Akaike Inf. Crit.           28.434           
## =============================================
## Note:             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<p>The coefficients are correct, but look at the significance levels! Those are some really undesirable results. I was actually using this for quite some time without noticing. In light of this problem I decided to create a small function that extracted the statistics separately and applied the appropriate conversion when needed. It’s far from being a flexible function, but it can surely help you run some quick-and-dirty logistic regressions with odds ratios instead of log odds.</p>
<p>Here’s the function and an example:</p>
<pre class="r"><code>stargazer2 &lt;- function(model, odd.ratio = F, ...) {
  if(!(&quot;list&quot; %in% class(model))) model &lt;- list(model)
    
  if (odd.ratio) {
    coefOR2 &lt;- lapply(model, function(x) exp(coef(x)))
    seOR2 &lt;- lapply(model, function(x) exp(coef(x)) * summary(x)$coef[, 2])
    p2 &lt;- lapply(model, function(x) summary(x)$coefficients[, 4])
    stargazer(model, coef = coefOR2, se = seOR2, p = p2, ...)
    
  } else {
    stargazer(model, ...)
  }
}

stargazer(m1, type = &quot;text&quot;) # Our standard log odds</code></pre>
<pre><code>## 
## =============================================
##                       Dependent variable:    
##                   ---------------------------
##                               vs             
## ---------------------------------------------
## hp                         -0.004**          
##                             (0.001)          
##                                              
## mpg                          0.022           
##                             (0.017)          
##                                              
## Constant                     0.566           
##                             (0.519)          
##                                              
## ---------------------------------------------
## Observations                  32             
## Log Likelihood              -11.217          
## Akaike Inf. Crit.           28.434           
## =============================================
## Note:             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<pre class="r"><code>stargazer2(m1, odd.ratio = T, type = &quot;text&quot;) </code></pre>
<pre><code>## 
## =============================================
##                       Dependent variable:    
##                   ---------------------------
##                               vs             
## ---------------------------------------------
## hp                          0.996**          
##                             (0.001)          
##                                              
## mpg                          1.022           
##                             (0.017)          
##                                              
## Constant                     1.762           
##                             (0.915)          
##                                              
## ---------------------------------------------
## Observations                  32             
## Log Likelihood              -11.217          
## Akaike Inf. Crit.           28.434           
## =============================================
## Note:             *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<pre class="r"><code># Now the coefficients and significance is correct!</code></pre>
<pre class="r"><code># You can also use lists
m1 &lt;- glm(mtcars$vs ~ mtcars$mpg)
m2 &lt;- glm(mtcars$vs ~ mtcars$mpg + mtcars$hp)
m3 &lt;- glm(mtcars$vs ~ mtcars$mpg + mtcars$hp + mtcars$am)

models &lt;- list(m1, m2, m3)

stargazer(models, type = &quot;text&quot;)</code></pre>
<pre><code>## 
## ===============================================
##                        Dependent variable:     
##                   -----------------------------
##                                vs              
##                      (1)        (2)      (3)   
## -----------------------------------------------
## mpg                0.056***    0.022    0.041* 
##                    (0.011)    (0.017)  (0.022) 
##                                                
## hp                           -0.004**  -0.003* 
##                               (0.001)  (0.002) 
##                                                
## am                                      -0.223 
##                                        (0.173) 
##                                                
## Constant          -0.678***    0.566    0.141  
##                    (0.239)    (0.519)  (0.611) 
##                                                
## -----------------------------------------------
## Observations          32        32        32   
## Log Likelihood     -14.669    -11.217  -10.299 
## Akaike Inf. Crit.   33.338    28.434    28.599 
## ===============================================
## Note:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<pre class="r"><code>stargazer2(models, odd.ratio = T, type = &quot;text&quot;)</code></pre>
<pre><code>## 
## ===============================================
##                        Dependent variable:     
##                   -----------------------------
##                                vs              
##                      (1)        (2)      (3)   
## -----------------------------------------------
## mpg                1.057***    1.022    1.042* 
##                    (0.012)    (0.017)  (0.023) 
##                                                
## hp                            0.996**   0.997* 
##                               (0.001)  (0.002) 
##                                                
## am                                      0.800  
##                                        (0.139) 
##                                                
## Constant           0.508***    1.762    1.151  
##                    (0.121)    (0.915)  (0.703) 
##                                                
## -----------------------------------------------
## Observations          32        32        32   
## Log Likelihood     -14.669    -11.217  -10.299 
## Akaike Inf. Crit.   33.338    28.434    28.599 
## ===============================================
## Note:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01</code></pre>
<pre class="r"><code># Same significance but different coefficients and SE&#39;s</code></pre>
<p>Caveats:</p>
<ul>
<li><p>It only accepts one model or one list containing several models. I did this because I didn’t want to get into distinguishing between several separate models. If you want to improve it, <a href="https://github.com/cimentadaj/cimentadaj/blob/master/R/stargazer2.R">here’s</a> the Github website, submit a pull request!</p></li>
<li><p>It doesn’t calculate confidence intervals as the formula is more complicated and I didn’t need them for now.</p></li>
</ul>
<hr />
<p>Update: I included this function in my personal package which you can install like this:</p>
<pre class="r"><code># install.packages(&quot;devtools&quot;)
devtools::install_github(&quot;cimentadaj/cimentadaj&quot;)</code></pre>
]]>
      </description>
    </item>
    
    <item>
      <title>T-tests, regression (and ANOVA): They&#39;re all the same!</title>
      <link>/blog/2016-08-20-ttests-regression-and-anova-theyre-all-the-same/ttests-regression-and-anova-theyre-all-the-same/</link>
      <pubDate>Sat, 20 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-08-20-ttests-regression-and-anova-theyre-all-the-same/ttests-regression-and-anova-theyre-all-the-same/</guid>
      <description><![CDATA[
      


<p>Upon reading “How Not To Lie with Statistics: Avoiding Common Mistakes in Quantitative Political Science” from Gary King, I stumbled into a section which proved that t-tests, ANOVA and Linear Regression are intimately related, both conceptually and algebraically. As a late-comer in statistics, one usually does not pay attention to these nuances. I decided to make a short simulation in R just to make sure my intuition was right.</p>
<pre class="r"><code>set.seed(1)

income &lt;- sample(1000:5000, 100, replace = T)
gender &lt;- rep(c(1, 0), 50)</code></pre>
<pre class="r"><code>t &lt;- t.test(income ~ gender)
unname(t$estimate[2] - t$estimate[1])</code></pre>
<pre><code>## [1] -30.2</code></pre>
<pre class="r"><code>coef(model &lt;- lm(income ~ gender))</code></pre>
<pre><code>## (Intercept)      gender 
##     2941.66      -30.20</code></pre>
<pre class="r"><code># The ANOVA model is actually computed through the lm
# call but we can use the anova() function to check if
# the differences are significant as well.

anova(model)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: income
##           Df    Sum Sq Mean Sq F value Pr(&gt;F)
## gender     1     22801   22801  0.0181 0.8932
## Residuals 98 123308676 1258252</code></pre>
<p>It’s fun to find out about these things and prove that they make sense. However, the strength of linear models is that you can ‘adjust’ for other important variables and get an adjusted estimated difference. Let’s add another variable called kids with the number of children per person and see how the difference changes.</p>
<pre class="r"><code>kids &lt;- sample(1:4, 100, replace = T)
lm(income ~ gender + kids)</code></pre>
<pre><code>## 
## Call:
## lm(formula = income ~ gender + kids)
## 
## Coefficients:
## (Intercept)       gender         kids  
##     2859.54       -32.89        33.66</code></pre>
<p>Well, we can now say that the difference in income between male and females is of about 200 adjusted for the number of children per person.</p>
]]>
      </description>
    </item>
    
  </channel>
</rss>
