blogdown::count_yaml()
clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
remove_missing()
library(tidyverse)
library(ess)
clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
remove_missing()
install.packages(ess)
install.packages("ess")
install.packages("ess")
clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
remove_missing()
library(tidyverse)
library(ess)
clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
remove_missing()
clean_df <-
ess_rounds(1, "your_email@gmail.com") %>%
recode_missings()
clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
recode_missings()
recode_strings_missing
recode_numeric_missing
?recode_missings
p <- ess_rounds(1, "cimentadaj@gmail.com") %>%
recode_missings(c("Refusal", "No answer"))
ess::show_themes()
show_rounds_country()
show_rounds_country(2)
map(show_rounds(), ~ show_rounds_country(.x)) %>%
reduce(intersect)
another_clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
recode_missings(c("Refusal", "No answer"))
another_clean_df$tvtot
another_clean_df <- recode_numeric_missing(another_clean_df$tvtot, "Don't know")
another_clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
recode_missings(c("Refusal", "No answer"))
another_clean_df$tvtot <-
recode_numeric_missing(
another_clean_df$tvtot,
"Don't know"
)
another_clean_df$tvtot
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
?opts_chunk
blogdown::serve_site()
servr::daemon_stop("4322940224")
blogdown::serve_site()
servr::daemon_stop("4718677824")
system("open .")
blogdown::hugo_build()
servr::daemon_stop("4718677824")
servr::daemon_list()
options(servr.daemon = TRUE, blogdown.author = "John Doe")
?servr::daemon_stop
servr::daemon_stop
install.packages("httpuv")
library(RSelenium)
library(xml2)
library(tidyverse)
the_df <-
as_tibble(set_names(rerun(4, character()),
c("school_name", "complexity", "social_fund", "score_6th")))
remDr <- remoteDriver(remoteServerAddr = "192.168.99.100",
port = 4445L,
browserName = "chrome")
remDr$open()
remDr <- remoteDriver(remoteServerAddr = "192.168.99.100",
port = 4445L,
browserName = "chrome")
remDr$open()
remDr$navigate("https://view-awesome-table.com/-L4lo3r-JA2iaWk1puUT/view")
remDr$screenshot(display = TRUE)
navigate_click <- function() {
webElem <- remDr$findElement(using = "class name",
"google-visualization-table-div-page")
Sys.sleep(0.5)
webElem$clickElement()
remDr$getPageSource()[[1]] %>%
read_xml() %>%
xml_ns_strip() %>%
xml_find_all(xpath = '//td') %>%
xml_text() %>%
set_names(c("school_name", "complexity", "social_fund", "score_6th")) %>%
as.list() %>% as_tibble()
}
blogdown::serve_site()
the_df
remDr <- remoteDriver(remoteServerAddr = "192.168.99.100",
port = 4445L,
browserName = "chrome")
remDr$open()
remDr$navigate("https://view-awesome-table.com/-L4lo3r-JA2iaWk1puUT/view")
navigate_click <- function() {
webElem <- remDr$findElement(using = "class name",
"google-visualization-table-div-page")
Sys.sleep(0.5)
webElem$clickElement()
remDr$getPageSource()[[1]] %>%
read_xml() %>%
xml_ns_strip() %>%
xml_find_all(xpath = '//td') %>%
xml_text() %>%
set_names(c("school_name", "complexity", "social_fund", "score_6th")) %>%
as.list() %>% as_tibble()
}
complete_df <-
map(1:160, ~ navigate_click()) %>%
bind_rows()
the_df
the_df
remDr$navigate("https://view-awesome-table.com/-L4lo3r-JA2iaWk1puUT/view")
webElem <- remDr$findElement(using = "class name",
"google-visualization-table-div-page")
webElem$clickElement()
remDr$getPageSource()[[1]]
remDr$getPageSource()[[1]] %>%
read_xml() %>%
xml_ns_strip() %>%
xml_find_all(xpath = '//td') %>%
xml_text() %>%
set_names(c("school_name", "complexity", "social_fund", "score_6th")) %>%
as.list() %>% as_tibble()
complete_df
blogdown::hugo_build()
blogdown::serve_site()
blogdown::hugo_build()
blogdown::serve_site()
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
grab_date <- function(link) {
link %>%
read_html() %>%
html_node(xpath = "//*[@id='contingutPrincipal']/div[1]/div[3]/p[3]") %>%
xml_text() %>%
str_extract("[0-9]{2,}-[0-9]{2,}-[0-9]{4,}") # grabe the date in dd-mm-yyyy format
}
all_dates <- map_chr(all_links, grab_date)
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
# remove links where it ends with a pdf or where there's 2 links
# 2 links means that the page is not there anymore and they
# have an archived link that downloads a file, so no webpage!
grab_date <- function(link) {
link %>%
read_html() %>%
html_node(xpath = "//*[@id='contingutPrincipal']/div[1]/div[3]/p[3]") %>%
xml_text() %>%
str_extract("[0-9]{2,}-[0-9]{2,}-[0-9]{4,}") # grabe the date in dd-mm-yyyy format
}
all_dates <- map_chr(all_links, grab_date)
all_links
all_links <-
map_chr(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map_chr(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
all_links
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
indp_time
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
grab_date <- function(link) {
link %>%
read_html() %>%
html_node(xpath = "//*[@id='contingutPrincipal']/div[1]/div[3]/p[3]") %>%
xml_text() %>%
str_extract("[0-9]{2,}-[0-9]{2,}-[0-9]{4,}") # grabe the date in dd-mm-yyyy format
}
safe_grab <- safely(grab_date)
all_dates <- map_chr(all_links, safe_grab)
all_dates <- map(all_links, safe_grab)
all_dates[[1]]
all_dates %>% transpose()
indp_time
indp_time %>%
transmute(date,
new_date = ymd(final_date),
aut_c = autonomous_community_percent) %>%
gather(cat, val, -(1:2))
indp_time %>%
transmute(date,
independent = independent_state_percent,
aut_c = autonomous_community_percent) %>%
gather(cat, val, -(1:2))
rdy_data <-
indp_time %>%
transmute(date,
independent = independent_state_percent,
aut_c = autonomous_community_percent) %>%
gather(cat, val, -(1:2))
rdy_data %>%
ggplot(aes(new_date, val, colour = cat)) +
geom_point(alpha = 0.4, size = 1) +
geom_line() +
theme_minimal() +
ylim(c(0, 100))
rdy_data %>%
ggplot(aes(date, val, colour = cat)) +
geom_point(alpha = 0.4, size = 1) +
geom_line() +
theme_minimal() +
ylim(c(0, 100))
getwd()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build
hugo
blogdown::hugo_cmd
blogdown:::find_hugo
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
cimentadaj::my_new_post("Login in, scraping and hidden fields")
all_fields <-
list(
adAS_username = "cimentadaj@gmail.com",
adAS_password = "Lolasouno2",
adAS_i18n_theme = 'en',
adAS_mode = 'authn'
)
login <- "https://sso.upf.edu/CAS/index.php/login?service=https%3A%2F%2Faulaglobal.upf.edu%2Flogin%2Findex.php"
website <- "https://aulaglobal.upf.edu/user/index.php?page=0&perpage=5000&mode=1&accesssince=0&search&roleid=5&contextid=185837&id=9829"
upf <- handle("https://aulaglobal.upf.edu")
library(tidyverse)
library(tidyverse)
library(httr)
library(xml2)
login <- "https://sso.upf.edu/CAS/index.php/login?service=https%3A%2F%2Faulaglobal.upf.edu%2Flogin%2Findex.php"
website <- "https://aulaglobal.upf.edu/user/index.php?page=0&perpage=5000&mode=1&accesssince=0&search&roleid=5&contextid=185837&id=9829"
upf <- handle("https://aulaglobal.upf.edu")
access <- POST(login,
body = list(
adAS_username = 'U116130',
adAS_password = "Lolasouno2",
adAS_i18n_theme = 'en',
adAS_mode = 'authn'),
handle = upf)
emails <- GET(website, handle = upf)
all_emails <-
read_html(emails) %>%
xml_ns_strip() %>%
xml_find_all("//table//a") %>%
as_list() %>%
unlist() %>%
str_subset(".+@upf.edu$")
cat(all_emails, sep = ",")
access <- POST(login,
body = all_fields
handle = upf)
access <- POST(login,
body = all_fields,
handle = upf)
blogdown::hugo_build()
blogdown::serve_site()
blogdown::hugo_build()
blogdown::serve_site()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::serve_site()
blogdown::serve_site()
blogdown::hugo_build()
blogdown::serve_site()
```{r}
library(rvest)
library(tidyverse)
library(tidyverse)
library(stringr)
library(tidytext)
main_url <- "http://transcripts.foreverdreaming.org"
all_pages <- paste0("http://transcripts.foreverdreaming.org/viewforum.php?f=177&start=", seq(0, 200, 25))
characters <- c("ted", "lily", "marshall", "barney", "robin")
episode_getter <- function(link) {
title_reference <-
link %>%
read_html() %>%
html_nodes(".topictitle") # Get the html node name with 'selector gadget'
episode_links <-
title_reference %>%
html_attr("href") %>%
gsub("^.", "", .) %>%
paste0(main_url, .) %>%
setNames(title_reference %>% html_text()) %>%
enframe(name = "episode_name", value = "link")
episode_links
}
all_episodes <- map_df(all_pages, episode_getter) # loop over all seasons and get all episode links
all_episodes$id <- 1:nrow(all_episodes)
all_episodes
episode_fun <- function(file) {
file %>%
read_html() %>%
html_nodes(".postbody") %>%
html_text() %>%
str_split("\n|\t") %>%
.[[1]] %>%
data_frame(text = .) %>%
filter(str_detect(text, ""), # Lots of empty spaces
!str_detect(text, "^\\t"), # Lots of lines with \t to delete
!str_detect(text, "^\\[.*\\]$"), # Text that start with brackets
!str_detect(text, "^\\(.*\\)$"), # Text that starts with parenthesis
str_detect(text, "^*.:"), # I want only lines with start with dialogue (:)
!str_detect(text, "^ad")) # Remove lines that start with ad (for 'ads', the link of google ads)
}
episode_fun(all_episodes$link[15])
all_episodes$text <- map(all_episodes$link, episode_fun)
blogdown::serve_site()
library(RSelenium)
library(xml2)
library(tidyverse)
the_df <-
as_tibble(set_names(rerun(4, character()),
c("school_name", "complexity", "social_fund", "score_6th")))
remDr <- remoteDriver(remoteServerAddr = "192.168.99.100",
port = 4445L,
browserName = "chrome")
remDr$open()
remDr <- remoteDriver(remoteServerAddr = "192.168.99.100",
port = 4445L,
browserName = "chrome")
remDr$open()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::hugo_build()
blogdown::serve_site()
library(DBI)
library(RMySQL)
library(caret)
library(viridis)
library(tidyverse)
con <- dbConnect(MySQL(),
dbname = "bicing", # in "" quotes
host = rstudioapi::askForPassword(), # ip of my server
user = "cimentadaj", # in "" quotes
password = rstudioapi::askForPassword())
con <- dbConnect(MySQL(),
dbname = "bicing", # in "" quotes
host = "82.196.1.229", # ip of my server
user = "cimentadaj", # in "" quotes
password = rstudioapi::askForPassword())
library(DBI)
library(RMySQL)
library(caret)
library(viridis)
library(tidyverse)
con <- dbConnect(MySQL(),
dbname = "bicing", # in "" quotes
host = "82.196.1.229", # ip of my server
user = "cimentadaj", # in "" quotes
password = rstudioapi::askForPassword())
