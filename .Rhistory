mod1 <- glm(empty ~ . + hour:day, data = training, family = "binomial")
pred1 <- predict(mod1, newdata = test, type = 'response')
arm::display(mod1)
empty_bicycle <-
mutate(bicing,
empty = ifelse(bikes == 0, 1, 0),
hour = lubridate::hour(time),
# morning = ifelse(time <= 10, 1, 0),
# day = lubridate::wday(time))
weekend = ifelse(between(day, 6, 7), 1, 0)) %>%
# day = as.character(day)) %>%
select(-(1:4))
training_rows <- createDataPartition(empty_bicycle$empty, 1, p = 0.8)[[1]]
empty_bicycle <-
mutate(bicing,
empty = ifelse(bikes == 0, 1, 0),
hour = lubridate::hour(time),
# morning = ifelse(time <= 10, 1, 0),
# day = lubridate::wday(time))
weekend = ifelse(between(lubridate::wday(time), 6, 7), 1, 0)) %>%
# day = as.character(day)) %>%
select(-(1:4))
training_rows <- createDataPartition(empty_bicycle$empty, 1, p = 0.8)[[1]]
training <- empty_bicycle[training_rows, ]
test <- empty_bicycle[-training_rows, ]
mod1 <- glm(empty ~ . + hour:day, data = training, family = "binomial")
mod1 <- glm(empty ~ ., data = training, family = "binomial")
pred1 <- predict(mod1, newdata = test, type = 'response')
arm::display(pred1)
arm::display(mod1)
empty_bicycle <-
mutate(bicing,
empty = ifelse(bikes == 0, 1, 0),
hour = as.character(lubridate::hour(time)),
# morning = ifelse(time <= 10, 1, 0),
# day = lubridate::wday(time))
weekend = ifelse(between(lubridate::wday(time), 6, 7), 1, 0)) %>%
# day = as.character(day)) %>%
select(-(1:4))
training_rows <- createDataPartition(empty_bicycle$empty, 1, p = 0.8)[[1]]
training <- empty_bicycle[training_rows, ]
test <- empty_bicycle[-training_rows, ]
mod1 <- glm(empty ~ ., data = training, family = "binomial")
pred1 <- predict(mod1, newdata = test, type = 'response')
arm::display(mod1)
hist(pred1)
empty_bicycle <-
mutate(bicing,
empty = ifelse(bikes == 0, 1, 0),
hour = as.character(lubridate::hour(time)),
morning = ifelse(time <= 10, 1, 0),
# day = lubridate::wday(time))
weekend = ifelse(between(lubridate::wday(time), 6, 7), 1, 0)) %>%
# day = as.character(day)) %>%
select(-(1:4))
training_rows <- createDataPartition(empty_bicycle$empty, 1, p = 0.8)[[1]]
training <- empty_bicycle[training_rows, ]
test <- empty_bicycle[-training_rows, ]
mod1 <- glm(empty ~ ., data = training, family = "binomial")
pred1 <- predict(mod1, newdata = test, type = 'response')
hist(pred1)
day = as.character(day)) %>%
training_rows <- createDataPartition(empty_bicycle$empty, 1, p = 0.8)[[1]]
day = as.character(day)) %>%
empty_bicycle <-
mutate(bicing,
empty = ifelse(bikes == 0, 1, 0),
hour = as.character(lubridate::hour(time)),
morning = ifelse(time <= 10, 1, 0),
day = lubridate::wday(time),
weekend = ifelse(between(lubridate::wday(time), 6, 7), 1, 0),
day = as.character(day)) %>%
select(-(1:4))
training_rows <- createDataPartition(empty_bicycle$empty, 1, p = 0.8)[[1]]
training <- empty_bicycle[training_rows, ]
mod1 <- glm(empty ~ ., data = training, family = "binomial")
pred1 <- predict(mod1, newdata = test, type = 'response')
display::arm(mod1)
arm::display(mod1)
mod1 <- glm(empty ~ ., data = training, family = "binomial")
pred1 <- predict(mod1, newdata = test, type = 'response')
empty_bicycle <-
mutate(bicing,
empty = ifelse(bikes == 0, 1, 0),
hour = as.character(lubridate::hour(time)),
morning = ifelse(time <= 10, 1, 0),
day = lubridate::wday(time),
weekend = ifelse(between(day, 6, 7), 1, 0),
day = as.character(day)) %>%
select(-(1:4))
training_rows <- createDataPartition(empty_bicycle$empty, 1, p = 0.8)[[1]]
training <- empty_bicycle[training_rows, ]
training <- empty_bicycle[training_rows, ]
test <- empty_bicycle[-training_rows, ]
mod1 <- glm(empty ~ ., data = training, family = "binomial")
pred1 <- predict(mod1, newdata = test, type = 'response')
hist(pred1)
mod1 <- glm(empty ~ . day:hour, data = training, family = "binomial")
mod1 <- glm(empty ~ ., day:hour, data = training, family = "binomial")
mod1 <- glm(empty ~ . + day:hour, data = training, family = "binomial")
mod1 <- glm(empty ~ . + day:hour, data = training, family = "binomial")
pred1 <- predict(mod1, newdata = test, type = 'response')
hist(pred1)
car::vif(mod1)
?car::vif(mod1)
summary(mod1)
empty_bicycle <-
mutate(bicing,
empty = ifelse(bikes == 0, 1, 0),
hour = as.character(lubridate::hour(time)),
# morning = ifelse(time <= 10, 1, 0),
day = lubridate::wday(time),
# weekend = ifelse(between(day, 6, 7), 1, 0),
day = as.character(day)) %>%
select(-(1:4))
training_rows <- createDataPartition(empty_bicycle$empty, 1, p = 0.8)[[1]]
training <- empty_bicycle[training_rows, ]
test <- empty_bicycle[-training_rows, ]
mod1 <- glm(empty ~ . + day:hour, data = training, family = "binomial")
mod1 <- glm(empty ~ . + day:hour, data = training, family = "binomial")
pred1 <- predict(mod1, newdata = test, type = 'response')
hist(pred1)
summary(mod1)
rbinom(1, prob = pred1)
?confusionMatrix()
?rbinom
rbinom(length(pred1), 1, prob = pred1)
pred_empty <- rbinom(length(pred1), 1, prob = pred1)
confusionMatrix(test$empty, pred_empty)
?confusionMatrix
181 / (563 + 181)
confusionMatrix(test$empty, pred_empty, positive = 1)
confusionMatrix(test$empty, pred_empty, positive = "1")
confusionMatrix(test$empty, pred_empty, positive = "0")
confusionMatrix(test$empty, pred_empty, reference = "1")
confusionMatrix(test$empty, pred_empty, positive = "1")
cimentadaj::my_new_post("ess 0.1.1 is out!")
blogdown::find_tags
blogdown::find_yaml(field = 'tags')
?blogdown::find_yaml
blogdown::count_yaml()
clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
remove_missing()
library(tidyverse)
library(ess)
clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
remove_missing()
install.packages(ess)
install.packages("ess")
install.packages("ess")
clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
remove_missing()
library(tidyverse)
library(ess)
clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
remove_missing()
clean_df <-
ess_rounds(1, "your_email@gmail.com") %>%
recode_missings()
clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
recode_missings()
recode_strings_missing
recode_numeric_missing
?recode_missings
p <- ess_rounds(1, "cimentadaj@gmail.com") %>%
recode_missings(c("Refusal", "No answer"))
ess::show_themes()
show_rounds_country()
show_rounds_country(2)
map(show_rounds(), ~ show_rounds_country(.x)) %>%
reduce(intersect)
another_clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
recode_missings(c("Refusal", "No answer"))
another_clean_df$tvtot
another_clean_df <- recode_numeric_missing(another_clean_df$tvtot, "Don't know")
another_clean_df <-
ess_rounds(1, "cimentadaj@gmail.com") %>%
recode_missings(c("Refusal", "No answer"))
another_clean_df$tvtot <-
recode_numeric_missing(
another_clean_df$tvtot,
"Don't know"
)
another_clean_df$tvtot
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
?opts_chunk
blogdown::serve_site()
servr::daemon_stop("4322940224")
blogdown::serve_site()
servr::daemon_stop("4718677824")
system("open .")
blogdown::hugo_build()
servr::daemon_stop("4718677824")
servr::daemon_list()
options(servr.daemon = TRUE, blogdown.author = "John Doe")
?servr::daemon_stop
servr::daemon_stop
install.packages("httpuv")
library(RSelenium)
library(xml2)
library(tidyverse)
the_df <-
as_tibble(set_names(rerun(4, character()),
c("school_name", "complexity", "social_fund", "score_6th")))
remDr <- remoteDriver(remoteServerAddr = "192.168.99.100",
port = 4445L,
browserName = "chrome")
remDr$open()
remDr <- remoteDriver(remoteServerAddr = "192.168.99.100",
port = 4445L,
browserName = "chrome")
remDr$open()
remDr$navigate("https://view-awesome-table.com/-L4lo3r-JA2iaWk1puUT/view")
remDr$screenshot(display = TRUE)
navigate_click <- function() {
webElem <- remDr$findElement(using = "class name",
"google-visualization-table-div-page")
Sys.sleep(0.5)
webElem$clickElement()
remDr$getPageSource()[[1]] %>%
read_xml() %>%
xml_ns_strip() %>%
xml_find_all(xpath = '//td') %>%
xml_text() %>%
set_names(c("school_name", "complexity", "social_fund", "score_6th")) %>%
as.list() %>% as_tibble()
}
blogdown::serve_site()
the_df
remDr <- remoteDriver(remoteServerAddr = "192.168.99.100",
port = 4445L,
browserName = "chrome")
remDr$open()
remDr$navigate("https://view-awesome-table.com/-L4lo3r-JA2iaWk1puUT/view")
navigate_click <- function() {
webElem <- remDr$findElement(using = "class name",
"google-visualization-table-div-page")
Sys.sleep(0.5)
webElem$clickElement()
remDr$getPageSource()[[1]] %>%
read_xml() %>%
xml_ns_strip() %>%
xml_find_all(xpath = '//td') %>%
xml_text() %>%
set_names(c("school_name", "complexity", "social_fund", "score_6th")) %>%
as.list() %>% as_tibble()
}
complete_df <-
map(1:160, ~ navigate_click()) %>%
bind_rows()
the_df
the_df
remDr$navigate("https://view-awesome-table.com/-L4lo3r-JA2iaWk1puUT/view")
webElem <- remDr$findElement(using = "class name",
"google-visualization-table-div-page")
webElem$clickElement()
remDr$getPageSource()[[1]]
remDr$getPageSource()[[1]] %>%
read_xml() %>%
xml_ns_strip() %>%
xml_find_all(xpath = '//td') %>%
xml_text() %>%
set_names(c("school_name", "complexity", "social_fund", "score_6th")) %>%
as.list() %>% as_tibble()
complete_df
blogdown::hugo_build()
blogdown::serve_site()
blogdown::hugo_build()
blogdown::serve_site()
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
grab_date <- function(link) {
link %>%
read_html() %>%
html_node(xpath = "//*[@id='contingutPrincipal']/div[1]/div[3]/p[3]") %>%
xml_text() %>%
str_extract("[0-9]{2,}-[0-9]{2,}-[0-9]{4,}") # grabe the date in dd-mm-yyyy format
}
all_dates <- map_chr(all_links, grab_date)
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
# remove links where it ends with a pdf or where there's 2 links
# 2 links means that the page is not there anymore and they
# have an archived link that downloads a file, so no webpage!
grab_date <- function(link) {
link %>%
read_html() %>%
html_node(xpath = "//*[@id='contingutPrincipal']/div[1]/div[3]/p[3]") %>%
xml_text() %>%
str_extract("[0-9]{2,}-[0-9]{2,}-[0-9]{4,}") # grabe the date in dd-mm-yyyy format
}
all_dates <- map_chr(all_links, grab_date)
all_links
all_links <-
map_chr(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map_chr(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
all_links
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
indp_time
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
grab_date <- function(link) {
link %>%
read_html() %>%
html_node(xpath = "//*[@id='contingutPrincipal']/div[1]/div[3]/p[3]") %>%
xml_text() %>%
str_extract("[0-9]{2,}-[0-9]{2,}-[0-9]{4,}") # grabe the date in dd-mm-yyyy format
}
safe_grab <- safely(grab_date)
all_dates <- map_chr(all_links, safe_grab)
all_dates <- map(all_links, safe_grab)
all_dates[[1]]
all_dates %>% transpose()
indp_time
indp_time %>%
transmute(date,
new_date = ymd(final_date),
aut_c = autonomous_community_percent) %>%
gather(cat, val, -(1:2))
indp_time %>%
transmute(date,
independent = independent_state_percent,
aut_c = autonomous_community_percent) %>%
gather(cat, val, -(1:2))
rdy_data <-
indp_time %>%
transmute(date,
independent = independent_state_percent,
aut_c = autonomous_community_percent) %>%
gather(cat, val, -(1:2))
rdy_data %>%
ggplot(aes(new_date, val, colour = cat)) +
geom_point(alpha = 0.4, size = 1) +
geom_line() +
theme_minimal() +
ylim(c(0, 100))
rdy_data %>%
ggplot(aes(date, val, colour = cat)) +
geom_point(alpha = 0.4, size = 1) +
geom_line() +
theme_minimal() +
ylim(c(0, 100))
getwd()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build
hugo
blogdown::hugo_cmd
blogdown:::find_hugo
blogdown::hugo_build()
blogdown::hugo_build()
blogdown::hugo_build()
