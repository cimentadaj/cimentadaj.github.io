~ .x(df$y, 5, fill = NA)) %>%
bind_cols(df, .)
list(zoo::rollmean, zoo::rollmedian) %>%
setNames(c("rollmean", "rollmedian")) %>%
map_dfc(~ .x(df$y, 5, fill = NA)) %>%
bind_cols(df, .)
df <-
list(zoo::rollmean, zoo::rollmedian) %>%
setNames(c("rollmean", "rollmedian")) %>%
map_dfc(~ .x(df$y, 5, fill = NA)) %>%
bind_cols(df, .)
df
df <-
list(zoo::rollmean, zoo::rollmedian) %>%
setNames(c("rollmean", "rollmedian")) %>%
map_dfc(~ .x(df$y, 5, fill = NA)) %>%
bind_cols(df, .)
df %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_line(aes(y = rollmedian), colour = "blue") +
geom_line(aes(y = rollmean), colour = "red")
df
df <-
list(zoo::rollmean, zoo::rollmedian) %>%
setNames(c("rollmean", "rollmedian")) %>%
map_dfc(~ .x(df$y, 3, fill = NA)) %>%
bind_cols(df, .)
df %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_line(aes(y = rollmedian), colour = "blue") +
geom_line(aes(y = rollmean), colour = "red")
df
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
# remove links where it ends with a pdf or where there's 2 links
# 2 links means that the page is not there anymore and they
# have an archived link that downloads a file, so no webpage!
grab_date <- function(link) {
link %>%
read_html() %>%
html_node(xpath = "//*[@id='contingutPrincipal']/div[1]/div[3]/p[3]") %>%
xml_text() %>%
str_extract("[0-9]{2,}-[0-9]{2,}-[0-9]{4,}") # grabe the date in dd-mm-yyyy format
}
all_dates <- map_chr(all_links, grab_date)
lookup <- as.character(code_dates)
col_date <- ifelse(lookup == "character(0)", NA, lookup)
# this is slow, but this is very short
rem_dates <- as.character(dmy(all_dates[col_date]))
int_date <- indp_time$date
normal_d <- grepl("[0-9]{4,}$", int_date)
int_date[normal_d] <- as.character(dmy(paste0("1 ", int_date[normal_d])))
other_dates <- int_date[is.na(rem_dates) & grepl("series", int_date)]
int_date[is.na(rem_dates) & grepl("series", int_date)] <-
str_replace(other_dates, "\\sseries(.*)$", "") %>%
gsub("1st", "January", .) %>%
gsub("2nd", "April", .) %>%
gsub("3rd", "August", .) %>%
gsub("4th", "November", .) %>%
paste0(" 1") %>%
ymd() %>%
as.character()
final_date <- unname(ifelse(!is.na(rem_dates), rem_dates, int_date))
rdy_data <-
indp_time %>%
transmute(date,
new_date = ymd(final_date),
independent = independent_state_percent,
aut_c = autonomous_community_percent) %>%
gather(cat, val, -(1:2))
rdy_data %>%
ggplot(aes(new_date, val, colour = cat)) +
geom_point(alpha = 0.4, size = 1) +
geom_line() +
theme_minimal() +
ylim(c(0, 100))
library(prophet)
df <-
rdy_data %>%
spread(cat, val) %>%
select(new_date, independent) %>%
rename(ds = new_date,
y = independent)
ph_df <-
df %>%
filter(ds <= "2008-01-01") %>%
prophet()
ph_df %>%
make_future_dataframe(periods = 40, freq = "month", include_history = TRUE) %>%
predict(ph_df, .) %>%
plot(ph_df, .)
df_roll <-
list(zoo::rollmean, zoo::rollmedian) %>%
setNames(c("rollmean", "rollmedian")) %>%
map_dfc(~ .x(df$y, 3, fill = NA)) %>%
bind_cols(df, .)
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_line(aes(y = rollmedian), colour = "blue") +
geom_line(aes(y = rollmean), colour = "red")
df_roll
df_gather <-
df_roll %>%
gather(roll, values, -(1:2))
df_gather
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_line(data = df_gather, aes(x = ds, y = values, colour = values))
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_line(data = df_gather, aes(x = ds, y = values, colour = roll))
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_smooth()
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_smooth(span = 0.2)
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_smooth(span = 0.5)
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_smooth(span = 0.7)
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_smooth(span = 1)
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_smooth(span = 1.5)
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_smooth(span = 2)
list(zoo::rollmean, zoo::rollmedian) %>%
setNames(c("rollmean", "rollmedian")) %>%
map_dfc(~ .x(df$y, 3, fill = NA)) %>%
bind_cols(df, .)
df_roll <-
list(zoo::rollmean, zoo::rollmedian) %>%
setNames(c("rollmean", "rollmedian")) %>%
map_dfc(~ .x(df$y, 3, fill = NA)) %>%
bind_cols(df, .)
df_gather <-
df_roll %>%
gather(roll, values, -(1:2))
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_line(data = df_gather, aes(x = ds, y = values, colour = roll))
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_line(data = df_gather, aes(x = ds, y = values, colour = roll))
df_roll
df_roll
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
prophet()
roll_ph <-
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
prophet()
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
filter(ds <= "2009-01-01") %>%
prophet()
roll_ph <-
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
filter(ds <= "2009-01-01") %>%
prophet()
roll_ph$growth
roll_ph
roll_ph
roll_ph <-
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
filter(ds <= "2009-01-01") %>%
prophet()
roll_ph %>%
make_future_dataframe(periods = 40, freq = "month", include_history = TRUE)
roll_ph %>%
make_future_dataframe(periods = 40, freq = "month", include_history = TRUE) %>%
predict(roll_ph, .) %>%
plot(roll_ph, .)
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
filter(ds <= "2009-01-01") %>%
prophet(mcmc.samples = 2000)
roll_ph <-
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
filter(ds <= "2009-01-01") %>%
prophet(mcmc.samples = 2000)
roll_ph %>%
make_future_dataframe(periods = 40, freq = "month", include_history = TRUE) %>%
predict(roll_ph, .) %>%
plot(roll_ph, .)
df_roll
df_roll %>% filter(ds <= "2009-01-01")
df_roll %>% filter(ds <= "2009-01-01") %>% arrange(ds)
roll_ph <-
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
filter(ds <= "2012-01-01") %>% # change this to 2009 and the relationship dissapears
prophet(mcmc.samples = 2000)
roll_ph %>%
make_future_dataframe(periods = 40, freq = "month", include_history = TRUE) %>%
predict(roll_ph, .) %>%
plot(roll_ph, .)
ph_df %>%
make_future_dataframe(periods = 40, freq = "month", include_history = TRUE) %>%
predict(ph_df, .) %>%
plot(ph_df, .)
ph_df
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
filter(ds <= "2012-01-01")
df %>%
filter(ds <= "2008-01-01")
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
# remove links where it ends with a pdf or where there's 2 links
# 2 links means that the page is not there anymore and they
# have an archived link that downloads a file, so no webpage!
grab_date <- function(link) {
link %>%
read_html() %>%
html_node(xpath = "//*[@id='contingutPrincipal']/div[1]/div[3]/p[3]") %>%
xml_text() %>%
str_extract("[0-9]{2,}-[0-9]{2,}-[0-9]{4,}") # grabe the date in dd-mm-yyyy format
}
all_dates <- map_chr(all_links, grab_date)
lookup <- as.character(code_dates)
col_date <- ifelse(lookup == "character(0)", NA, lookup)
# this is slow, but this is very short
rem_dates <- as.character(dmy(all_dates[col_date]))
int_date <- indp_time$date
normal_d <- grepl("[0-9]{4,}$", int_date)
int_date[normal_d] <- as.character(dmy(paste0("1 ", int_date[normal_d])))
other_dates <- int_date[is.na(rem_dates) & grepl("series", int_date)]
int_date[is.na(rem_dates) & grepl("series", int_date)] <-
str_replace(other_dates, "\\sseries(.*)$", "") %>%
gsub("1st", "January", .) %>%
gsub("2nd", "April", .) %>%
gsub("3rd", "August", .) %>%
gsub("4th", "November", .) %>%
paste0(" 1") %>%
ymd() %>%
as.character()
final_date <- unname(ifelse(!is.na(rem_dates), rem_dates, int_date))
rdy_data <-
indp_time %>%
transmute(date,
new_date = ymd(final_date),
independent = independent_state_percent,
aut_c = autonomous_community_percent) %>%
gather(cat, val, -(1:2))
rdy_data %>%
ggplot(aes(new_date, val, colour = cat)) +
geom_point(alpha = 0.4, size = 1) +
geom_line() +
theme_minimal() +
ylim(c(0, 100))
df <-
rdy_data %>%
spread(cat, val) %>%
select(new_date, independent) %>%
rename(ds = new_date,
y = independent)
df %>%
filter(ds <= "2008-01-01")
ph_df <-
df %>%
filter(ds <= "2008-01-01") %>%
prophet()
library(prophet)
library(prophet)
df <-
rdy_data %>%
spread(cat, val) %>%
select(new_date, independent) %>%
rename(ds = new_date,
y = independent)
ph_df <-
df %>%
filter(ds <= "2008-01-01") %>%
prophet()
ph_df %>%
make_future_dataframe(periods = 40, freq = "month", include_history = TRUE) %>%
predict(ph_df, .) %>%
plot(ph_df, .)
df_roll <-
list(zoo::rollmean, zoo::rollmedian) %>%
setNames(c("rollmean", "rollmedian")) %>%
map_dfc(~ .x(df$y, 3, fill = NA)) %>%
bind_cols(df, .)
df_gather <-
df_roll %>%
gather(roll, values, -(1:2))
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_line(data = df_gather, aes(x = ds, y = values, colour = roll))
df_roll %>%
ggplot(aes(ds, y)) +
geom_point() +
geom_smooth(span = 2)
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
filter(ds <= "2009-01-01")
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
filter(ds <= "2009-01-01")
roll_ph <-
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
filter(ds <= "2009-01-01") %>% # change this to 2009 and the relationship dissapears
prophet()
roll_ph %>%
make_future_dataframe(periods = 40, freq = "month", include_history = TRUE) %>%
predict(roll_ph, .) %>%
plot(roll_ph, .)
roll_ph %>%
make_future_dataframe(periods = 20, freq = "month", include_history = TRUE) %>%
predict(roll_ph, .) %>%
plot(roll_ph, .)
roll_ph <-
df_roll %>%
select(ds, rollmean) %>%
rename(y = rollmean) %>%
filter(ds <= "2009-01-01") %>% # change this to 2009 and the relationship dissapears
remove_empty_rows() %>%
prophet()
roll_ph %>%
make_future_dataframe(periods = 20, freq = "month", include_history = TRUE) %>%
predict(roll_ph, .) %>%
plot(roll_ph, .)
duplicated
methods(duplicated)
duplicated.data.frame
duplicated.numeric_version
duplicated.default
duplicated
duplicated(1:10)
duplicated(c(1, 1:10)
duplicated(c(1, 1:10))
c(1, 1:10)[duplicated(c(1, 1:10))]
c(1, 1:10)[c(1, 1:10) == c(1, 1:10)[duplicated(c(1, 1:10))]]
janitor::get_dupes
roll_ph <-
df_roll %>%
select(ds, rollmedian) %>%
rename(y = rollmedian) %>%
filter(ds <= "2009-01-01") %>% # change this to 2009 and the relationship dissapears
prophet()
roll_ph %>%
make_future_dataframe(periods = 20, freq = "month", include_history = TRUE) %>%
predict(roll_ph, .) %>%
plot(roll_ph, .)
roll_ph %>%
make_future_dataframe(periods = 40, freq = "month", include_history = TRUE) %>%
predict(roll_ph, .) %>%
plot(roll_ph, .)
634 + 500
(634 + 500) - 900
q <- tribble(
~name, ~g1, ~g2, ~g3,
"t1", 0,  1,  2,
"t1", 2,  2,  2,
"t2", 1,  2,  3,
"t2", 3,  3,  3,
"t4", 4,  4,  4
)
q %>%
rownames_to_column %>%
gather(row, value, -rowname) %>%
spread(rowname, value)
q
q %>%
rownames_to_column %>%
gather(row, value, -rowname)
q %>%
rownames_to_column
q
q %>%
gather(cat, val, -name)
q %>%
gather(cat, val, -name) %>% spread(name, val)
q %>% mutate(n = 1:nrow(.)) %>%
gather(cat, val, -name, -n) %>% spread(name, val)
q
t(Q)
t(q)
as_tibble(t(q))
transpose(q)
as_tibble(transpose(q))
q
t(q)
t(q)[-1]
t(q)[-1, ]
q
q %>% mutate(name = paste0(name, "_", 1:nrow(.)))
q %>% mutate(name = paste0(name, "_", 1:nrow(.))) %>%
gather(cat, val, -name)
q %>% mutate(name = paste0(name, "_", 1:nrow(.))) %>%
gather(cat, val, -name) %>% spread(name, val)
cimentadaj::my_new_post("Scraping at scale: daily scraping to your database")
cimentadaj::my_new_post
blogdown::find_tags()
blogdown::find_tags("R")
blogdown::serve_site()
servr::daemon_stop("4325252736")
system("open .")
blogdown::serve_site()
?blogdown::build_site
blogdown::build_site()
blogdown::hugo_build()
system("open .")
