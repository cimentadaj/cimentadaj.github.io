library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
grab_date <- function(link) {
link %>%
read_html() %>%
html_node(xpath = "//*[@id='contingutPrincipal']/div[1]/div[3]/p[3]") %>%
xml_text() %>%
str_extract("[0-9]{2,}-[0-9]{2,}-[0-9]{4,}") # grabe the date in dd-mm-yyyy format
}
all_dates <- map_chr(all_links, grab_date)
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
example_path <-
read_web %>%
html_nodes(".references-column-width") %>% # go to the references
xml_children() %>% # dive into the referneces
xml_children() %>%
xml_path() %>% # get the xpath
.[[1]] %>% # and extract one sample
str_replace_all("\\[[0-9]\\]$", "") # replace the last [1]
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
# remove links where it ends with a pdf or where there's 2 links
# 2 links means that the page is not there anymore and they
# have an archived link that downloads a file, so no webpage!
grab_date <- function(link) {
link %>%
read_html() %>%
html_node(xpath = "//*[@id='contingutPrincipal']/div[1]/div[3]/p[3]") %>%
xml_text() %>%
str_extract("[0-9]{2,}-[0-9]{2,}-[0-9]{4,}") # grabe the date in dd-mm-yyyy format
}
all_dates <- map_chr(all_links, grab_date)
lookup <- as.character(code_dates)
col_date <- ifelse(lookup == "character(0)", NA, lookup)
# this is slow, but this is very short
rem_dates <- as.character(dmy(all_dates[col_date]))
int_date <- indp_time$date
normal_d <- grepl("[0-9]{4,}$", int_date)
int_date[normal_d] <- as.character(dmy(paste0("1 ", int_date[normal_d])))
other_dates <- int_date[is.na(rem_dates) & grepl("series", int_date)]
library(rvest)
library(janitor)
library(stringr)
library(lubridate)
library(tidyverse)
read_web <- read_html("https://en.wikipedia.org/wiki/Catalan_independence_movement")
indp_time <-
read_web %>%
html_nodes(xpath = '//table') %>%
.[[3]] %>%
html_table() %>%
as_tibble() %>%
clean_names()
indp_time
indp_time
indp_time$date
rdy_data <-
indp_time %>%
transmute(date,
new_date = ymd(final_date),
independent = independent_state_percent,
aut_c = autonomous_community_percent) %>%
gather(cat, val, -(1:2))
lookup <- as.character(code_dates)
col_date <- ifelse(lookup == "character(0)", NA, lookup)
# this is slow, but this is very short
rem_dates <- as.character(dmy(all_dates[col_date]))
code_dates <- str_extract_all(indp_time$date, "\\[[0-9]{3,3}\\]$")
code_dates
end_paths <- unlist(code_dates)
complete_paths <- paste0(example_path, end_paths)
all_nodes <- map(complete_paths, ~ html_node(read_web, xpath =  .x)) %>% setNames(end_paths)
all_nodes
extract_gencat <- function(node) {
int_node <-
node %>%
xml_children() %>%
xml_children() %>%
xml_children()
node_href <- int_node[xml_has_attr(int_node, "rel")]
xml_attr(node_href, "href")
}
all_links <-
map(all_nodes, extract_gencat) %>%
discard(~ any(grepl(".pdf$", .x)) || length(.x) > 1)
all_links
grab_date <- function(link) {
link %>%
read_html() %>%
html_node(xpath = "//*[@id='contingutPrincipal']/div[1]/div[3]/p[3]") %>%
xml_text() %>%
str_extract("[0-9]{2,}-[0-9]{2,}-[0-9]{4,}") # grabe the date in dd-mm-yyyy format
}
all_dates <- map_chr(all_links, grab_date)
all_links
int_date <- indp_time$date
normal_d <- grepl("[0-9]{4,}$", int_date)
normal_d
int_date[normal_d] <- as.character(dmy(paste0("1 ", int_date[normal_d])))
other_dates <- int_date[is.na(rem_dates) & grepl("series", int_date)]
