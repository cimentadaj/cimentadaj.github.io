---
title: A list of must pre-project questions
author: Jorge Cimentada
date: '2018-05-23'
slug: a-list-of-must-pre-project-questions
categories: []
tags: ['machine-learning', 'projects']
comments: no
showcomments: yes
showpagemeta: yes
---



<p>Rumbling through Twitter I found a Jupyter Notebook of Paige Bailey written at the rOpensci unconf about Ethical Machine Learning which you can read <a href="https://github.com/ropenscilabs/proxy-bias-vignette/blob/master/EthicalMachineLearning.ipynb">here</a>. It was very interesting to look at her workflow but even more interesting was the set of questions she asked herself before and during the analysis. I paste them here just to keep them as a reference.</p>
<p><strong>As you design the goal and the purpose of your machine learning product, you must first ask: Who is your audience?</strong></p>
<ul>
<li>Is your product or analysis meant to include all people?</li>
<li>And, if not: is it targeted to an exclusive audience?</li>
<li>Is there a person on your team tasked specifically with identifying and resolving bias and discrimination issues?</li>
</ul>
<p><strong>Once the concept and scope have been defined, it is time to focus on the acquisition, evaluation, and cleaning of data. We have received a single .csv file filled with information on customers from the bankâ€™s manager. Some questions to consider:</strong></p>
<ul>
<li>Did the data come from a system prone to human error?</li>
<li>Is the data current?</li>
<li>What technology facilitated the collection of the data?</li>
<li>Was participation of the data subjects voluntary?</li>
<li>Does the context of the collection match the context of your use?</li>
<li>Was your data collected by people or a system that was operating with quotas or a particular incentive structure?</li>
</ul>
<p><strong>Now that your data has been collected, it would be a great idea to evaluate and describe it:</strong></p>
<ul>
<li>Who is represented in the data?</li>
<li>Who is under-represented or absent from your data?</li>
<li>Can you find additional data, or use statistical methods, to make your data more inclusive?</li>
<li>Was the data collected in an environment where data subjects had meaningful choices?</li>
<li>How does the data reflect the perspective of the institution that collected it?</li>
<li>Were fields within the data inferred or appended beyond what was clear to the data subject?
Would this use of the data surprise the data subjects?</li>
</ul>
<p><strong>The next step would be cleaning the data.</strong></p>
<ul>
<li>Are there any fields that should be eliminated from your data?</li>
<li>Can you use anonymization or pseudonymization techniques to avoid needless evaluation or processing of individual data?</li>
</ul>
<p><strong>Establishing logic for variables</strong></p>
<ul>
<li>Can you describe the logic that connects the variables to the output of your equation?</li>
<li>Do your variables have a causal relationship to the results they predict?</li>
<li>How did you determine what weight to give each variable?</li>
</ul>
<p><strong>Identifying assumptions</strong></p>
<ul>
<li>Will your variables apply equally across race, gender, age, disability, ethnicity, socioeconomic status, education, etc.?</li>
<li>What are you assuming about the kinds of people in your data set?</li>
<li>Would you be comfortable explaining your assumptions to the public?</li>
<li>What assumptions are you relying on to determine the relevant variables and their weights?</li>
</ul>
<p><strong>Defining success</strong>
- What amount and type of error do you expect?
- How will you ensure your system is behaving the way you intend? How reliable is it?</p>
<p><strong>How will you choose your analytical method? For example, predictive analytics, machine learning (supervised, unsupervised), neural networks or deep learning, etc.</strong></p>
<ul>
<li>How much transparency does this method allow your end users and yourself?</li>
<li>Are non-deterministic outcomes acceptable given your legal or ethical obligations around transparency and explainability?</li>
<li>Does your choice of analytical method allow you to sufficiently explain your results?</li>
<li>What particular tasks are associated with the type of analytical method you are using?</li>
</ul>
<p><strong>Tools</strong></p>
<ul>
<li>How could results that look successful still contain bias?</li>
<li>Is there a trustworthy or audited source for the tools you need?</li>
<li>Have the tools you are using been associated with biased products?</li>
<li>Or, if you build from scratch: can you or a third-party test your tools for any features that can result in biased or unfair outcomes?</li>
</ul>
