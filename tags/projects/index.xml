<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>projects on Jorge Cimentada</title>
    <link>https://cimentadaj.github.io/tags/projects/</link>
    <description>Recent content in projects on Jorge Cimentada</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sat, 26 Jan 2019 00:00:00 +0000</lastBuildDate>
    
    <atom:link href="https://cimentadaj.github.io/tags/projects/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Turning a pdf book into machine readable format</title>
      <link>https://cimentadaj.github.io/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/turning-a-pdf-book-into-machine-readable-format/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/turning-a-pdf-book-into-machine-readable-format/</guid>
      <description><![CDATA[
      


<p>A few days ago a well known Sociologist, Erik Olin Wright, died from Leukemia. Torkild Lyngstand then <a href="https://twitter.com/torkildl/status/1088325262758969344">posted on twitter</a> his <a href="https://www.ssc.wisc.edu/~wright/Published%20writing/FallingIntoMarxismChoosingToStay.pdf">‘intellectual biography’</a> which is an interesting document that outlines how he ended up being a Marxist. This document is a pdf book that has two actual book pages per pdf page.</p>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-1-1.png" width="702" /></p>
<p>Although this is perfectly fine for reading on a computer, I usually don’t like to read anything longer than 15 pages on my computer. So I decided I would turn this book into machine readable text with R for my Kindle.</p>
<p>Spoiler: I couldn’t do it, so help me out!</p>
<p>Firs things first. I will use the <code>magick</code> and <code>tabulizer</code> packages. <code>tabulizer</code> has a dependency with <code>rJava</code> which is a bit difficult to handle. I wrote <a href="blog/2018-05-25-installing-rjava-on-windows-10/installing-rjava-on-windows-10/index.html">this blogpost</a> explaining how to install <code>rJava</code> on Windows 10 and it’s helped me inmensely not to waste time in the installation process.</p>
<p>After installing both packages successfully, I loaded them, and split the pdf into separate pages using <code>tabulizer::split_pdf</code>.</p>
<pre class="r"><code>library(magick)
Sys.setenv(JAVA_HOME=&quot;C:/Program Files/Java/jdk-11.0.2/&quot;)
library(tabulizer)

url &lt;- &quot;https://www.ssc.wisc.edu/~wright/Published%20writing/FallingIntoMarxismChoosingToStay.pdf&quot;
all_pages &lt;- tabulizer::split_pdf(url)

all_pages</code></pre>
<pre><code>##  [1] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce401.pdf&quot;
##  [2] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce402.pdf&quot;
##  [3] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce403.pdf&quot;
##  [4] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce404.pdf&quot;
##  [5] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce405.pdf&quot;
##  [6] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce406.pdf&quot;
##  [7] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce407.pdf&quot;
##  [8] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce408.pdf&quot;
##  [9] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce409.pdf&quot;
## [10] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce410.pdf&quot;
## [11] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce411.pdf&quot;
## [12] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce412.pdf&quot;
## [13] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce413.pdf&quot;
## [14] &quot;C:\\Users\\Cimentadaj\\AppData\\Local\\Temp\\RtmpYnb7KJ\\file1ba438de3ce414.pdf&quot;</code></pre>
<p><code>tabulizer::split_df</code> saved each page on a separate pdf in a temporary directory. Now we only have to develop a function to clean one page and apply it to all middle pages (that is, excluding the first and last because they have a slightly different format).</p>
<p>After hard work, I developed the function <code>convert_page</code> which accepts one pdf page crops all the corners so that only text is available.</p>
<pre class="r"><code>convert_page &lt;- function(page) {
  page &lt;- magick::image_read_pdf(page)
  separator &lt;- image_info(page)$width / 2
  first_page &lt;- image_crop(page, geometry_area(width = separator))
  second_page &lt;- image_crop(page, geometry_area(x_off = separator, y_off = 1))
  
  size &lt;- geometry_area(width = 1400,
                        height = 2200,
                        x_off = 300,
                        y_off = 200)
  
  first_page &lt;- image_crop(first_page, size)
  
  
  size &lt;- geometry_area(width = 1400,
                        height = 2200,
                        x_off = 130,
                        y_off = 200)
  
  second_page &lt;- image_crop(second_page, size)
  
  f_text &lt;- image_ocr(first_page)
  s_text &lt;- image_ocr(second_page)
  
  complete_page &lt;- paste0(f_text, s_text)
  
  complete_page
}</code></pre>
<p>Let’s look at an actual example. Below is a picture of page 4:</p>
<pre class="r"><code>page_four &lt;- magick::image_read_pdf(all_pages[4])
image_resize(page_four, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-5-1.png" width="702" /></p>
<p><code>convert_page</code> crops the sides to obtain the leftmost page:</p>
<pre class="r"><code>separator &lt;- image_info(page_four)$width / 2
first_page &lt;- image_crop(page_four, geometry_area(width = separator))
second_page &lt;- image_crop(page_four, geometry_area(x_off = separator, y_off = 1))

size &lt;- geometry_area(width = 1400,
                      height = 2200,
                      x_off = 300,
                      y_off = 200)

first_page &lt;- image_crop(first_page, size)


size &lt;- geometry_area(width = 1400,
                      height = 2200,
                      x_off = 130,
                      y_off = 200)

second_page &lt;- image_crop(second_page, size)

image_resize(first_page, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-6-1.png" width="280" /></p>
<p>And for the rightmost page:</p>
<pre class="r"><code>image_resize(second_page, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-7-1.png" width="280" /></p>
<p>Finally, it converts and merges both pages into text with:</p>
<pre class="r"><code>f_text &lt;- image_ocr(first_page)
s_text &lt;- image_ocr(second_page)

complete_page &lt;- paste0(f_text, s_text)

cat(complete_page)</code></pre>
<pre><code>## thusiastic and involved in their children’s school projects and intellectual pur-
## suits. My mother would carefully go over term papers with each of us, giving us
## both editorial advice and substantive suggestions. We were members of the Law-
## rence Unitarian Fellowship, which was made up of, to a substantial extent, uni-
## versity families. Sunday morning services were basically interdisciplinary semi-
## nars on matters of philosophical and social concern; Sunday school was an
## extended curriculum on world religions. | knew by about age ten that | wanted
## to be a professor. Both of my parents were academics. Both of my siblings be-
## came academics. Both of their spouses are academics. (Only my wife, a clinical
## psychologist, is not an academic, although her father was a professor.) ‘The only
## social mobility in my family was interdepartmental. It just felt natural to go into
## the family business.
## 
## Lawrence was a delightful, easy place to grow up. Although Kansas was a po-
## litically conservative state, Lawrence was a vibrant, liberal community. My ear-
## liest form of political activism centered on religion: | was an active member of a
## Unitarian youth group called Liberal Religious Youth, and in high school { went
## out of my way to argue with Bible Belt Christians about their belief in God. The
## early 1960s also witnessed my earliest engagement with social activism. The civil
## rights movement came to Lawrence first in the form of an organized boycott of
## a local segregated swimming pool in the 1950s and then in the form of civil rights
## rallies in the 1960s. In 1963 I went to the Civil Rights March on Washington and
## heard Martin Luther King Jr’s “I have a dream” speech. My earliest sense of pol-
## itics was that at its core it was about moral questions of social justice, not prob-
## lems of economic power and interests.
## 
## My family, also, was liberal, supporting the civil rights movement and other
## liberal causes; but while the family culture encouraged an intellectual interest in
## social and moral concerns, it was not intensely political. We would often talk
## about values, and the Unitarian Fellowship we attended also stressed humanis-
## tic, socially concerned values, but these were mostly framed as matters of indi-
## vidual responsibility and morality not as the grounding of a coherent political
## challenge to social injustice. My only real exposure toa more radical political per-
## spective came through my maternal grandparents, Russian Jewish immigrants
## who had come to the United States before World War I and lived near us in Law-
## rence, and my mother’s sister’s family in New York. Although I was not aware of
## this at the time, my grandparents and the New York relatives were Communists.
## This was never openly talked about, but from time to time I would hear glowing
## things said about the Soviet Union, socialism would be held out as an ideal, and
## America and capitalism would be criticized in emotionally laden ways. My cous-
## ins in New York were especially vocal about this, and in the mid-1g60s when I be-
## came more engaged in political matters, intense political discussions with my
## New York relatives contributed significantly to anchoring my radical sensibilities.
## 
## My interest in social sciences began in earnest in high school. fn Lawrence it
## was easy for academically oriented kids to take courses at the University of Kan-
## sas, and in my senior year | took a political science course on American politics.
## For my term project | decided to do a survey of children’s attitudes toward the
## American presidency and got permission to administer a questionnaire to several
## hundred students from grades 1-12 in the public schools. | then organized a party
## with my friends to code the data and produce graphs of how various attitudes
## changed by age. I&#39;he most striking finding was that, in response to the question,
## “Would you like to be President of the United States when you grow up?” there
## were more girls who said yes than boys through third grade, after which the rate
## for girls declined dramatically.
## 
## By the time I graduated from high school in 1964, I had enough university
## credits and advanced placement credits to enter KU as a second-semester soph-
## omore, and that is what | had planned to do. Nearly all of my friends were going
## to KU. It just seemed like the thing to do. A friend of my parents, Karl Heider,
## gave me, as a Christmas present in my senior year in high school, an application
## form to Harvard. He was a graduate student at Harvard in anthropology at the
## time. I filled it out and sent it in. Harvard was the only place to which I applied,
## not out of inflated self-confidence but because it was the only application I got as
## a Christmas present. When | eventually was accepted (initially I was on the wait-
## ing list), the choice was thus between KU and Harvard. I suppose this was a
## “choice” since I could have decided to stay at KU. However, it just seemed so ob-
## vious; there was no angst, no weighing of alternatives, no thinking about the pros
## and cons. Thus, going to Harvard in a way just happened.
## 
## Like many students who began university in the mid-1960s, my political ideas
## were rapidly radicalized as the Viet Nam War escalated and began to impinge on
## our lives. I was not a student leader in activist politics, but I did actively partici-
## pate in demonstrations, rallies, fasts for peace, and endless political debate. At
## Harvard I majored in social studies, an intense interdisciplinary social science
## major centering on the classics of social theory, and in that program I was first ex-
## posed to the more abstract theoretical issues that bore on the political concerns
## of the day: the dynamics of capitalism, the nature of power and domination, the
## importance of elites in shaping American foreign policy, and the problem of</code></pre>
<p>If we pass the pdf page directly to <code>convert_page</code>, it will do it all in one take:</p>
<pre class="r"><code>cat(convert_page(all_pages[4]))</code></pre>
<pre><code>## thusiastic and involved in their children’s school projects and intellectual pur-
## suits. My mother would carefully go over term papers with each of us, giving us
## both editorial advice and substantive suggestions. We were members of the Law-
## rence Unitarian Fellowship, which was made up of, to a substantial extent, uni-
## versity families. Sunday morning services were basically interdisciplinary semi-
## nars on matters of philosophical and social concern; Sunday school was an
## extended curriculum on world religions. | knew by about age ten that | wanted
## to be a professor. Both of my parents were academics. Both of my siblings be-
## came academics. Both of their spouses are academics. (Only my wife, a clinical
## psychologist, is not an academic, although her father was a professor.) ‘The only
## social mobility in my family was interdepartmental. It just felt natural to go into
## the family business.
## 
## Lawrence was a delightful, easy place to grow up. Although Kansas was a po-
## litically conservative state, Lawrence was a vibrant, liberal community. My ear-
## liest form of political activism centered on religion: | was an active member of a
## Unitarian youth group called Liberal Religious Youth, and in high school { went
## out of my way to argue with Bible Belt Christians about their belief in God. The
## early 1960s also witnessed my earliest engagement with social activism. The civil
## rights movement came to Lawrence first in the form of an organized boycott of
## a local segregated swimming pool in the 1950s and then in the form of civil rights
## rallies in the 1960s. In 1963 I went to the Civil Rights March on Washington and
## heard Martin Luther King Jr’s “I have a dream” speech. My earliest sense of pol-
## itics was that at its core it was about moral questions of social justice, not prob-
## lems of economic power and interests.
## 
## My family, also, was liberal, supporting the civil rights movement and other
## liberal causes; but while the family culture encouraged an intellectual interest in
## social and moral concerns, it was not intensely political. We would often talk
## about values, and the Unitarian Fellowship we attended also stressed humanis-
## tic, socially concerned values, but these were mostly framed as matters of indi-
## vidual responsibility and morality not as the grounding of a coherent political
## challenge to social injustice. My only real exposure toa more radical political per-
## spective came through my maternal grandparents, Russian Jewish immigrants
## who had come to the United States before World War I and lived near us in Law-
## rence, and my mother’s sister’s family in New York. Although I was not aware of
## this at the time, my grandparents and the New York relatives were Communists.
## This was never openly talked about, but from time to time I would hear glowing
## things said about the Soviet Union, socialism would be held out as an ideal, and
## America and capitalism would be criticized in emotionally laden ways. My cous-
## ins in New York were especially vocal about this, and in the mid-1g60s when I be-
## came more engaged in political matters, intense political discussions with my
## New York relatives contributed significantly to anchoring my radical sensibilities.
## 
## My interest in social sciences began in earnest in high school. fn Lawrence it
## was easy for academically oriented kids to take courses at the University of Kan-
## sas, and in my senior year | took a political science course on American politics.
## For my term project | decided to do a survey of children’s attitudes toward the
## American presidency and got permission to administer a questionnaire to several
## hundred students from grades 1-12 in the public schools. | then organized a party
## with my friends to code the data and produce graphs of how various attitudes
## changed by age. I&#39;he most striking finding was that, in response to the question,
## “Would you like to be President of the United States when you grow up?” there
## were more girls who said yes than boys through third grade, after which the rate
## for girls declined dramatically.
## 
## By the time I graduated from high school in 1964, I had enough university
## credits and advanced placement credits to enter KU as a second-semester soph-
## omore, and that is what | had planned to do. Nearly all of my friends were going
## to KU. It just seemed like the thing to do. A friend of my parents, Karl Heider,
## gave me, as a Christmas present in my senior year in high school, an application
## form to Harvard. He was a graduate student at Harvard in anthropology at the
## time. I filled it out and sent it in. Harvard was the only place to which I applied,
## not out of inflated self-confidence but because it was the only application I got as
## a Christmas present. When | eventually was accepted (initially I was on the wait-
## ing list), the choice was thus between KU and Harvard. I suppose this was a
## “choice” since I could have decided to stay at KU. However, it just seemed so ob-
## vious; there was no angst, no weighing of alternatives, no thinking about the pros
## and cons. Thus, going to Harvard in a way just happened.
## 
## Like many students who began university in the mid-1960s, my political ideas
## were rapidly radicalized as the Viet Nam War escalated and began to impinge on
## our lives. I was not a student leader in activist politics, but I did actively partici-
## pate in demonstrations, rallies, fasts for peace, and endless political debate. At
## Harvard I majored in social studies, an intense interdisciplinary social science
## major centering on the classics of social theory, and in that program I was first ex-
## posed to the more abstract theoretical issues that bore on the political concerns
## of the day: the dynamics of capitalism, the nature of power and domination, the
## importance of elites in shaping American foreign policy, and the problem of</code></pre>
<p>We pass all middle pages to <code>convert_page</code> to convert them to text:</p>
<pre class="r"><code>middle_pages &lt;- lapply(all_pages[3:(length(all_pages) - 1)], convert_page)
cat(middle_pages[[1]])</code></pre>
<pre><code>## versity of Western Australia); music camp (1 played viola); assisting in a lab. And
## in college, it was much the same: volunteering as a photographer on an archae-
## ological dig in Hawaii; teaching in a high school enrichment program for mi-
## nority kids; traveling in urope. The closest thing to an ordinary paying job |
## ever had was occasionally selling hot dogs at football games in my freshman year
## in college. What is more, the ivory towers that [ have inhabited since the mid-
## 1960s have been located in beautiful physical settings, filled with congenial and
## interesting colleagues and students, and animated by exciting ideas. This, then,
## is the first fundamental fact of my life as an academic: [ have been extraordinar-
## ily lucky and have always lived what can only be considered a life of extreme priv-
## ilege. Nearly all of the time [ am doing what [ want to do; what I do gives me a
## sense of fulfillment and purpose; and | am paid well for doing it.
## 
## Here is the second fundamental fact of my academic life: since the early
## 19708, my intellectual life has been firmly anchored in the Marxist tradition. The
## core of my teaching as a professor has centered on communicating the central
## ideas and debates of contemporary Marxism and allied traditions of emancipa-
## tory social theory. The courses I have taught have had names like Class, State and
## Ideology: An Introduction to Marxist Sociology; Envisioning Real Utopias; Mars-
## ist Theories of the State; Alternative Foundations of Class Analysis. My energies
## in institution building have all involved creating and expanding arenas within
## which radical system-challenging ideas could flourish: creating a graduate pro-
## gram in class analysis and historical change in the Sociology Department at the
## University of Wisconsin—Madison; establishing the A. E. Havens Center, a re-
## search institute for critical scholarship at Wisconsin; organizing an annual con-
## ference for activists and academics, now called RadFest, which has been held
## every year since 1983. And my scholarship has been primarily devoted to recon-
## structing Marxism as a theoretical framework and research tradition. While the
## substantive preoccupations of this scholarship have shifted over the past thirty
## years, its central mission has not.
## 
## As in any biography, this pair of facts is the result of a trajectory of circum-
## stances and choices: circumstances that formed me and shaped the range of
## choices I encountered, and choices that in turn shaped my future circumstances.
## Some of these choices were made easily, with relatively little weighing of alter-
## natives, sometimes even without much awareness that a choice was actually be-
## ing made; others were the result of protracted reflection and conscious decision
## making, sometimes with the explicit understanding that the choice being made
## would constrain possible choices in the future. Six such junctures of circum-
## stance and choice seem especially important to me in shaping the contours of
## my academic career. ‘The first was posed incrementally in the early 1970s: the
## choice to identify my work primarily as contributing to Marxism rather than
## simply using Marxism. The second concerns the choice, made just before grad-
## uate school at the University of California, Berkeley, to be a sociologist, rather
## than some other ist. ‘The third was the choice to become what some people de-
## scribe as multivariate Marxist: to be a Marxist sociologist who engages in grandi-
## ose, perhaps overblown, quantitative research, The fourth choice was the choice
## of which academic department to be in. This choice was acutely posed to me
## in 1987 when I spent a year as a visiting professor at the University of Califor-
## nia, Berkeley. | had been offered a position there, and | had to decide whether
## I wanted to return to Wisconsin. Returning to Madison was unquestionably a
## choice that shaped subsequent contexts of choice. The fifth choice has been
## posed and reposed to me with increasing intensity since the late 1980s: the
## choice to stay a Marxist in this world of post-Marxisms when many of my intel-
## lectual comrades have decided for various good, and sometimes perhaps not so
## good, reasons to recast their intellectual agenda as being perhaps friendly to, but
## outside of, the Marxist tradition. Finally, the sixth important choice was to shift
## my central academic work from the study of class structure to the problem of en-
## visioning real utopias.
## 
## To set the stage for this reflection on choice and constraint, I need to give a
## brief account of the circumstances of my life that brought me into the arena of
## these choices.
## 
## Growing Up
## 
## I was born in Berkeley, California, in 1947 while my father, who had received a
## PhD in psychology before World War II, was in medical school on the GI Bill.
## When he finished his medical training in 1951, we moved to Lawrence, Kansas,
## where he became the head of the program in clinical psychology at Kansas Uni-
## versity (KU) and a professor of psychiatry in the KU Medical School. Because of
## antinepotism rules at the time, my mother, who also had a PhD in psychology,
## was not allowed to be employed at the university, so throughout the 1950s she did
## research on various research grants. In 1961, when the state law on such things
## changed, she became a professor of rehabilitation psychology.
## 
## Life in my family was intensely intellectual. Dinner table conversation would
## often revolve around intellectual matters, and my parents were always deeply en-</code></pre>
<p>Ok, everything’s looking good. Because the first and last pages have different croping dimensions, I slightly adapt the <code>geometry_area</code> to do it manually:</p>
<pre class="r"><code>### First page
first_page &lt;- magick::image_read_pdf(all_pages[2])
image_resize(first_page, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-11-1.png" width="702" /></p>
<pre class="r"><code>separator &lt;- image_info(first_page)$width / 2

size &lt;- geometry_area(width = 1400,
                      height = 1700,
                      x_off = separator + 100,
                      y_off = 650)

first_page &lt;- image_crop(first_page, size)
image_resize(first_page, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-11-2.png" width="280" /></p>
<pre class="r"><code>first_page &lt;- image_ocr(first_page)
###


### Last page
last_page &lt;- magick::image_read_pdf(all_pages[14])
image_resize(last_page, geometry_size_percent(width = 40))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-11-3.png" width="702" /></p>
<pre class="r"><code>separator &lt;- image_info(last_page)$width / 2

size &lt;- geometry_area(width = separator - 400,
                      height = 500,
                      x_off = 150,
                      y_off = 260)

last_page &lt;- image_crop(last_page, size)
image_resize(last_page, geometry_size_percent(width = 70))</code></pre>
<p><img src="/blog/2019-01-26-turning-a-pdf-book-into-machine-readable-format/2019-01-26-turning-a-pdf-book-into-machine-readable-format_files/figure-html/unnamed-chunk-11-4.png" width="474" /></p>
<pre class="r"><code>last_page &lt;- image_ocr(last_page)
###</code></pre>
<p>Ok, the hard work is over! Now we need to merge all of the pages together and print a subset of the text:</p>
<pre class="r"><code>final_document &lt;- paste0(first_page, Reduce(paste0, middle_pages), last_page)
cat(paste0(substring(final_document, 0, 5000), &quot;...&quot;))</code></pre>
<pre><code>## Falling into Marxism; Choosing to Stay
## 
## Erik Olin Wright received his PhD from the University of California, Berkeley, and
## has taught at the University of Wisconsin since then. His academic work has been
## centrally concerned with reconstructing the Marxist tradition of social theory and
## research in ways that attempt to make it more relevant to contemporary concerns
## and more cogent as a scientific framework of analysis. His empirical research has
## focused especially on the changing character of class relations in developed capi-
## talist societies. Since 1992 he has directed the Real Utopias Project, which explores
## a range of proposals for new institutional designs that embody emancipatory ideals
## and yet are attentive to issues of pragmatic feasibility. His principle publications
## include The Politics of Punishment: A Critical Analysis of Prisons in America;
## Class, Crisis and the State; Classes; Reconstructing Marxism (with Elliott Sober
## and Andrew Levine); Interrogating Inequality; Class Counts: Comparative Stud-
## ies in Class Analysis; and Deepening Democracy: Innovations in Empowered
## Participatory Governance (with Archon Fung). He is married to Marcia Kahn
## Wright, a clinical psychologist working in community mental health, and has tvo
## grown daughters, Jennifer and Rebecca.
## 
## [ have been in school continuously for more than fifty vears: since I entered
## kindergarten in 1952, there has never been a September when I wasn’t beginning
## a school year. | have never held a nine-to-five job with fixed hours and a boss
## telling me what to do. In high school, my summers were always spent in vari-
## ous kinds of interesting and engaging activities — traveling home from Australia
## where my family spent a year (my parents were Fulbright professors at the Uni-
## versity of Western Australia); music camp (1 played viola); assisting in a lab. And
## in college, it was much the same: volunteering as a photographer on an archae-
## ological dig in Hawaii; teaching in a high school enrichment program for mi-
## nority kids; traveling in urope. The closest thing to an ordinary paying job |
## ever had was occasionally selling hot dogs at football games in my freshman year
## in college. What is more, the ivory towers that [ have inhabited since the mid-
## 1960s have been located in beautiful physical settings, filled with congenial and
## interesting colleagues and students, and animated by exciting ideas. This, then,
## is the first fundamental fact of my life as an academic: [ have been extraordinar-
## ily lucky and have always lived what can only be considered a life of extreme priv-
## ilege. Nearly all of the time [ am doing what [ want to do; what I do gives me a
## sense of fulfillment and purpose; and | am paid well for doing it.
## 
## Here is the second fundamental fact of my academic life: since the early
## 19708, my intellectual life has been firmly anchored in the Marxist tradition. The
## core of my teaching as a professor has centered on communicating the central
## ideas and debates of contemporary Marxism and allied traditions of emancipa-
## tory social theory. The courses I have taught have had names like Class, State and
## Ideology: An Introduction to Marxist Sociology; Envisioning Real Utopias; Mars-
## ist Theories of the State; Alternative Foundations of Class Analysis. My energies
## in institution building have all involved creating and expanding arenas within
## which radical system-challenging ideas could flourish: creating a graduate pro-
## gram in class analysis and historical change in the Sociology Department at the
## University of Wisconsin—Madison; establishing the A. E. Havens Center, a re-
## search institute for critical scholarship at Wisconsin; organizing an annual con-
## ference for activists and academics, now called RadFest, which has been held
## every year since 1983. And my scholarship has been primarily devoted to recon-
## structing Marxism as a theoretical framework and research tradition. While the
## substantive preoccupations of this scholarship have shifted over the past thirty
## years, its central mission has not.
## 
## As in any biography, this pair of facts is the result of a trajectory of circum-
## stances and choices: circumstances that formed me and shaped the range of
## choices I encountered, and choices that in turn shaped my future circumstances.
## Some of these choices were made easily, with relatively little weighing of alter-
## natives, sometimes even without much awareness that a choice was actually be-
## ing made; others were the result of protracted reflection and conscious decision
## making, sometimes with the explicit understanding that the choice being made
## would constrain possible choices in the future. Six such junctures of circum-
## stance and choice seem especially important to me in shaping the contours of
## my academic career. ‘The first was posed incrementally in the early 1970s: the
## choice to identify my work primarily as contributing to Marxism rather than
## simply using Marxism. The second concerns the choice, made just before grad-
## uate school at the University ...</code></pre>
<p>There we go, nicely formatted text all obtained from pdf images (after carefully revising the text there are many mistakes, but this was a lightning post, so no time to tidy up the text).</p>
<div id="converting-the-text-to-an-epub" class="section level3">
<h3>Converting the text to an epub</h3>
<p>I thought this was going to be much easier, but <code>knitr</code> seems to crash when compiling this text. According to <a href="https://bookdown.org/yihui/bookdown/build-the-book.html">bookdown</a>, I would need a <code>.Rmd</code> file and then use <code>bookdown::render_book(&quot;my_book.Rmd&quot;, bookdown::epub_book())</code>. However, I cannot compile the <code>.Rmd</code> file using this text because it runs out of memory. Run the example below:</p>
<pre class="r"><code>rmd_path &lt;- tempfile(pattern = &#39;our_book&#39;, fileext = &quot;.Rmd&quot;)

rmd_preamble &lt;-&quot;---
  title: &#39;Final Book&#39;
  output: html_document
---\n\n&quot;

final_document &lt;- paste0(rmd_preamble, final_document)
  
writeLines(final_document, con = rmd_path, useBytes = TRUE)

# Bookdown compiles all .Rmd in the working directory, so we move
# to the temporary directory where the book is
setwd(dirname(rmd_path))
bookdown::render_book(rmd_path, bookdown::epub_book())</code></pre>
<p>If you figure out how make to this work, I’d love to hear about it in the comment section.</p>
<p>EDIT:</p>
<p>Thanks to the <a href="https://twitter.com/leonawicz/status/1089537068550651907">tweet by Matthew Leonawicz</a> I managed to do it!</p>
<pre class="r"><code>txt_path &lt;- tempfile(pattern = &#39;our_book&#39;, fileext = &quot;.txt&quot;)

writeLines(final_document, con = txt_path, useBytes = TRUE)

# First download Calibre
path &lt;- paste0(Sys.getenv(&quot;PATH&quot;), &quot;;&quot;, &quot;C:\\Program Files\\Calibre2&quot;)
Sys.setenv(PATH = path)
bookdown::calibre(txt_path, paste0(dirname(txt_path), &quot;/erik_wright.mobi&quot;))</code></pre>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>Some guides and pre-project documents on ML</title>
      <link>https://cimentadaj.github.io/blog/2019-01-22-some-guides-and-preproject-documents-on-ml/some-guides-and-pre-project-documents-on-ml/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2019-01-22-some-guides-and-preproject-documents-on-ml/some-guides-and-pre-project-documents-on-ml/</guid>
      <description><![CDATA[
      


<p>I was just browsing the web and found <a href="https://developers.google.com/machine-learning/guides/rules-of-ml/">this cool resource on ML from Google</a>. They’re not like you’re typical tutorial but rather bullet-point type questions with advice on what to do on certain scenarios. I just came up with the idea of an interesting series of posts where each of the questions outlined in the documents is accompanied with a concrete example that shows how it works under certain scenarios.</p>
<p>There’s also other tutorial on that website such as with <a href="https://developers.google.com/machine-learning/guides/text-classification/">text classification</a>. This reminds me of the <a href="https://github.com/cimentadaj/info_to_read">list I keep of interesting blog posts/books/courses</a> I would to follow through in the future</p>
]]>
      </description>
    </item>
    
    <item>
      <title>A list of must pre-project questions</title>
      <link>https://cimentadaj.github.io/blog/2018-05-23-a-list-of-must-preproject-questions/a-list-of-must-pre-project-questions/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2018-05-23-a-list-of-must-preproject-questions/a-list-of-must-pre-project-questions/</guid>
      <description><![CDATA[
      


<p>Rumbling through Twitter I found a Jupyter Notebook of Paige Bailey written at the rOpensci unconf about Ethical Machine Learning which you can read <a href="https://github.com/ropenscilabs/proxy-bias-vignette/blob/master/EthicalMachineLearning.ipynb">here</a>. It was very interesting to look at her workflow but even more interesting was the set of questions she asked herself before and during the analysis. I paste them here just to keep them as a reference.</p>
<p><strong>As you design the goal and the purpose of your machine learning product, you must first ask: Who is your audience?</strong></p>
<ul>
<li>Is your product or analysis meant to include all people?</li>
<li>And, if not: is it targeted to an exclusive audience?</li>
<li>Is there a person on your team tasked specifically with identifying and resolving bias and discrimination issues?</li>
</ul>
<p><strong>Once the concept and scope have been defined, it is time to focus on the acquisition, evaluation, and cleaning of data. We have received a single .csv file filled with information on customers from the bank’s manager. Some questions to consider:</strong></p>
<ul>
<li>Did the data come from a system prone to human error?</li>
<li>Is the data current?</li>
<li>What technology facilitated the collection of the data?</li>
<li>Was participation of the data subjects voluntary?</li>
<li>Does the context of the collection match the context of your use?</li>
<li>Was your data collected by people or a system that was operating with quotas or a particular incentive structure?</li>
</ul>
<p><strong>Now that your data has been collected, it would be a great idea to evaluate and describe it:</strong></p>
<ul>
<li>Who is represented in the data?</li>
<li>Who is under-represented or absent from your data?</li>
<li>Can you find additional data, or use statistical methods, to make your data more inclusive?</li>
<li>Was the data collected in an environment where data subjects had meaningful choices?</li>
<li>How does the data reflect the perspective of the institution that collected it?</li>
<li>Were fields within the data inferred or appended beyond what was clear to the data subject?
Would this use of the data surprise the data subjects?</li>
</ul>
<p><strong>The next step would be cleaning the data.</strong></p>
<ul>
<li>Are there any fields that should be eliminated from your data?</li>
<li>Can you use anonymization or pseudonymization techniques to avoid needless evaluation or processing of individual data?</li>
</ul>
<p><strong>Establishing logic for variables</strong></p>
<ul>
<li>Can you describe the logic that connects the variables to the output of your equation?</li>
<li>Do your variables have a causal relationship to the results they predict?</li>
<li>How did you determine what weight to give each variable?</li>
</ul>
<p><strong>Identifying assumptions</strong></p>
<ul>
<li>Will your variables apply equally across race, gender, age, disability, ethnicity, socioeconomic status, education, etc.?</li>
<li>What are you assuming about the kinds of people in your data set?</li>
<li>Would you be comfortable explaining your assumptions to the public?</li>
<li>What assumptions are you relying on to determine the relevant variables and their weights?</li>
</ul>
<p><strong>Defining success</strong>
- What amount and type of error do you expect?
- How will you ensure your system is behaving the way you intend? How reliable is it?</p>
<p><strong>How will you choose your analytical method? For example, predictive analytics, machine learning (supervised, unsupervised), neural networks or deep learning, etc.</strong></p>
<ul>
<li>How much transparency does this method allow your end users and yourself?</li>
<li>Are non-deterministic outcomes acceptable given your legal or ethical obligations around transparency and explainability?</li>
<li>Does your choice of analytical method allow you to sufficiently explain your results?</li>
<li>What particular tasks are associated with the type of analytical method you are using?</li>
</ul>
<p><strong>Tools</strong></p>
<ul>
<li>How could results that look successful still contain bias?</li>
<li>Is there a trustworthy or audited source for the tools you need?</li>
<li>Have the tools you are using been associated with biased products?</li>
<li>Or, if you build from scratch: can you or a third-party test your tools for any features that can result in biased or unfair outcomes?</li>
</ul>
]]>
      </description>
    </item>
    
    <item>
      <title>Cleaning in-door positioning data</title>
      <link>https://cimentadaj.github.io/blog/2018-02-03-predicting-location-via-indoor-positioning-systems/predicting-location-via-indoor-positioning-systems/</link>
      <pubDate>Sat, 03 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2018-02-03-predicting-location-via-indoor-positioning-systems/predicting-location-via-indoor-positioning-systems/</guid>
      <description><![CDATA[
      


<p>I’ve just started reading the wonderful book <a href="http://rdatasciencecases.org/">Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving</a>. I’ve just begun the first chapter and I wanted to document some of the things I found interesting. In this post I’ll walkthrough the example on how to transform a text file with GPS locations into a well formatted rectangular dataset. For a detailed explanation see their book, which I highly recommend buying.</p>
<p>Note: When it makes senses/it’s possible, I always try to find an equivalent tidyverse solution to everything they do in the book.</p>
<p>This is the data.</p>
<pre class="r"><code>library(tidyverse)

ex_file &lt;- read_lines(&quot;http://rdatasciencecases.org/Data/offline.final.trace.txt&quot;)
ex_file[1:4]</code></pre>
<pre><code>## [1] &quot;# timestamp=2006-02-11 08:31:58&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                 
## [2] &quot;# usec=250&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                      
## [3] &quot;# minReadings=110&quot;                                                                                                                                                                                                                                                                                                                                                                                                                                               
## [4] &quot;t=1139643118358;id=00:02:2D:21:0F:33;pos=0.0,0.0,0.0;degree=0.0;00:14:bf:b1:97:8a=-38,2437000000,3;00:14:bf:b1:97:90=-56,2427000000,3;00:0f:a3:39:e1:c0=-53,2462000000,3;00:14:bf:b1:97:8d=-65,2442000000,3;00:14:bf:b1:97:81=-65,2422000000,3;00:14:bf:3b:c7:c6=-66,2432000000,3;00:0f:a3:39:dd:cd=-75,2412000000,3;00:0f:a3:39:e0:4b=-78,2462000000,3;00:0f:a3:39:e2:10=-87,2437000000,3;02:64:fb:68:52:e6=-88,2447000000,1;02:00:42:55:31:00=-84,2457000000,1&quot;</code></pre>
<p>Some lines are comments and the 4th line is the actual data. Basically, everything that is <code>something=</code> is the name of the column and columns are separated by a <code>;</code>. Now, within each column there can also be several values like in the column <code>pos</code> where numbers are separated by a comma.</p>
<p>First, let’s separate everything now that we know all of the delimiters.</p>
<pre class="r"><code>tokens &lt;- str_split(ex_file[4], pattern = &quot;[;=,]&quot;)[[1]]</code></pre>
<p>From the documentation we know that the first 4 columns are constant in every line. The remaining columns can vary by each line, which is why they decide to transform the data into stacked/long format. So each unique <code>id</code> will be repeate the number of times that there’s MAC columns (the columns that vary).</p>
<pre class="r"><code>tmp &lt;- matrix(tokens[-(1:10)], ncol = 4, byrow = TRUE)
# We got the MAC in a long format, now we have to get unique id
# of each of the macs (along with time and other vars) to be repeated
# the number of rows that tmp has


# There we go
tmp_two &lt;- matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, byrow = TRUE)

mat &lt;- cbind(tmp_two, tmp)
mat</code></pre>
<pre><code>##       [,1]            [,2]                [,3]  [,4]  [,5]  [,6] 
##  [1,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [2,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [3,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [4,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [5,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [6,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [7,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [8,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##  [9,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
## [10,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
## [11,] &quot;1139643118358&quot; &quot;00:02:2D:21:0F:33&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot; &quot;0.0&quot;
##       [,7]                [,8]  [,9]         [,10]
##  [1,] &quot;00:14:bf:b1:97:8a&quot; &quot;-38&quot; &quot;2437000000&quot; &quot;3&quot;  
##  [2,] &quot;00:14:bf:b1:97:90&quot; &quot;-56&quot; &quot;2427000000&quot; &quot;3&quot;  
##  [3,] &quot;00:0f:a3:39:e1:c0&quot; &quot;-53&quot; &quot;2462000000&quot; &quot;3&quot;  
##  [4,] &quot;00:14:bf:b1:97:8d&quot; &quot;-65&quot; &quot;2442000000&quot; &quot;3&quot;  
##  [5,] &quot;00:14:bf:b1:97:81&quot; &quot;-65&quot; &quot;2422000000&quot; &quot;3&quot;  
##  [6,] &quot;00:14:bf:3b:c7:c6&quot; &quot;-66&quot; &quot;2432000000&quot; &quot;3&quot;  
##  [7,] &quot;00:0f:a3:39:dd:cd&quot; &quot;-75&quot; &quot;2412000000&quot; &quot;3&quot;  
##  [8,] &quot;00:0f:a3:39:e0:4b&quot; &quot;-78&quot; &quot;2462000000&quot; &quot;3&quot;  
##  [9,] &quot;00:0f:a3:39:e2:10&quot; &quot;-87&quot; &quot;2437000000&quot; &quot;3&quot;  
## [10,] &quot;02:64:fb:68:52:e6&quot; &quot;-88&quot; &quot;2447000000&quot; &quot;1&quot;  
## [11,] &quot;02:00:42:55:31:00&quot; &quot;-84&quot; &quot;2457000000&quot; &quot;1&quot;</code></pre>
<p>There we go. We have a stacked matrix with all the variables we need. Let’s wrap the line maker into a function:</p>
<pre class="r"><code>processLine &lt;- function(x) {
  tokens &lt;- str_split(x, pattern = &quot;[;=,]&quot;)[[1]]
  
  # We got the MAC in a long format, now we have to get unique id
  # of each of the macs (along with time and other vars) to be repeated
  # the number of rows that tmp has
  tmp &lt;- matrix(tokens[-(1:10)], ncol = 4, byrow = TRUE)
  
  # There we go
  tmp_two &lt;- matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, byrow = TRUE)
  
  mat &lt;- cbind(tmp_two, tmp)
  mat
}</code></pre>
<p>Let’s apply it to a few sample rows:</p>
<pre class="r"><code>tmp &lt;- map(ex_file[4:20], processLine)

offline &lt;- as.data.frame(do.call(&quot;rbind&quot;, tmp))
head(offline)</code></pre>
<pre><code>##              V1                V2  V3  V4  V5  V6                V7  V8
## 1 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:8a -38
## 2 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:90 -56
## 3 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:0f:a3:39:e1:c0 -53
## 4 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:8d -65
## 5 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:81 -65
## 6 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:3b:c7:c6 -66
##           V9 V10
## 1 2437000000   3
## 2 2427000000   3
## 3 2462000000   3
## 4 2442000000   3
## 5 2422000000   3
## 6 2432000000   3</code></pre>
<p>Good! Now we can apply it to all lines, excluding of course the ones which are commented out!</p>
<pre class="r"><code>tmp &lt;- map(ex_file[!str_sub(ex_file, 1, 1) == &quot;#&quot;], processLine)</code></pre>
<pre><code>## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix</code></pre>
<p>Aha.. so there’s a few warnings? What’s happening? If we ran the previous with <code>options(error, warn = 2)</code> we would see that it looks like there are some anomalous cases where there’s no MAC information. We either fill out those values with NA’s or we simply exclude them. Because working with the MAC’s is of utmost importance for the analysis, we drop it to save memory. We redefine our function so that if there’s only the 10 starting values it returns a NULL.</p>
<pre class="r"><code>processLine &lt;- function(x) {
  tokens &lt;- str_split(x, pattern = &quot;[;=,]&quot;)[[1]]
  
  # We exclude rows where there&#39;s no MAC information
  if (length(tokens) == 10) return(NULL)
  
  # We got the MAC in a long format, now we have to get unique id
  # of each of the macs (along with time and other vars) to be repeated
  # the number of rows that tmp has
  tmp &lt;- matrix(tokens[-(1:10)], ncol = 4, byrow = TRUE)
  
  # There we go
  tmp_two &lt;- matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, byrow = TRUE)
  
  mat &lt;- cbind(tmp_two, tmp)
  mat
}</code></pre>
<p>And apply it now..</p>
<pre class="r"><code>tmp &lt;- map(ex_file[!str_sub(ex_file, 1, 1) == &quot;#&quot;], processLine)

offline &lt;- as_tibble(do.call(&quot;rbind&quot;, tmp))</code></pre>
<p>Good, let’s set warnings back: <code>options(error = recover, warn = 1)</code></p>
<p>To finish off let’s set some names.</p>
<pre class="r"><code>names(offline) &lt;- c(&quot;time&quot;, &quot;scanMac&quot;, &quot;posX&quot;, &quot;posY&quot;, &quot;posZ&quot;,
                    &quot;orientation&quot;, &quot;mac&quot;, &quot;signal&quot;, &quot;channel&quot;, &quot;type&quot;)

offline</code></pre>
<pre><code>## # A tibble: 1,181,628 x 10
##    time   scanMac posX  posY  posZ  orientation mac   signal channel type 
##    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;
##  1 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -38    243700~ 3    
##  2 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -56    242700~ 3    
##  3 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -53    246200~ 3    
##  4 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -65    244200~ 3    
##  5 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -65    242200~ 3    
##  6 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -66    243200~ 3    
##  7 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -75    241200~ 3    
##  8 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -78    246200~ 3    
##  9 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -87    243700~ 3    
## 10 11396~ 00:02:~ 0.0   0.0   0.0   0.0         02:6~ -88    244700~ 1    
## # ... with 1,181,618 more rows</code></pre>
<p>— <strong>BONUS</strong> —</p>
<p>Just wanted to try to get the data in a wide format where each MAC indicator is a column rather than stacked.</p>
<pre class="r"><code># Define the MAC colums as wide. Because each MAC columns
# has three associated values, I stack them up so there should
# be three rows pero every MAC column
right_col &lt;- tokens[-(1:10)]

right_names &lt;- seq(1, length(right_col), by = 4)

mac_tibble &lt;-
  matrix(right_col[-right_names], nrow = 3, ncol = length(right_names),
         dimnames = list(NULL, right_col[right_names])) %&gt;%
  as_tibble() %&gt;%
  add_column(mac_indicators = c(&quot;signal&quot;, &quot;chanel&quot;, &quot;type&quot;),
             .before = 1)

# Define the first four columns
left_col &lt;- tokens[1:10]

left_names &lt;- seq(1, length(left_col), by = 2)

left_tibble &lt;-
  matrix(left_col[-left_names], nrow = 3, ncol = length(left_names), byrow = TRUE,
         dimnames = list(NULL, left_col[left_names])) %&gt;%
  as_tibble()

# Bind both dfs
mat &lt;- bind_cols(left_tibble, mac_tibble)
mat</code></pre>
<pre><code>## # A tibble: 3 x 17
##   t         id         pos   `0.0` degree mac_indicators `00:14:bf:b1:97:~
##   &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;            
## 1 11396431~ 00:02:2D:~ 0.0   0.0   0.0    signal         -38              
## 2 11396431~ 00:02:2D:~ 0.0   0.0   0.0    chanel         2437000000       
## 3 11396431~ 00:02:2D:~ 0.0   0.0   0.0    type           3                
## # ... with 10 more variables: `00:14:bf:b1:97:90` &lt;chr&gt;,
## #   `00:0f:a3:39:e1:c0` &lt;chr&gt;, `00:14:bf:b1:97:8d` &lt;chr&gt;,
## #   `00:14:bf:b1:97:81` &lt;chr&gt;, `00:14:bf:3b:c7:c6` &lt;chr&gt;,
## #   `00:0f:a3:39:dd:cd` &lt;chr&gt;, `00:0f:a3:39:e0:4b` &lt;chr&gt;,
## #   `00:0f:a3:39:e2:10` &lt;chr&gt;, `02:64:fb:68:52:e6` &lt;chr&gt;,
## #   `02:00:42:55:31:00` &lt;chr&gt;</code></pre>
<p>Let’s wrap it into a function excluding those which dont have MAC values.</p>
<pre class="r"><code>processLine &lt;- function(x) {
  tokens &lt;- str_split(x, pattern = &quot;[;=,]&quot;)[[1]]
  
  if (length(tokens) == 10) return(NULL) # exclude non-MAC lines
  
  right_col &lt;- tokens[-(1:10)]
  
  right_names &lt;- seq(1, length(right_col), by = 4)
  
  mac_tibble &lt;-
    matrix(right_col[-right_names], nrow = 3, ncol = length(right_names),
           dimnames = list(NULL, right_col[right_names]))

  # Define the first four columns
  left_col &lt;- tokens[1:10]
  
  left_names &lt;- seq(1, length(left_col), by = 2)
  
  left_tibble &lt;-
    matrix(left_col[-left_names], nrow = 3, ncol = length(left_names), byrow = TRUE,
           dimnames = list(NULL, left_col[left_names]))

  # Bind both dfs
  mat &lt;- cbind(left_tibble, mac_tibble)
  mat
}</code></pre>
<p>Let’s apply it to each line:</p>
<pre class="r"><code>tmp &lt;- map(ex_file[!str_sub(ex_file, 1, 1) == &quot;#&quot;], processLine)

# Interestingly, applying as_tibble instead of as.data.frame is
# very slow. So I opt for data frame and then convert the binded
# df to a tibble
final_data &lt;-
  bind_rows(map(tmp, as.data.frame, stringsAsFactors = FALSE)) %&gt;%
  as_tibble() %&gt;%
  add_column(mac_indicators = rep(c(&quot;signal&quot;, &quot;chanel&quot;, &quot;type&quot;), length(unique(.$t))),
             .after = &quot;degree&quot;)

final_data</code></pre>
<pre><code>## # A tibble: 438,222 x 40
##    t         id        pos   `0.0` degree mac_indicators `00:14:bf:b1:97:~
##    &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;chr&gt;            
##  1 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
##  2 11396431~ 00:02:2D~ 0.0   0.0   0.0    chanel         2437000000       
##  3 11396431~ 00:02:2D~ 0.0   0.0   0.0    type           3                
##  4 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
##  5 11396431~ 00:02:2D~ 0.0   0.0   0.0    chanel         2437000000       
##  6 11396431~ 00:02:2D~ 0.0   0.0   0.0    type           3                
##  7 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
##  8 11396431~ 00:02:2D~ 0.0   0.0   0.0    chanel         2437000000       
##  9 11396431~ 00:02:2D~ 0.0   0.0   0.0    type           3                
## 10 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
## # ... with 438,212 more rows, and 33 more variables:
## #   `00:14:bf:b1:97:90` &lt;chr&gt;, `00:0f:a3:39:e1:c0` &lt;chr&gt;,
## #   `00:14:bf:b1:97:8d` &lt;chr&gt;, `00:14:bf:b1:97:81` &lt;chr&gt;,
## #   `00:14:bf:3b:c7:c6` &lt;chr&gt;, `00:0f:a3:39:dd:cd` &lt;chr&gt;,
## #   `00:0f:a3:39:e0:4b` &lt;chr&gt;, `00:0f:a3:39:e2:10` &lt;chr&gt;,
## #   `02:64:fb:68:52:e6` &lt;chr&gt;, `02:00:42:55:31:00` &lt;chr&gt;,
## #   `00:04:0e:5c:23:fc` &lt;chr&gt;, `00:30:bd:f8:7f:c5` &lt;chr&gt;, `1.0` &lt;chr&gt;,
## #   `2.0` &lt;chr&gt;, `3.0` &lt;chr&gt;, `4.0` &lt;chr&gt;, `5.0` &lt;chr&gt;, `6.0` &lt;chr&gt;,
## #   `7.0` &lt;chr&gt;, `8.0` &lt;chr&gt;, `9.0` &lt;chr&gt;, `10.0` &lt;chr&gt;, `11.0` &lt;chr&gt;,
## #   `12.0` &lt;chr&gt;, `13.0` &lt;chr&gt;, `00:e0:63:82:8b:a9` &lt;chr&gt;,
## #   `02:37:fd:3b:54:b5` &lt;chr&gt;, `02:2e:58:22:f1:ac` &lt;chr&gt;,
## #   `02:42:1c:4e:b5:c0` &lt;chr&gt;, `02:0a:3d:06:94:88` &lt;chr&gt;,
## #   `02:5c:e0:50:49:de` &lt;chr&gt;, `02:4f:99:43:30:cd` &lt;chr&gt;,
## #   `02:b7:00:bb:a9:35` &lt;chr&gt;</code></pre>
<p>There we go! It’s a bit refreshing to work on datasets that are not pre-cleaned for you.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>How long should I wait for my bike?</title>
      <link>https://cimentadaj.github.io/blog/2017-12-01-how-long-should-i-wait-for-my-bike/how-long-should-i-wait-for-my-bike/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2017-12-01-how-long-should-i-wait-for-my-bike/how-long-should-i-wait-for-my-bike/</guid>
      <description><![CDATA[
      


<p>I’ve just started a project which I’m very excited about. Everyday I take my bike to work and most days I have one of two problems. First, whenever I get to my station there are no bikes available; no problem, there’s an app that shows the closest stations with bikes available. The problem is that these stations might be far and sometimes I’m relucant to walk that much. I’d love for bicing to give me some time estimation until a new bike arrives.</p>
<p>Second, whenever you’re trying to return a bike the station might not have any parking spaces available. Similarly, it would be very cool if bicing (the public bicycle company) gave me an estimate of how much time I should wait until a new bike will be taken. I started thinking on how I could implement this and started looking for bicing data online. To my surprise, bicing actually releases their <strong>live</strong> data as a json! But for this type of estimation I need historical data. I want to know the pattern usage of the station and use that information for the prediction.</p>
<p>With that idea in mind, I got to work. I needed to set up my Virtual Private Server (VPS) to pull the data from the bicing API everyday. Because this is still a work in progress, I will only describe here how I set my VPS to scrape the bicing API everyday and how I set <code>cron</code> to send me an email after every scrape.</p>
<p>I have a VPS from <a href="https://www.digitalocean.com/">Digital Ocean</a> with an Ubuntu OS and 512 mb of RAM and 2 GB of hard disk. That’s enough for this task because the data should not be very big, even in the long run. In any case you can adjust for your VPS to have more memory/ram without losing information. Assuming you have <a href="https://www.digitalocean.com/community/tutorials/how-to-install-r-on-ubuntu-16-04-2">R installed in your Ubuntu VPS</a> with your favorite packages, then make sure your script works by running <code>Rscript path/to/your/script.R</code>. It might be better to type <code>which Rscript</code> in the terminal and paste the path to the executable, similar to <code>/usr/bin/Rscript path/to/your/script.R</code></p>
<p>My workflow is as follows: I first create an empty dataset saved as <code>.rds</code> and my script reads the data, scrapes the bicing data and then saves the data by appending both the empty and the scraped data. It finishes by saving the same <code>.rds</code> for a later scrape. I tested this very thoroughly to make sure the script wouldn’t fail and I always get the expected data.</p>
<p>All good so far, right? This took me no time. The hard problem came when setting the <code>cron</code> job, which is a way of scheduling tasks in OSx and Ubuntu. For an explanation of how <code>cron</code> works, check out how I set <a href="blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/index.html">my PISA twitter bot</a>.</p>
<p>First, make sure you have <code>cron</code> <a href="https://www.digitalocean.com/community/tutorials/how-to-use-cron-to-automate-tasks-on-a-vps">installed</a>. I followed <strong>a lot</strong> of tutorials and dispered information. What worked for me perhaps does not work for you, but here it is.</p>
<p>Type <code>crontab -e</code> and the cron interface should appear. The lines starting with <code>#</code> are coments, so scroll down until the end of the comments. First we have to set a few environmental variables that <code>cron</code> uses to execute your script. I followed <a href="http://krisjordan.com/essays/timesaving-crontab-tips">these tips</a>.</p>
<p>When I finished my crontab looked like this:</p>
<pre class="bash"><code>SHELL=/bin/bash
PATH=/home/cimentadaj/bin:/home/cimentadaj/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
HOME=/home/cimentadaj/bicycle
MAILTO=my_email # set email here!
15,50 16  * * * /usr/bin/Rscript scrape_bicing.R</code></pre>
<ul>
<li><p>SHELL is the path to the pre-determined program to run on the cron job. Be default I set it to bash (but it could be anything else you want).</p></li>
<li><p>PATH I’m not sure what’s for but I pasted the output of <code>echo $PATH</code>, as the tips suggested.</p></li>
<li><p>HOME is the root directory where the script will be executed, I set it to where the script is (or where your project is at).</p></li>
<li><p>MAILTO is the email where I will get the cron job alert when it finishes.</p></li>
<li><p><code>15,50 16  * * * /usr/bin/Rscript scrape_bicing.R</code> is the schedule, program and script to run. Here I set arbitrary times, so the the script is scheduled to run at <code>16:15</code> and <code>16:50</code> every day, every month and every year. I will run using <code>Rscript</code> and the name of the script to run.</p></li>
</ul>
<p><strong>WARNING:</strong> remember that the <code>cron</code> is set relative to the time of where your server is. Mine did not have the same timezone of where I lived, so I had to set the <code>cron</code> one hour before of my actual time. Use <code>date</code> to print the time of your VPS.</p>
<p>Even after this, the <code>cron</code> job was still not running. Nothing, no email, no log, no change in the data. I then figured out that Ubuntu systems have some <a href="https://serverfault.com/a/754104">pecularities</a> when it comes to <code>cron</code>. So I went to <code>./etc/</code> and renamed every <code>cron.</code> file for <code>cron-</code> with <code>rename 's/cron./cron-/g' *</code>, thanks to this <a href="https://stackoverflow.com/a/20657563/3617958">answer</a>.</p>
<p>Run again and it worked! Great. However, I didn’t receive an email stating that the <code>cron</code> job finished. I looked up many solutions and ended up installing <code>ssmtp</code> which is a library for sending emails from terminal. I won’t bore you with the details. Here are the steps I took:</p>
<ul>
<li>Install <code>ssmtp</code> with <code>sudo apt-get update</code> and <code>sudo apt-get install ssmtp</code>.</li>
<li>Edit <code>ssmtp.conf</code> with <code>sudo nano /etc/ssmtp/ssmtp.conf</code></li>
</ul>
<p>Here’s the config that worked for me using <code>gmail</code>:</p>
<pre class="bash"><code># Config file for sSMTP sendmail
#
# The person who gets all mail for userids &lt; 1000
# Make this empty to disable rewriting.
root=your_email@gmail.com

# The place where the mail goes. The actual machine name is required no 
# MX records are consulted. Commonly mailhosts are named mail.domain.com
mailhub=smtp.gmail.com:587

AuthUser=your_email@gmail.com
AuthPass=your_password
UseTLS=YES
UseSTARTTLS=yes
TLS_CA_FILE=/etc/ssl/certs/ca-certificates.crt

# Where will the mail seem to come from?
#rewriteDomain=gmail.com

# The full hostname
hostname=your_host_name

# Are users allowed to set their own From: address?
# YES - Allow the user to specify their own From: address
# NO - Use the system generated From: address
#FromLineOverride=YES</code></pre>
<p>Three caveats that took me a lot of time to figure out.</p>
<ul>
<li><p>First, <a href="https://www.digitalocean.com/community/tutorials/how-to-use-google-s-smtp-server">some docs</a> say you should use another port in <code>mailhub</code>, but <code>587</code> worked for me.</p></li>
<li><p><code>TLS_CA_FILE</code>: make sure that <a href="https://askubuntu.com/questions/342484/etc-pki-tls-certs-ca-bundle-crt-not-found">this file exists</a>! For Ubuntu/Debian the file is at <code>/etc/ssl/certs/ca-certificates.crt</code> while on other platforms it might be in <code>/etc/pki/tls/certs/ca-bundle.crt</code>. Note the different file names!</p></li>
<li><p><code>hostname</code> should be the result of typing <code>hostname</code> in your server.</p></li>
</ul>
<p>Lastly, I also added the line <code>root:your_EMAIL_@gmail.com:smtp.gmail.com:587</code> with <code>sudo nano /etc/ssmtp/revaliases</code>.</p>
<p>After an entire day figuring out all this information, the <code>cron</code> job worked! I now set my <code>cron</code> job and whenever it finished I receive an email directly showing the log of the script.</p>
<p>I wrote this primarily for me not to forget any of this, but it might be useful for other people.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Scraping and visualizing How I Met Your Mother</title>
      <link>https://cimentadaj.github.io/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</guid>
      <description><![CDATA[
      


<p>How I Met Your Mother (HIMYM from here after) is a television series very similar to the classical ‘Friends’ series from the 90’s. Following the release of the <a href="http://tidytextmining.com/">tidy text</a> book I was looking for a project in which I could apply these skills. I decided I would scrape all the transcripts from HIMYM and analyze patterns between characters. This post really took me to the limit in terms of web scraping and pattern matching, which was specifically what I wanted to improve in the first place. Let’s begin!</p>
<p>My first task was whether there was any consistency in the URL’s that stored the transcripts. If you ever watched HIMYM, we know there’s around nine seasons, each one with about 22 episodes. This makes about 200 episodes give or take. It would be a big pain to manually write down 200 complicated URL’s. Luckily, there is a way of finding the 200 links without writing them down manually.</p>
<p>First, we create the links for the 9 websites that contain all episodes (1 through season 9)</p>
<pre class="r"><code>library(rvest)
library(tidyverse)
library(stringr)
library(tidytext)

main_url &lt;- &quot;http://transcripts.foreverdreaming.org&quot;
all_pages &lt;- paste0(&quot;http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=&quot;, seq(0, 200, 25))
characters &lt;- c(&quot;ted&quot;, &quot;lily&quot;, &quot;marshall&quot;, &quot;barney&quot;, &quot;robin&quot;)</code></pre>
<p>Each of the URL’s of <code>all_pages</code> contains all episodes for that season (so around 22 URL’s). I also picked the characters we’re gonna concentrate for now. From here the job is very easy. We create a function that reads each link and parses the section containing all links for that season. We can do that using <a href="http://selectorgadget.com/.">SelectorGadget</a> to find the section we’re interested in. We then search for the <code>href</code> attribute to grab all links in that attribute and finally create a tibble with each episode together with it’s link.</p>
<pre class="r"><code>episode_getter &lt;- function(link) {
  title_reference &lt;-
    link %&gt;%
    read_html() %&gt;%
    html_nodes(&quot;.topictitle&quot;) # Get the html node name with &#39;selector gadget&#39;
  
  episode_links &lt;-
    title_reference %&gt;%
    html_attr(&quot;href&quot;) %&gt;%
    gsub(&quot;^.&quot;, &quot;&quot;, .) %&gt;%
    paste0(main_url, .) %&gt;%
    setNames(title_reference %&gt;% html_text()) %&gt;%
    enframe(name = &quot;episode_name&quot;, value = &quot;link&quot;)
  
  episode_links
}

all_episodes &lt;- map_df(all_pages, episode_getter) # loop over all seasons and get all episode links
all_episodes$id &lt;- 1:nrow(all_episodes)</code></pre>
<p>There we go! Now we have a very organized <code>tibble</code>.</p>
<pre class="r"><code>all_episodes
# # A tibble: 208 x 3
#    episode_name                   link                                  id
#    &lt;chr&gt;                          &lt;chr&gt;                              &lt;int&gt;
#  1 01x01 - Pilot                  http://transcripts.foreverdreamin~     1
#  2 01x02 - Purple Giraffe         http://transcripts.foreverdreamin~     2
#  3 01x03 - Sweet Taste of Liberty http://transcripts.foreverdreamin~     3
#  4 01x04 - Return of the Shirt    http://transcripts.foreverdreamin~     4
#  5 01x05 - Okay Awesome           http://transcripts.foreverdreamin~     5
#  6 01x06 - Slutty Pumpkin         http://transcripts.foreverdreamin~     6
#  7 01x07 - Matchmaker             http://transcripts.foreverdreamin~     7
#  8 01x08 - The Duel               http://transcripts.foreverdreamin~     8
#  9 01x09 - Belly Full of Turkey   http://transcripts.foreverdreamin~     9
# 10 01x10 - The Pineapple Incident http://transcripts.foreverdreamin~    10
# # ... with 198 more rows</code></pre>
<p>The remaining part is to actually scrape the text from each episode. We can work that out for a single episode and then turn that into a function and apply for all episodes.</p>
<pre class="r"><code>episode_fun &lt;- function(file) {
  
  file %&gt;%
    read_html() %&gt;%
    html_nodes(&quot;.postbody&quot;) %&gt;%
    html_text() %&gt;%
    str_split(&quot;\n|\t&quot;) %&gt;%
    .[[1]] %&gt;%
    data_frame(text = .) %&gt;%
    filter(str_detect(text, &quot;&quot;), # Lots of empty spaces
           !str_detect(text, &quot;^\\t&quot;), # Lots of lines with \t to delete
           !str_detect(text, &quot;^\\[.*\\]$&quot;), # Text that start with brackets
           !str_detect(text, &quot;^\\(.*\\)$&quot;), # Text that starts with parenthesis
           str_detect(text, &quot;^*.:&quot;), # I want only lines with start with dialogue (:)
           !str_detect(text, &quot;^ad&quot;)) # Remove lines that start with ad (for &#39;ads&#39;, the link of google ads)
}</code></pre>
<p>The above function reads each episode, turns the html text into a data frame and organizes it clearly for text analysis. For example:</p>
<pre class="r"><code>episode_fun(all_episodes$link[15])
# # A tibble: 195 x 1
#    text                                                                   
#    &lt;chr&gt;                                                                  
#  1 Ted from 2030: Kids, something you might not know about your Uncle Mar~
#  2 &quot;Ted: You don&#39;t have to shout out \&quot;poker\&quot; when you win.&quot;             
#  3 Marshall: I know. It&#39;s just fun to say.                                
#  4 &quot;Ted from 2030: We all finally agreed Marshall should be running our g~
#  5 &quot;Marshall: It&#39;s called \&quot;Marsh-gammon.\&quot; It combines all the best feat~
#  6 Robin: Backgammon, obviously.                                          
#  7 &quot;Marshall: No. Backgammon sucks. I took the only good part of backgamm~
#  8 Lily: I&#39;m so excited Victoria&#39;s coming.                                
#  9 Robin: I&#39;m going to go get another round.                              
# 10 Ted: Okay, I want to lay down some ground rules for tonight. Barney, I~
# # ... with 185 more rows</code></pre>
<p>We now have a data frame with only dialogue for each character. We need to apply that function to each episode and <code>bind</code> everything together. We first apply the function to every episode.</p>
<pre class="r"><code>all_episodes$text &lt;- map(all_episodes$link, episode_fun)</code></pre>
<p>The <code>text</code> list-column is an organized list with text for each episode. However, manual inspection of some episodes actually denotes a small error that limits our analysis greatly. Among the main interests of this document is to study relationships and presence between characters. For that, we need each line of text to be accompanied by the character who said it. Unfortunately, some of these scripts don’t have that.</p>
<p>For example, check any episode from season <a href="http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=175">8</a> and <a href="http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;start=200">9</a>. The writer didn’t write the dialogue and just rewrote the lines. There’s nothing we can do so far to improve that and we’ll be excluding these episodes. This pattern is also present in random episodes like in season 4 or season 6. We can exclude chapters based on the number of lines we parsed. On average, each of these episodes has about 200 lines of dialogue. Anything significantly lower, like 30 or 50 lines, is an episode which doesn’t have a lot of dialogue.</p>
<pre class="r"><code>all_episodes$count &lt;- map_dbl(all_episodes$text, nrow)</code></pre>
<p>We can extend the previous <code>tibble</code> to be a bit more organized by separating the episode-season column into separate season and episo numbers.</p>
<pre class="r"><code>all_episodes &lt;-
  all_episodes %&gt;%
  separate(episode_name, c(&quot;season&quot;, &quot;episode&quot;), &quot;-&quot;, extra = &quot;merge&quot;) %&gt;%
  separate(season, c(&quot;season&quot;, &quot;episode_number&quot;), sep = &quot;x&quot;)</code></pre>
<p>Great! We now have a very organized <code>tibble</code> with all the information we need. Next step is to actually break down the lines into words and start looking for general patterns. We can do that by looping through all episodes that have over 100 lines (just an arbitrary threshold) and unnesting each line for each <strong>valid</strong> character.</p>
<pre class="r"><code>lines_characters &lt;-
  map(filter(all_episodes, count &gt; 100) %&gt;% pull(text), ~ { 
    # only loop over episodes that have over 100 lines
    .x %&gt;%
      separate(text, c(&quot;character&quot;, &quot;text&quot;), sep = &quot;:&quot;, extra = &#39;merge&#39;) %&gt;%
      # separate character dialogue from actual dialogo
      unnest_tokens(character, character) %&gt;%
      filter(str_detect(character, paste0(paste0(&quot;^&quot;, characters, &quot;$&quot;), collapse = &quot;|&quot;))) %&gt;%
      # only count the lines of our chosen characters
      mutate(episode_lines_id = 1:nrow(.))
  }) %&gt;%
  setNames(filter(all_episodes, count &gt; 100) %&gt;% # name according to episode
             unite(season_episode, season, episode_number, sep = &quot;x&quot;) %&gt;%
             pull(season_episode)) %&gt;%
  enframe() %&gt;%
  unnest() %&gt;%
  mutate(all_lines_id = 1:nrow(.))</code></pre>
<p>Ok, our text is sort of ready. Let’s remove some bad words.</p>
<pre class="r"><code>words_per_character &lt;-
  lines_characters %&gt;%
  unnest_tokens(word, text) %&gt;% # expand all sentences into words
  anti_join(stop_words) %&gt;% # remove bad words
  filter(!word %in% characters) %&gt;% # only select characters we&#39;re interested
  arrange(name) %&gt;%
  separate(name, c(&quot;season&quot;, &quot;episode&quot;), sep = &quot;x&quot;, remove = FALSE) %&gt;%
  mutate(name = factor(name, ordered = TRUE),
         season = factor(season, ordered = TRUE),
         episode = factor(episode, ordered = TRUE)) %&gt;%
  filter(season != &quot;07&quot;)</code></pre>
<p>Just to make sure, let’s look at the <code>tibble</code>.</p>
<pre class="r"><code>words_per_character
# # A tibble: 88,174 x 7
#    name     season episode character episode_lines_id all_lines_id word   
#    &lt;ord&gt;    &lt;ord&gt;  &lt;ord&gt;   &lt;chr&gt;                &lt;int&gt;        &lt;int&gt; &lt;chr&gt;  
#  1 &quot;01x01 &quot; 01     &quot;01 &quot;   marshall                 1            1 ring   
#  2 &quot;01x01 &quot; 01     &quot;01 &quot;   marshall                 1            1 marry  
#  3 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 perfect
#  4 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 engaged
#  5 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 pop    
#  6 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 champa~
#  7 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 drink  
#  8 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 toast  
#  9 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 kitchen
# 10 &quot;01x01 &quot; 01     &quot;01 &quot;   ted                      2            2 floor  
# # ... with 88,164 more rows</code></pre>
<p>Perfect! One row per word, per character, per episode with the id of the line of the word.</p>
<p>Alright, let’s get our hands dirty. First, let visualize the presence of each character in terms of words over time.</p>
<pre class="r"><code># Filtering position of first episode of all seasons to
# position the X axis in the next plot.
first_episodes &lt;-
  all_episodes %&gt;%
  filter(count &gt; 100, episode_number == &quot;01 &quot;) %&gt;%
  pull(id)

words_per_character %&gt;%
  split(.$name) %&gt;%
  setNames(1:length(.)) %&gt;%
  enframe(name = &quot;episode_id&quot;) %&gt;%
  unnest() %&gt;%
  count(episode_id, character) %&gt;%
  group_by(episode_id) %&gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&gt;%
  ggplot(aes(as.numeric(episode_id), perc, group = character, colour = character)) +
  geom_line() +
  geom_smooth(method = &quot;lm&quot;) +
  scale_colour_discrete(guide = FALSE) +
  scale_x_continuous(name = &quot;Seasons&quot;,
                     breaks = first_episodes, labels = paste0(&quot;S&quot;, 1:7)) +
  scale_y_continuous(name = &quot;Percentage of words per episode&quot;) +
  theme_minimal() +
  facet_wrap(~ character, ncol = 3)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-13-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Ted is clearly the character with the highest number of words per episode followed by Barney. Lily and Robin, the only two women have very low presence compared to the men. In fact, if one looks closely, Lily seemed to have decreased slightly over time, having an all time low in season 4. Marshall, Lily’s partner in the show, does have much lower presence than both Barney and Ted but he has been catching up over time.</p>
<p>We also see an interesting pattern where Barney has a lot of peaks, suggesting that in some specific episodes he gains predominance, where Ted has an overall higher level of words per episode. And when Ted has peaks, it’s usually below its trend-line.</p>
<p>Looking at the distribution:</p>
<pre class="r"><code># devtools::install_github(&quot;clauswilke/ggjoy&quot;)
library(ggjoy)

words_per_character %&gt;%
  split(.$name) %&gt;%
  setNames(1:length(.)) %&gt;%
  enframe(name = &quot;episode_id&quot;) %&gt;%
  unnest() %&gt;%
  count(season, episode_id, character) %&gt;%
  group_by(episode_id) %&gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&gt;%
  ggplot(aes(x = perc, y = character, fill = character)) +
  geom_joy(scale = 0.85) +
  scale_fill_discrete(guide = F) +
  scale_y_discrete(name = NULL, expand=c(0.01, 0)) +
  scale_x_continuous(name = &quot;Percentage of words&quot;, expand=c(0.01, 0)) +
  ggtitle(&quot;Percentage of words per season&quot;) +
  facet_wrap(~ season, ncol = 7) +
  theme_minimal()</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>we see the differences much clearer. For example, we see Barney’s peaks through out every season with Season 6 seeing a clear peak of 40%. On the other hand, we see that their distributions don’t change that much over time! Suggesting that the presence of each character is very similar in all seasons. Don’t get me wrong, there are differences like Lily in Season 2 and then in Season 6, but in overall terms the previous plot suggests no increase over seasons, and this plot suggests that between seasons, there’s not a lot of change in their distributions that affects the overall mean.</p>
<p>If you’ve watched the TV series, you’ll remember Barney always repeating one similar trademark word: legendary! Although it is a bit cumbersome for us to count the number of occurrences of that sentence once we unnested each sentence, we can at least count the number of words per character and see whether some characters have particular words.</p>
<pre class="r"><code>count_words &lt;-
  words_per_character %&gt;%
  filter(!word %in% characters) %&gt;%
  count(character, word, sort = TRUE)

count_words %&gt;%
  group_by(character) %&gt;%
  top_n(20) %&gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &quot;free_y&quot;)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Here we see that a lot of the words we capture are actually nouns or expressions which are common to everyone, such as ‘yeah’, ‘hey’ or ‘time’. We can weight down commonly used words for other words which are important but don’t get repeated a lot. We can exclude those words using <code>bind_tf_idf()</code>, which for each character decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection or corpus of documents (see 3.3 in <a href="http://tidytextmining.com/tfidf.html" class="uri">http://tidytextmining.com/tfidf.html</a>).</p>
<pre class="r"><code>count_words %&gt;%
  bind_tf_idf(word, character, n) %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  group_by(character) %&gt;%
  top_n(20) %&gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &quot;free_y&quot;)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Now Barney has a very distinctive word usage, one particularly sexist with words such as couger, bang and tits. Also, we see the word legendary as the thirdly repeated word, something we were expecting! On the other hand, we see Ted with things like professor (him), aunt (because of aunt Lily and such).</p>
<p>Knowing that Ted is the main character in the series is no surprise. To finish off, we’re interested in knowing which characters are related to each other. First, let’s turn the data frame into a suitable format.</p>
<p>Here we turn all lines to lower case and check which characters are present in the text of each dialogue. The loop will return a vector of logicals whether there was a mention of any of the characters. For simplicity I exclude all lines where there is more than 1 mention of a character, that is, 2 or more characters.</p>
<pre class="r"><code>lines_characters &lt;-
  lines_characters %&gt;%
  mutate(text = str_to_lower(text))

rows_fil &lt;-
  map(characters, ~ str_detect(lines_characters$text, .x)) %&gt;%
  reduce(`+`) %&gt;%
  ifelse(. &gt;= 2, 0, .) # excluding sentences which have 2 or more mentions for now
  # ideally we would want to choose to count the number of mentions
  # per line or randomly choose another a person that was mentioned.</code></pre>
<p>Now that we have the rows that have a mention of another character, we subset only those rows. Then we want know which character was mentioned in which line. I loop through each line and test which character is present in that specific dialogue line. The loop returns the actual character name for each dialogue. Because we already filtered lines that <strong>have</strong> a character name mentioned, the loop should return a vector of the same length.</p>
<pre class="r"><code>character_relation &lt;-
  lines_characters %&gt;%
  filter(as.logical(rows_fil)) %&gt;%
  mutate(who_said_what =
           map_chr(.$text, ~ { # loop over all each line
             who_said_what &lt;- map_lgl(characters, function(.y) str_detect(.x, .y))
             # loop over each character and check whether he/she was mentioned
             # in that line
             characters[who_said_what]
             # subset the character that matched
           }))
</code></pre>
<p>Finally, we plot the relationship using the <code>ggraph</code> package.</p>
<pre class="r"><code>library(ggraph)
library(igraph)

character_relation %&gt;%
  count(character, who_said_what) %&gt;%
  graph_from_data_frame() %&gt;%
  ggraph(layout = &quot;linear&quot;, circular = TRUE) +
  geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                width = 2.5, show.legend = FALSE) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>A very clear pattern emerges. There is a strong relationship between Robin and Barney towards Ted. In fact, their direct relationship is very weak, but both are very well connected to Ted. On the other hand, Marshall and Lily are also reasonably connected to Ted but with a weaker link. Both of them are indeed very connected, as should be expected since they were a couple in the TV series.</p>
<p>We also see that the weakest members of the group are Robin and Barney with only strong bonds toward Ted but no strong relationship with the other from the group. Overall, there seems to be a division: Marshall and Lily hold a somewhat close relationship with each other and towards Ted and Barney and Robin tend to be related to Ted but no one else.</p>
<p>As a follow-up question, is this pattern of relationships the same across all seasons? We can do that very quickly by filtering each season using the previous plot.</p>
<pre class="r"><code>library(cowplot)

# Loop through each season
seasons &lt;- paste0(0, 1:7)

all_season_plots &lt;- lapply(seasons, function(season_num) {

  set.seed(2131)
  
  character_relation %&gt;%
    # Extract the season number from the `name` column
    mutate(season = str_replace_all(character_relation$name, &quot;x(.*)$&quot;, &quot;&quot;)) %&gt;%
    filter(season == season_num) %&gt;%
    count(character, who_said_what) %&gt;%
    graph_from_data_frame() %&gt;%
    ggraph(layout = &quot;linear&quot;, circular = TRUE) +
    geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                  width = 2.5, show.legend = FALSE) +
    geom_node_text(aes(label = name), repel = TRUE) +
    theme_void()
})

# Plot all graphs side-by-side
cowplot::plot_grid(plotlist = all_season_plots, labels = seasons)</code></pre>
<p><img src="/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-20-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>There are reasonable changes for all non-Ted relationship! For example, for season 2 the relationship Marshall-Lily-Ted becomes much stronger and it disappears in season 3. Let’s remember that these results might be affected by the fact that I excluded some episodes because of low number of dialogue lines. Keeping that in mind, we also see that for season 7 the Robin-Barney relationship became much stronger (is this the season the started dating?). All in all, the relationships don’t look dramatically different from the previous plot. Everyone seems to be strongly related to Ted. The main difference is the changes in relationship between the other members of the cast.</p>
<p>This dataset has a lot of potential and I’m sure I’ve scratched the surface of what one can do with this data. I encourage anyone interested in the topic to use the code to analyze the data further. One idea I might explore in the future is to build a model that attempts to predict who said what for all dialogue lines that didn’t have a character member. This can be done by extracting features from all sentences and using these patterns try to classify which. Any feedback is welcome, so feel free to message me at <a href="mailto:cimentadaj@gmail.com">cimentadaj@gmail.com</a></p>
]]>
      </description>
    </item>
    
    <item>
      <title>perccalc package</title>
      <link>https://cimentadaj.github.io/blog/2017-08-01-perccalc-package/perccalc-package/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2017-08-01-perccalc-package/perccalc-package/</guid>
      <description><![CDATA[
      


<p>Reardon (2011) introduced a very interesting concept in which he calculates percentile differences from ordered categorical variables. He explains his procedure very much in detail in the appendix of the book chapter but no formal implementation has been yet available on the web. With this package I introduce a function that applies the procedure, following a step-by-step Stata script that Sean Reardon kindly sent me.</p>
<p>In this vignette I show you how to use the function and match the results to the Stata code provided by Reardon himself.</p>
<p>For this example, we’ll use a real world data set, one I’m very much familiar with: PISA. We’ll use the PISA 2012 wave for Germany because it asked parents about their income category. For this example we’ll need the packages below.</p>
<pre class="r"><code># install.packages(c(&quot;devtools&quot;, &quot;matrixStats&quot;, &quot;tidyverse&quot;))
# devtools::install_github(&quot;pbiecek/PISA2012lite&quot;)

library(matrixStats)
library(tidyverse)
library(haven)
library(PISA2012lite)</code></pre>
<p>If you haven’t installed any of the packages above, uncomment the first two lines to install them. Beware that the <code>PISA2012lite</code> package contains the PISA 2012 data and takes a while to download.</p>
<p>Let’s prepare the data. Below we filter only German students, select only the math test results and calculate the median of all math plausible values to get one single math score. Finally, we match each student with their corresponding income data from their parents data and their sample weights.</p>
<pre class="r"><code>ger_student &lt;- student2012 %&gt;%
  filter(CNT == &quot;Germany&quot;) %&gt;%
  select(CNT, STIDSTD, matches(&quot;^PV*.MATH$&quot;)) %&gt;%
  transmute(CNT, STIDSTD,
            avg_score = rowMeans(student2012[student2012$CNT == &quot;Germany&quot;, paste0(&quot;PV&quot;, 1:5, &quot;MATH&quot;)]))

ger_parent &lt;-
  parent2012 %&gt;%
  filter(CNT == &quot;Germany&quot;) %&gt;%
  select(CNT, STIDSTD, PA07Q01)

ger_weights &lt;-
  student2012weights %&gt;%
  filter(CNT == &quot;Germany&quot;) %&gt;%
  select(CNT, STIDSTD, W_FSTUWT)

dataset_ready &lt;-
  ger_student %&gt;%
  left_join(ger_parent, by = c(&quot;CNT&quot;, &quot;STIDSTD&quot;)) %&gt;%
  left_join(ger_weights, by = c(&quot;CNT&quot;, &quot;STIDSTD&quot;)) %&gt;%
  as_tibble() %&gt;%
  rename(income = PA07Q01,
         score = avg_score,
         wt = W_FSTUWT) %&gt;%
  select(-CNT, -STIDSTD)</code></pre>
<p>The final results is this dataset:</p>
<pre><code>## # A tibble: 10 x 3
##   score income            wt
##   &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;
## 1  440. Less than &lt;$A&gt;  137.
## 2  523. Less than &lt;$A&gt;  170.
## 3  291. Less than &lt;$A&gt;  162.
## 4  437. Less than &lt;$A&gt;  162.
## 5  367. Less than &lt;$A&gt;  115.
## # ... with 5 more rows</code></pre>
<p>This is the minimum dataset that the function will accept. This means that it needs to have at least a categorical variable and a continuous variable (the vector of weights is optional).</p>
<p>The package is called <code>perccalc</code>, short for percentile calculator and we can install and load it with this code:</p>
<pre class="r"><code>install.packages(&quot;perccalc&quot;, repo = &quot;https://cran.rediris.es/&quot;)
## package &#39;perccalc&#39; successfully unpacked and MD5 sums checked
## 
## The downloaded binary packages are in
##  C:\Users\cimentadaj\AppData\Local\Temp\RtmpYFnDzN\downloaded_packages
library(perccalc)</code></pre>
<p>The package has two functions, which I’ll show some examples. The first one is called <code>perc_diff</code> and it’s very easy to use, we just specify the data, the name of the categorical and continuous variable and the percentile difference we want.</p>
<p>Let’s put it to use!</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, percentiles = c(90, 10))
## Error: is_ordered_fct is not TRUE</code></pre>
<p>I generated that error on purpose to raise a very important requirement of the function. The categorical variable needs to be an ordered factor (categorical). It is very important because otherwise we could be calculating percentile differences of categorical variables such as married, single and widowed, which doesn’t make a lot of sense.</p>
<p>We can turn it into an ordered factor with the code below.</p>
<pre class="r"><code>dataset_ready &lt;-
  dataset_ready %&gt;%
  mutate(income = factor(income, ordered = TRUE))</code></pre>
<p>Now it’ll work.</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, percentiles = c(90, 10))
## difference         se 
##   97.00706    8.74790</code></pre>
<p>We can play around with other percentiles</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, percentiles = c(50, 10))
## difference         se 
##  58.776200   8.291083</code></pre>
<p>And we can add a vector of weights</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902</code></pre>
<p>Now, how are we sure that these estimates are as accurate as the Reardon (2011) implementation? We can compare the Stata ouput using this data set.</p>
<pre class="r"><code># Saving the dataset to a path
dataset_ready %&gt;%
  write_dta(path = &quot;/Users/cimentadaj/Downloads/pisa_income.dta&quot;, version = 13)</code></pre>
<p>Running the code below using the <code>pisa_income.dta</code>..</p>
<pre class="r"><code>*--------
use &quot;/Users/cimentadaj/Downloads/pisa_income.dta&quot;, clear

tab income, gen(inc)
*--------

/*-----------------------
    Making a data set that has 
    one observation per income category
    and has mean and se(mean) in each category
    and percent of population in the category
------------------------*/

tempname memhold
tempfile results
postfile `memhold&#39; income mean se_mean per using `results&#39;

forv i = 1/6 {
    qui sum inc`i&#39; [aw=wt]
    loc per`i&#39; = r(mean)    
                                
    qui sum score if inc`i&#39;==1 
                            
    if `r(N)&#39;&gt;0 {
        qui regress score if inc`i&#39;==1 [aw=wt]
        post `memhold&#39; (`i&#39;) (_b[_cons]) (_se[_cons]) (`per`i&#39;&#39;)
                            
    }               
}
postclose `memhold&#39; 

/*-----------------------
    Making income categories
    into percentiles
------------------------*/


    use `results&#39;, clear

    sort income
    gen cathi = sum(per)
    gen catlo = cathi[_n-1]
    replace catlo = 0 if income==1
    gen catmid = (catlo+cathi)/2
    
    /*-----------------------
        Calculate income 
        achievement gaps
    ------------------------*/

    sort income
    
    g x1 = catmid
    g x2 = catmid^2 + ((cathi-catlo)^2)/12
    g x3 = catmid^3 + ((cathi-catlo)^2)/4

    g cimnhi = mean + 1.96*se_mean
    g cimnlo = mean - 1.96*se_mean

    reg mean x1 x2 x3 [aw=1/se_mean^2] 

    twoway (rcap cimnhi cimnlo catmid) (scatter mean catmid) ///
        (function y = _b[_cons] + _b[x1]*x + _b[x2]*x^2 + _b[x3]*x^3, ran(0 1)) 
    
    loc hi_p = 90
    loc lo_p = 10

    loc d1 = [`hi_p&#39; - `lo_p&#39;]/100
    loc d2 = [(`hi_p&#39;)^2 - (`lo_p&#39;)^2]/(100^2)
    loc d3 = [(`hi_p&#39;)^3 - (`lo_p&#39;)^3]/(100^3)

    lincom `d1&#39;*x1 + `d2&#39;*x2 + `d3&#39;*x3
    loc diff`hi_p&#39;`lo_p&#39; = r(estimate)
    loc se`hi_p&#39;`lo_p&#39; = r(se)
    
    di &quot;`hi_p&#39;-`lo_p&#39; gap:     `diff`hi_p&#39;`lo_p&#39;&#39;&quot;
    di &quot;se(`hi_p&#39;-`lo_p&#39; gap): `se`hi_p&#39;`lo_p&#39;&#39;&quot;</code></pre>
<p>I get that the 90/10 difference is <code>95.22</code> with a standard error of <code>8.45</code>. Does it sound familiar?</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902</code></pre>
<p>The second function of the package is called <code>perc_dist</code> and instead of calculating the difference of two percentiles, it returns the score and standard error of every percentile. The arguments of the function are exactly the same but without the <code>percentiles</code> argument, because this will return the whole set of percentiles.</p>
<pre class="r"><code>perc_dist(dataset_ready, income, score)
## # A tibble: 100 x 3
##   percentile estimate std.error
##        &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1          1     3.69      1.33
## 2          2     7.28      2.59
## 3          3    10.8       3.79
## 4          4    14.1       4.93
## 5          5    17.4       6.01
## # ... with 95 more rows</code></pre>
<p>We can also add the optional set of weights and graph it:</p>
<pre class="r"><code>perc_dist(dataset_ready, income, score, wt) %&gt;%
  mutate(ci_low = estimate - 1.96 * std.error,
         ci_hi = estimate + 1.96 * std.error) %&gt;%
  ggplot(aes(percentile, estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_hi))</code></pre>
<p><img src="/blog/2017-08-01-perccalc-package/2017-08-01-perccalc-package_files/figure-html/unnamed-chunk-15-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Please note that for calculating the difference between two percentiles it is more accurate to use the <code>perc_diff</code> function. The <code>perc_diff</code> calculates the difference through a linear combination of coefficients resulting in a different standard error.</p>
<p>For example:</p>
<pre class="r"><code>perc_dist(dataset_ready, income, score, wt) %&gt;%
  filter(percentile %in% c(90, 10)) %&gt;%
  summarize(diff = diff(estimate),
            se_diff = diff(std.error))
## # A tibble: 1 x 2
##    diff se_diff
##   &lt;dbl&gt;   &lt;dbl&gt;
## 1  95.2    5.68</code></pre>
<p>compared to</p>
<pre class="r"><code>perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902</code></pre>
<p>They both have the same point estimate but a different standard error.</p>
<p>I hope this was a convincing example, I know this will be useful for me. All the intelectual ideas come from Sean Reardon and the Stata code was written by Sean Reardon, Ximena Portilla, and Jenna Finch. The R implemention is my own work.</p>
<p>You can find the package repository <a href="https://github.com/cimentadaj/perccalc">here</a>.</p>
<ul>
<li>Reardon, Sean F. “The widening academic achievement gap between the rich and the poor: New evidence and possible explanations.” Whither opportunity (2011): 91-116.</li>
</ul>
]]>
      </description>
    </item>
    
    <item>
      <title>My PISA twitter bot</title>
      <link>https://cimentadaj.github.io/blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/</guid>
      <description><![CDATA[
      


<p>I’ve long wanted to prepare a project with R related to education. I knew I’d found the idea when I read Thomas Lumley’s <a href="http://notstatschat.tumblr.com/post/156007757906/a-bus-watching-bot">attempt to create a Twitter bot in which he tweeted bus arrivals in New Zealand</a>. Quoting him, “Is it really hard to write a bot? No. Even I can do it. And I’m old.”</p>
<p>So I said to myself, alright, you have to create a Twitter bot but it has to be related to education. It’s an easy project which shouldn’t take a lot of your time. I then came up with this idea: what if you could randomly sample questions from the <a href="http://www.oecd.org/pisa/aboutpisa/">PISA databases</a> and create a sort of random facts generator. The result would be one graph a day, showing a question for some random sample of countries. I figured, why not prepare a post (both for me to remember how I did it but also so others can contribute to the project) where I explained step-by-step how I did it?</p>
<p>The repository for the project is <a href="https://github.com/cimentadaj/PISAfacts_twitterBot">here</a>, so feel free to drop any comments or improvements. The idea is to load the <a href="http://vs-web-fs-1.oecd.org/pisa/PUF_SPSS_COMBINED_CMB_STU_QQQ.zip">PISA 2015 data</a>, randomly pick a question that doesn’t have a lot of labels (because then it’s very difficult to plot it nicely), and based on the type of question create an appropriate graph. Of course, all of this needs to be done on the fly, without human assistance. You can follow this twitter account at <span class="citation">@DailyPISA_Facts</span>. Let’s start!</p>
<div id="data-wrangling" class="section level2">
<h2>Data wrangling</h2>
<p>First we load some of the packages we’ll use and read the PISA 2015 student data.</p>
<pre class="r"><code>library(tidyverse)
library(forcats)
library(haven)
library(intsvy) # For correct estimation of PISA estimates
library(countrycode) # For countrycodes
library(cimentadaj) # devtools::install_github(&quot;cimentadaj/cimentadaj&quot;)
library(lazyeval)
library(ggthemes) # devtools::install_github(&quot;jrnold/ggthemes&quot;)</code></pre>
<pre class="r"><code>file_name &lt;- file.path(tempdir(), &quot;pisa.zip&quot;)

download.file(
  &quot;http://vs-web-fs-1.oecd.org/pisa/PUF_SPSS_COMBINED_CMB_STU_QQQ.zip&quot;,
  destfile = file_name
)

unzip(file_name, exdir = tempdir())

pisa_2015 &lt;- read_spss(file.path(tempdir(), &quot;CY6_MS_CMB_STU_QQQ.sav&quot;))</code></pre>
<p>Downloading the data takes a bit but make sure to download the zip file and unzip it as I’ve just outlined.</p>
<p>The idea is to generate a script that can be used with all PISA datasets, so at some point we should be able not only to randomly pick question but also randomly pick PISA surveys (PISA has been implemented since the year 2000 in three year intervals). We create some places holders for the variable country name, the format of the country names and the missing labels we want to ignore for each question (I think these labels should be the same across all surveys).</p>
<pre class="r"><code>country_var &lt;- &quot;cnt&quot;
country_types &lt;- &quot;iso3c&quot;

missing_labels &lt;- c(&quot;Valid Skip&quot;,
                    &quot;Not Reached&quot;,
                    &quot;Not Applicable&quot;,
                    &quot;Invalid&quot;,
                    &quot;No Response&quot;)

int_data &lt;- pisa_2015 # Create a safe copy of the data, since it takes about 2 mins to read.</code></pre>
<p>After this, I started doing some basic data manipulation. Each line is followed by a comment on why I did it.</p>
<pre class="r"><code>names(int_data) &lt;- tolower(names(int_data)) # It&#39;s easier to write variable names as lower case
int_data$region &lt;- countrycode(int_data[[country_var]], country_types, &quot;continent&quot;)
# Create a region variable to add regional colours to plots at some point.</code></pre>
<p>Most PISA datasets are in SPSS format, where the variable’s question has been written as a label. If you’ve used SPSS or SAS you know that labels are very common; they basically outline the question of that variable. In R, this didn’t properly exists until the <code>foreign</code> and <code>haven</code> package. With <code>read_spss()</code>, each variable has now two important attributes called <code>label</code> and <code>labels</code>. Respectively, the first one contains the question, while the second contains the value labels (assuming the file to be read has these labels). This information will be vital to our PISA bot. In fact, this script works only if the data has these two attributes. If you’re feeling particularly adventurous, you can fork this repository and make the script work also with metadata!</p>
<p>Have a look at the country names in <code>int_data[[country_var]][1:10]</code>. They’re all written as 3-letter country codes. But to our luck, the <code>labels</code> attribute has the correct names with the 3-letter equivalent. We can save these attributes and recode the 3-letter country name to long names.</p>
<pre class="r"><code># Saving country names to change 3 letter country name to long country names
country_labels &lt;- attr(int_data[[country_var]], &quot;labels&quot;)

# Reversing the 3-letter code to names so I can search for countries
# in a lookup table
country_names &lt;- reverse_name(country_labels)

# Lookup 3-letter code and change them for long country names
int_data[, country_var] &lt;- country_names[int_data[[country_var]]]
attr(int_data[[country_var]], &quot;labels&quot;) &lt;- country_labels</code></pre>
<p>Next thing I’d like to do is check which variables will be valid, i.e. those which have a <code>labels</code> attribute, have 2 or more <code>labels</code> aside from the <code>missing</code> category of labels and are not either characters or factors (remember that all variables should be numeric with an attribute that contains the labels; character columns are actually invalid here). This will give me the list of variables that I’ll be able to use.</p>
<pre class="r"><code>subset_vars &lt;- 
  int_data %&gt;%
  map_lgl(function(x)
    !is.null(attr(x, &quot;labels&quot;)) &amp;&amp;
    length(setdiff(names(attr(x, &quot;labels&quot;)), missing_labels)) &gt;= 2 &amp;&amp;
    !typeof(x) %in% c(&quot;character&quot;, &quot;factor&quot;)) %&gt;%
  which()</code></pre>
<p>Great, we have our vector of valid columns.</p>
<p>The next steps are fairly straight forward. I randomply sample one of those indexes (which have the variale name as a <code>names</code> attribute, check <code>subset_vars</code>), together with the <code>cnt</code> and <code>region</code> variables.</p>
<pre class="r"><code>valid_df_fun &lt;- function(data, vars_select) {
  data %&gt;%
  select_(&quot;cnt&quot;, &quot;region&quot;, sample(names(vars_select), 1)) %&gt;%
  as.data.frame()
}

valid_df &lt;- valid_df_fun(int_data, subset_vars)
random_countries &lt;- unique(valid_df$cnt) # To sample unique countries later on</code></pre>
<p>We also need to check how many labels we have, aside from the <code>missing</code> labels. In any case, if those unique labels have more than 5, we need to resample a new variable. It’s difficult to understand a plot with that many labels. We need to make our plots as simple and straightforward as possible.</p>
<pre class="r"><code>var_labels &lt;- attr(valid_df[[names(valid_df)[3]]], &#39;labels&#39;) # Get labels

# Get unique labels
valid_labels &lt;- function(variable_label, miss) {
  variable_label %&gt;%
    names() %&gt;%
    setdiff(miss)
}

len_labels &lt;- length(valid_labels(var_labels, missing_labels)) # length of unique labels

# While the length of the of the labels is &gt; 4, sample a new variable.
while (len_labels &gt; 4) {
  valid_df &lt;- valid_df_fun(int_data, subset_vars)
  var_labels &lt;- attr(valid_df[[names(valid_df)[3]]], &#39;labels&#39;) # Get labels
  len_labels &lt;- length(valid_labels(var_labels, missing_labels))
}

# Make 100% sure we get the results:
stopifnot(len_labels &lt;= 4)

(labels &lt;- reverse_name(var_labels)) 
# Reverse vector names to objects and viceversa for 
# later recoding.

var_name &lt;- names(valid_df)[3]</code></pre>
<p>Before estimating the <code>PISA</code> proportions, I want to create a record of all variables that have been used. Whenever a graph has something wrong we wanna know which variable it was, so we can reproduce the problem and fix it later in the future.</p>
<pre class="r"><code>new_var &lt;- paste(var_name, Sys.Date(), sep = &quot; - &quot;)
write_lines(new_var, path = &quot;./all_variables.txt&quot;, append = T) 
# I create an empty .txt file to write the vars</code></pre>
<p>Now comes the estimation section. Using the <code>pisa.table</code> function from the package <code>intsvy</code> we can correctly estimate the population proportions of any variable for any valid country. This table will be the core data behind our plot.</p>
<pre class="r"><code>try_df &lt;-
  valid_df %&gt;%
  filter(!is.na(region)) %&gt;%
  pisa.table(var_name, data = ., by = &quot;cnt&quot;) %&gt;%
  filter(complete.cases(.))</code></pre>
<p>Let’s check out the contents of <code>try_df</code>:</p>
<pre><code>##                   cnt pa039q01ta Freq Percentage Std.err.
## 1             Belgium          1 3972      85.16        0
## 2             Belgium          2  692      14.84        0
## 3               Chile          1 6062      96.88        0
## 4               Chile          2  195       3.12        0
## 5             Croatia          1 4353      81.17        0
## 6             Croatia          2 1010      18.83        0
## 7  Dominican Republic          1 4335      98.37        0
## 8  Dominican Republic          2   72       1.63        0
## 9              France          1 4463      84.46        0
## 10             France          2  821      15.54        0</code></pre>
<p>Great! To finish with the data, we simply need one more thing: to recode the value labels with the <code>labels</code> vector.</p>
<pre class="r"><code>try_df[var_name] &lt;- labels[try_df[, var_name]]</code></pre>
<p>Awesome. We have the data ready, more or less. Let’s produce a dirty plot to check how long the title is.</p>
<pre class="r"><code>title_question &lt;- attr(valid_df[, var_name], &#39;label&#39;)

ggplot(try_df, aes_string(names(try_df)[2], &quot;Percentage&quot;)) +
  geom_col() +
  xlab(title_question)</code></pre>
<p><img src="/blog/2017-03-08-my-pisa-twitter-bot/2017-03-08-my-pisa-twitter-bot_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><em>Note: Because the question is randomly sampled, you might be getting a short title. Rerun the script and eventually you’ll get a long one.</em></p>
<p>So, the question <em>might</em> have two problems. The wording is a bit confusing (something we can’t really do anything about because that’s how it’s written in the questionnaire) and it’s too long. For the second problem I created a function that cuts the title in an arbitrary cutoff point (based on experimental tests on how many letters fit into a ggplot coordinate plane) but it makes sure that the cutoff is not in the middle of a word, i.e. it searches for the closest end of a word.</p>
<pre class="r"><code>## Section: Get the title
cut &lt;- 60 # Arbitrary cutoff

# This function accepts a sentence (or better, a title) and cuts it between
# the start and cutoff arguments (just as substr).
# But if the cutoff is not an empty space it will search +-1 index by
# index from the cutoff point until it reaches
# the closest empty space. It will return from start to the new cutoff
sentence_cut &lt;- function(sentence, start, cutoff) {
  
  if (nchar(sentence) &lt;= cutoff) return(substr(sentence, start, cutoff))
  
  excerpt &lt;- substr(sentence, start, cutoff)
  actual_val &lt;- cutoff
  neg_val &lt;- pos_val &lt;- actual_val
  
  if (!substr(excerpt, actual_val, actual_val) == &quot; &quot;) {
    
    expr &lt;- c(substr(sentence, neg_val, neg_val) == &quot; &quot;, substr(sentence, pos_val, pos_val) == &quot; &quot;)
    
    while (!any(expr)) {
      neg_val &lt;- neg_val - 1
      pos_val &lt;- pos_val + 1
      
      expr &lt;- c(substr(sentence, neg_val, neg_val) == &quot; &quot;, substr(sentence, pos_val, pos_val) == &quot; &quot;)
    }
    
    cutoff &lt;- ifelse(which(expr) == 1, neg_val, pos_val)
    excerpt &lt;- substr(sentence, start, cutoff)
    return(excerpt)
    
  } else {
    
    return(excerpt)
    
  }
}

# How many lines in ggplot2 should this new title have? Based on the cut off
sentence_vecs &lt;- ceiling(nchar(title_question) / cut)

# Create an empty list with the length of `lines` of the title.
# In this list I&#39;ll paste the divided question and later paste them together
list_excerpts &lt;- replicate(sentence_vecs, vector(&quot;character&quot;, 0))</code></pre>
<p>Just to make sure our function works, let’s do some quick tests. Let’s create the sentence <code>This is my new sentence</code> and subset from index <code>1</code> to index <code>17</code>. Index <code>17</code> is the letter <code>e</code> from the word <code>sentence</code>, so we should cut the sentence to the closest space, in our case, <code>This is my new</code>.</p>
<pre class="r"><code>sentence_cut(&quot;This is my new sentence&quot;, 1, 17)</code></pre>
<pre><code>## [1] &quot;This is my new &quot;</code></pre>
<p>A more complicated test using <code>I want my sentence to be cut where no word is still running</code>. Let’s pick from index <code>19</code>, which is the space between <code>sentence</code> and <code>to</code>, the index <code>27</code>, which is the <code>u</code> of <code>cut</code>. Because the length to a space <code>-1 and +1</code> is the same both ways, the function always picks the shortest length as a defensive mechanism to long titles.</p>
<pre class="r"><code>sentence_cut(&quot;I want my sentence to be cut where no word is still running&quot;, 19, 27)</code></pre>
<pre><code>## [1] &quot; to be &quot;</code></pre>
<p>Now that we have the function ready, we have to automate the process so that the first line is cut, then the second line should start where the first line left off and so on.</p>
<pre class="r"><code>for (list_index in seq_along(list_excerpts)) {
  
  non_empty_list &lt;- Filter(f = function(x) !(is_empty(x)), list_excerpts)
  
  # If this is the first line, the start should 1, otherwise the sum of all characters
  # of previous lines
  start &lt;- ifelse(list_index == 1, 1, sum(map_dbl(non_empty_list, nchar)))
  
  # Because start gets updated every iteration, simply cut from start to start + cut
  # The appropriate exceptions are added when its the first line of the plot.
  list_excerpts[[list_index]] &lt;-
    sentence_cut(title_question, start, ifelse(list_index == 1, cut, start + cut))
}

final_title &lt;- paste(list_excerpts, collapse = &quot;\n&quot;)</code></pre>
<p>The above loop gives you a list with the title separate into N lines based on the cutoff point. For the ggplot title, we finish by collapsing the separate titles with the <code>\n</code> as the separator.</p>
<p>So, I wrapped all of this into this function:</p>
<pre class="r"><code>label_cutter &lt;- function(variable_labels, cut) {
  
  variable_label &lt;- unname(variable_labels)
  
  # This function accepts a sentence (or better, a title) and cuts it between
  # the start and cutoff arguments ( just as substr). But if the cutoff is not an empty space
  # it will search +-1 index by index from the cutoff point until it reaches
  # the closest empty space. It will return from start to the new cutoff
  sentence_cut &lt;- function(sentence, start, cutoff) {
    
    if (nchar(sentence) &lt;= cutoff) return(substr(sentence, start, cutoff))
    
    excerpt &lt;- substr(sentence, start, cutoff)
    actual_val &lt;- cutoff
    neg_val &lt;- pos_val &lt;- actual_val
    
    if (!substr(excerpt, actual_val, actual_val) == &quot; &quot;) {
      
      expr &lt;- c(substr(sentence, neg_val, neg_val) == &quot; &quot;, substr(sentence, pos_val, pos_val) == &quot; &quot;)
      
      while (!any(expr)) {
        neg_val &lt;- neg_val - 1
        pos_val &lt;- pos_val + 1
        
        expr &lt;- c(substr(sentence, neg_val, neg_val) == &quot; &quot;, substr(sentence, pos_val, pos_val) == &quot; &quot;)
      }
      
      cutoff &lt;- ifelse(which(expr) == 1, neg_val, pos_val)
      excerpt &lt;- substr(sentence, start, cutoff)
      return(excerpt)
      
    } else {
      
      return(excerpt)
      
    }
  }
  
  # How many lines should this new title have? Based on the cut off
  sentence_vecs &lt;- ceiling(nchar(variable_label) / cut)
  
  # Create an empty list with the amount of lines for the excerpts
  # to be stored.
  list_excerpts &lt;- replicate(sentence_vecs, vector(&quot;character&quot;, 0))
  
  for (list_index in seq_along(list_excerpts)) {
    
    non_empty_list &lt;- Filter(f = function(x) !(is_empty(x)), list_excerpts)
    
    # If this is the first line, the start should 1, otherwise the sum of all characters
    # of previous lines
    start &lt;- ifelse(list_index == 1, 1, sum(map_dbl(non_empty_list, nchar)))
    
    # Because start gets updated every iteration, simply cut from start to start + cut
    # The appropriate exceptions are added when its the first line of the plot.
    list_excerpts[[list_index]] &lt;-
      sentence_cut(variable_label, start, ifelse(list_index == 1, cut, start + cut))
  }
  
  final_title &lt;- paste(list_excerpts, collapse = &quot;\n&quot;)
  final_title
}</code></pre>
<p>The function accepts a string and a cut off point. It will automatically create new lines if needed and return the separated title based on the cutoff point. We apply this function over the title and the labels, to make sure everything is clean.</p>
<pre class="r"><code>final_title &lt;- label_cutter(title_question, 60)
labels &lt;- map_chr(labels, label_cutter, 35)</code></pre>
<p>Finally, as I’ve outlined above, each question should have less then four labels. I though that it might be a good idea if I created different graphs for different number of labels. For example, for the two label questions, I thought a simple dot plot might be a good idea —— the space between the dots will sum up to one making it quite intuitive. However, for three and four labels, I though of a cutomized dotplot.</p>
<p>At the time I was writing this bot I was learning object oriented programming, so I said to myself, why not create a generic function that generates different plots for different labels? First, I need to assign the data frame the appropriate class.</p>
<pre class="r"><code>label_class &lt;-
  c(&quot;2&quot; = &quot;labeltwo&quot;, &#39;3&#39; = &quot;labelthree&quot;, &#39;4&#39; = &quot;labelfour&quot;)[as.character(len_labels)]

class(try_df) &lt;- c(class(try_df), label_class)</code></pre>
<p>The generic function, together with its cousin functions, are located in the <code>ggplot_funs.R</code> script in the PISA bot repository linked in the beginning.</p>
<p>The idea is simple. Create a generic function that dispatches based on the class of the object.</p>
<pre class="r"><code>pisa_graph &lt;- function(data, y_title, fill_var) UseMethod(&quot;pisa_graph&quot;)</code></pre>
<pre class="r"><code>pisa_graph.labeltwo &lt;- function(data, y_title, fill_var) {
  
  dots &lt;- setNames(list(interp(~ fct_reorder2(x, y, z),
                               x = quote(cnt),
                               y = as.name(fill_var),
                               z = quote(Percentage))), &quot;cnt&quot;)
  # To make sure we can randomly sample a number lower than the length
  unique_cnt &lt;- length(unique(data$cnt))
  
  data %&gt;%
    filter(cnt %in% sample(unique(cnt), ifelse(unique_cnt &gt;= 15, 15, 10))) %&gt;%
    mutate_(.dots = dots) %&gt;%
    ggplot(aes(cnt, Percentage)) +
    geom_point(aes_string(colour = fill_var)) +
    labs(y = y_title, x = NULL) +
    scale_colour_discrete(name = NULL) +
    guides(colour = guide_legend(nrow = 1)) +
    scale_y_continuous(limits = c(0, 100),
                       breaks = seq(0, 100, 20),
                       labels = paste0(seq(0, 100, 20), &quot;%&quot;)) +
    coord_flip() +
    theme_minimal() +
    theme(legend.position = &quot;top&quot;)
}</code></pre>
<p>This is the graph for the <code>labeltwo</code> class. Using a work around for non-standard evaluation, I reorder the <code>x</code> axis. This took me some time to understand but it’s very easy once you’ve written two or three expressions. Create a list with the formula (this might be for <code>mutate</code>, <code>filter</code> or whatever <code>tidyverse</code> function) and <strong>rename</strong> the placeholders in the formula with the appropriate names. Make sure to name that list object with the new variable name you want for this variable. So, for my example, we’re creating a new variable called <code>cnt</code> that will be the same variable reordered by the <code>fill_var</code> and the <code>Percentage</code> variable.</p>
<p>After this, I just built a usual <code>ggplot2</code> object (although notice that I used <code>mutate_</code> instead of <code>mutate</code> for the non-standard evaluation).</p>
<p>If you’re interested in learning more about standard and non-standard evaluation, I found these resources very useful (<a href="http://www.carlboettiger.info/2015/02/06/fun-standardizing-non-standard-evaluation.html">here</a>, <a href="http://adv-r.had.co.nz/Computing-on-the-language.html">here</a> and <a href="https://cran.r-project.org/web/packages/lazyeval/vignettes/lazyeval.html">here</a>)</p>
<p>The generic for <code>labelthree</code> and <code>labelfour</code> are pretty much the same as the previous plot but using a slightly different <code>geom</code>. Have a look at the original file <a href="https://raw.githubusercontent.com/cimentadaj/PISAfacts_twitterBot/master/ggplot_funs.R">here</a></p>
<p>We’ll, we’re almost there. After this, we simply, <code>source</code> the <code>ggplot_funs.R</code> script and produce the plot.</p>
<pre class="r"><code>source(&quot;https://raw.githubusercontent.com/cimentadaj/PISAfacts_twitterBot/master/ggplot_funs.R&quot;)
pisa_graph(data = try_df,
             y_title = final_title,
             fill_var = var_name)</code></pre>
<p><img src="/blog/2017-03-08-my-pisa-twitter-bot/2017-03-08-my-pisa-twitter-bot_files/figure-html/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>file &lt;- tempfile()
ggsave(file, device = &quot;png&quot;)</code></pre>
</div>
<div id="setting-the-twitter-bot" class="section level2">
<h2>Setting the twitter bot</h2>
<p>The final part is automating the twitter bot. I followed <a href="https://www.r-bloggers.com/programming-a-twitter-bot-and-the-rescue-from-procrastination/">this</a> and <a href="http://www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/">this</a>. I won’t go into the specifics because I probably wouldn’t do justice to the the second post, but you have to create your account on Twitter, this will give you some keys that make sure you’re the right person. <em>You need to write these key-value pairs as environment variables</em> (follow the second post) and then delete them from your R script (they’re secret! You shouldn’t keep them on your script but on some folder on your computer). Finally, make sure you identify your twitter account and make your first tweet!</p>
<pre class="r"><code>library(twitteR) # devtools::install_github(&quot;geoffjentry/twitteR&quot;)
setwd(&quot;./folder_with_my_credentials/&quot;)

api_key             &lt;- Sys.getenv(&quot;twitter_api_key&quot;)
api_secret          &lt;- Sys.getenv(&quot;twitter_api_secret&quot;)
access_token        &lt;- Sys.getenv(&quot;twitter_access_token&quot;)
access_token_secret &lt;- Sys.getenv(&quot;twitter_access_token_secret&quot;)
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)

tweet(&quot;&quot;, mediaPath = file)
unlink(file)</code></pre>
<p>That’s it! The last line should create the<code>tweet</code>.</p>
</div>
<div id="automating-the-bot" class="section level2">
<h2>Automating the bot</h2>
<p>The only thing left to do is automate this to run every day. I’ll explain how I did it for OSx by following <a href="http://www.techradar.com/how-to/computing/apple/terminal-101-creating-cron-jobs-1305651">this</a> tutorial. You can find a Windows explanation in step 3 <a href="https://www.r-bloggers.com/programming-a-twitter-bot-and-the-rescue-from-procrastination/">here</a>.</p>
<p>First, we need to figure out the specific time we want to schedule the script. We define the time by filling out five stars:</p>
<p><code>*****</code></p>
<ul>
<li>The first asterisk is for specifying the minute of the run (0-59)</li>
<li>The second asterisk is for specifying the hour of the run (0-23)</li>
<li>The third asterisk is for specifying the day of the month for the run (1-31)</li>
<li>The fourth asterisk is for specifying the month of the run (1-12)</li>
<li>The fifth asterisk is for specifying the day of the week (where Sunday is equal to 0, up to Saturday is equal to 6)</li>
</ul>
<p>Taken from <a href="http://www.techradar.com/how-to/computing/apple/terminal-101-creating-cron-jobs-1305651">here</a></p>
<p>For example, let’s say we wanted to schedule the script for <code>3:00 pm</code> every day, then the combination would be <code>0 15 * * *</code>. If we wanted something every <code>15</code> minutes, then <code>15 * * * *</code> would do it. If we wanted to schedule the script for Mondays and Wednesdays at <code>15:00</code> and <code>17:00</code> respectively, then we would write <code>0 15,17 * * 1,3</code>. In this last example the <code>* *</code> are the placeholders for day of the month and month.</p>
<p>In my example, I want the script to run every weekday at <code>9:30</code> am, so my equivalent would be <code>30 9 * * 1-5</code>.</p>
<p>To begin, we type <code>env EDITOR=nano crontab -e</code> in the <code>terminal</code> to initiate the <code>cron</code> file that will run the script. Next, type our time schedule followed by the command that will run the script in R. The command is <code>RScript</code>. However, because your terminal might not know where <code>RScript</code> is we need to type the directory to where RScript is. Type <code>which RScript</code> in the terminal and you shall get something like <code>/usr/local/bin/RScript</code>. Then the expression would be something like <code>30 9 * * 1-5 /usr/local/bin/RScript path_to_your/script.R</code>. See <a href="https://support.rstudio.com/hc/en-us/articles/218012917-How-to-run-R-scripts-from-the-command-line">here</a> for the <code>RScript</code> explanation.</p>
<p>The whole sequence would be like this:</p>
<pre class="bash"><code>env EDITOR=nano crontab -e
30 9 * * 1-5 /usr/local/bin/RScript path_to_your/script.R</code></pre>
<p>To save the file, press Control + O (to write out the file), then enter to accept the file name, then
press Control + X (to exit nano). If all went well, then you should see “crontab: installing new crontab” without anything after that.</p>
<p>Aaaaaand that’s it! You now have a working script that will be run from Monday to Friday at 9:30 am. This script will read the PISA data, pick a random variable, make a graph and tweet it. You can follow this twitter account at <span class="citation">[@DailyPISA_Facts]</span>(<a href="https://twitter.com/DailyPISA_Facts" class="uri">https://twitter.com/DailyPISA_Facts</a>).</p>
<p>Hope this was useful!</p>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>Cognitive inequality around the world – Shiny app</title>
      <link>https://cimentadaj.github.io/blog/2016-12-12-cognitive-inequality-around-the-world--shiny-app/cognitive-inequality-around-the-world--shiny-app/</link>
      <pubDate>Mon, 12 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2016-12-12-cognitive-inequality-around-the-world--shiny-app/cognitive-inequality-around-the-world--shiny-app/</guid>
      <description><![CDATA[
      


<p>For the last month I’ve been working on this massive dataset that combines all PISA, TIMSS and PIRLS surveys into one major database. It has over 3 million students and over 2,000 variables, including student background and school and teacher information. I started playing around with it and ending up doing this: <a href="https://cimentadaj.shinyapps.io/shiny/" class="uri">https://cimentadaj.shinyapps.io/shiny/</a>. Feel free to check it out and drop any comments below.</p>
<p>If you want to contribute, <a href="https://github.com/cimentadaj/Inequality_Shinyapp">this</a> is the Github repository. I plan to keep adding some stuff to the app, including new surveys and automatic plot downloading, so don’t forget to check it out.</p>
]]>
      </description>
    </item>
    
  </channel>
</rss>
