<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Jorge Cimentada</title>
    <link>/tags/projects/</link>
    <description>Recent content in Projects on Jorge Cimentada</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 16 Oct 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/tags/projects/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Scraping and visualizing How I Met Your Mother</title>
      <link>/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</guid>
      <description>&lt;p&gt;How I Met Your Mother (HIMYM from here after) is a television series very similar to the classical ‘Friends’ series from the 90’s. Following the release of the &lt;a href=&#34;http://tidytextmining.com/&#34;&gt;tidy text&lt;/a&gt; book I was looking for a project in which I could apply these skills. I decided I would scrape all the transcripts from HIMYM and analyze patterns between characters. This post really took me to the limit in terms of web scraping and pattern matching, which was specifically what I wanted to improve in the first place. Let’s begin!&lt;/p&gt;
&lt;p&gt;My first task was whether there was any consistency in the URL’s that stored the transcripts. If you ever watched HIMYM, we know there’s around nine seasons, each one with about 22 episodes. This makes about 200 episodes give or take. It would be a big pain to manually write down 200 complicated URL’s. Luckily, there is a way of finding the 200 links without writing them down manually.&lt;/p&gt;
&lt;p&gt;First, we create the links for the 9 websites that contain all episodes (1 through season 9)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(tidyverse)
library(stringr)
library(tidytext)

main_url &amp;lt;- &amp;quot;http://transcripts.foreverdreaming.org&amp;quot;
all_pages &amp;lt;- paste0(&amp;quot;http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;amp;start=&amp;quot;, seq(0, 200, 25))
characters &amp;lt;- c(&amp;quot;ted&amp;quot;, &amp;quot;lily&amp;quot;, &amp;quot;marshall&amp;quot;, &amp;quot;barney&amp;quot;, &amp;quot;robin&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each of the URL’s of &lt;code&gt;all_pages&lt;/code&gt; contains all episodes for that season (so around 22 URL’s). I also picked the characters we’re gonna concentrate for now. From here the job is very easy. We create a function that reads each link and parses the section containing all links for that season. We can do that using &lt;a href=&#34;http://selectorgadget.com/.&#34;&gt;SelectorGadget&lt;/a&gt; to find the section we’re interested in. We then search for the &lt;code&gt;href&lt;/code&gt; attribute to grab all links in that attribute and finally create a tibble with each episode together with it’s link.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;episode_getter &amp;lt;- function(link) {
  title_reference &amp;lt;-
    link %&amp;gt;%
    read_html() %&amp;gt;%
    html_nodes(&amp;quot;.topictitle&amp;quot;) # Get the html node name with &amp;#39;selector gadget&amp;#39;
  
  episode_links &amp;lt;-
    title_reference %&amp;gt;%
    html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
    gsub(&amp;quot;^.&amp;quot;, &amp;quot;&amp;quot;, .) %&amp;gt;%
    paste0(main_url, .) %&amp;gt;%
    setNames(title_reference %&amp;gt;% html_text()) %&amp;gt;%
    enframe(name = &amp;quot;episode_name&amp;quot;, value = &amp;quot;link&amp;quot;)
  
  episode_links
}

all_episodes &amp;lt;- map_df(all_pages, episode_getter) # loop over all seasons and get all episode links
all_episodes$id &amp;lt;- 1:nrow(all_episodes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There we go! Now we have a very organized &lt;code&gt;tibble&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_episodes
# # A tibble: 208 x 3
#                      episode_name
#                             &amp;lt;chr&amp;gt;
#  1                  01x01 - Pilot
#  2         01x02 - Purple Giraffe
#  3 01x03 - Sweet Taste of Liberty
#  4    01x04 - Return of the Shirt
#  5           01x05 - Okay Awesome
#  6         01x06 - Slutty Pumpkin
#  7             01x07 - Matchmaker
#  8               01x08 - The Duel
#  9   01x09 - Belly Full of Turkey
# 10 01x10 - The Pineapple Incident
# # ... with 198 more rows, and 2 more variables: link &amp;lt;chr&amp;gt;, id &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The remaining part is to actually scrape the text from each episode. We can work that out for a single episode and then turn that into a function and apply for all episodes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;episode_fun &amp;lt;- function(file) {
  
  file %&amp;gt;%
    read_html() %&amp;gt;%
    html_nodes(&amp;quot;.postbody&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    str_split(&amp;quot;\n|\t&amp;quot;) %&amp;gt;%
    .[[1]] %&amp;gt;%
    data_frame(text = .) %&amp;gt;%
    filter(str_detect(text, &amp;quot;&amp;quot;), # Lots of empty spaces
           !str_detect(text, &amp;quot;^\\t&amp;quot;), # Lots of lines with \t to delete
           !str_detect(text, &amp;quot;^\\[.*\\]$&amp;quot;), # Text that start with brackets
           !str_detect(text, &amp;quot;^\\(.*\\)$&amp;quot;), # Text that starts with parenthesis
           str_detect(text, &amp;quot;^*.:&amp;quot;), # I want only lines with start with dialogue (:)
           !str_detect(text, &amp;quot;^ad&amp;quot;)) # Remove lines that start with ad (for &amp;#39;ads&amp;#39;, the link of google ads)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above function reads each episode, turns the html text into a data frame and organizes it clearly for text analysis. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;episode_fun(all_episodes$link[15])
# # A tibble: 195 x 1
#                                                                           text
#                                                                          &amp;lt;chr&amp;gt;
#  1 Ted from 2030: Kids, something you might not know about your Uncle Marshall
#  2                  &amp;quot;Ted: You don&amp;#39;t have to shout out \&amp;quot;poker\&amp;quot; when you win.&amp;quot;
#  3                                     Marshall: I know. It&amp;#39;s just fun to say.
#  4 &amp;quot;Ted from 2030: We all finally agreed Marshall should be running our game n
#  5 &amp;quot;Marshall: It&amp;#39;s called \&amp;quot;Marsh-gammon.\&amp;quot; It combines all the best features 
#  6                                               Robin: Backgammon, obviously.
#  7 &amp;quot;Marshall: No. Backgammon sucks. I took the only good part of backgammon, t
#  8                                     Lily: I&amp;#39;m so excited Victoria&amp;#39;s coming.
#  9                                   Robin: I&amp;#39;m going to go get another round.
# 10 Ted: Okay, I want to lay down some ground rules for tonight. Barney, I actu
# # ... with 185 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a data frame with only dialogue for each character. We need to apply that function to each episode and &lt;code&gt;bind&lt;/code&gt; everything together. We first apply the function to every episode.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_episodes$text &amp;lt;- map(all_episodes$link, episode_fun)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;text&lt;/code&gt; list-column is an organized list with text for each episode. However, manual inspection of some episodes actually denotes a small error that limits our analysis greatly. Among the main interests of this document is to study relationships and presence between characters. For that, we need each line of text to be accompanied by the character who said it. Unfortunately, some of these scripts don’t have that.&lt;/p&gt;
&lt;p&gt;For example, check any episode from season &lt;a href=&#34;http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;amp;start=175&#34;&gt;8&lt;/a&gt; and &lt;a href=&#34;http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;amp;start=200&#34;&gt;9&lt;/a&gt;. The writer didn’t write the dialogue and just rewrote the lines. There’s nothing we can do so far to improve that and we’ll be excluding these episodes. This pattern is also present in random episodes like in season 4 or season 6. We can exclude chapters based on the number of lines we parsed. On average, each of these episodes has about 200 lines of dialogue. Anything significantly lower, like 30 or 50 lines, is an episode which doesn’t have a lot of dialogue.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_episodes$count &amp;lt;- map_dbl(all_episodes$text, nrow)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can extend the previous &lt;code&gt;tibble&lt;/code&gt; to be a big more organized by separating the episode-season column into separate season and episo numbers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_episodes &amp;lt;-
  all_episodes %&amp;gt;%
  separate(episode_name, c(&amp;quot;season&amp;quot;, &amp;quot;episode&amp;quot;), &amp;quot;-&amp;quot;, extra = &amp;quot;merge&amp;quot;) %&amp;gt;%
  separate(season, c(&amp;quot;season&amp;quot;, &amp;quot;episode_number&amp;quot;), sep = &amp;quot;x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! We now have a very organized &lt;code&gt;tibble&lt;/code&gt; with all the information we need. Next step is to actually break down the lines into words and start looking for general patterns. We can do that by looping through all episodes that have over 100 lines (just an arbitrary threshold) and unnesting each line for each &lt;strong&gt;valid&lt;/strong&gt; character.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lines_characters &amp;lt;-
  map(filter(all_episodes, count &amp;gt; 100) %&amp;gt;% pull(text), ~ { 
    # only loop over episodes that have over 100 lines
    .x %&amp;gt;%
      separate(text, c(&amp;quot;character&amp;quot;, &amp;quot;text&amp;quot;), sep = &amp;quot;:&amp;quot;, extra = &amp;#39;merge&amp;#39;) %&amp;gt;%
      # separate character dialogue from actual dialogo
      unnest_tokens(character, character) %&amp;gt;%
      filter(str_detect(character, paste0(paste0(&amp;quot;^&amp;quot;, characters, &amp;quot;$&amp;quot;), collapse = &amp;quot;|&amp;quot;))) %&amp;gt;%
      # only count the lines of our chosen characters
      mutate(episode_lines_id = 1:nrow(.))
  }) %&amp;gt;%
  setNames(filter(all_episodes, count &amp;gt; 100) %&amp;gt;% # name according to episode
             unite(season_episode, season, episode_number, sep = &amp;quot;x&amp;quot;) %&amp;gt;%
             pull(season_episode)) %&amp;gt;%
  enframe() %&amp;gt;%
  unnest() %&amp;gt;%
  mutate(all_lines_id = 1:nrow(.))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, our text is sort of ready. Let’s remove some bad words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;words_per_character &amp;lt;-
  lines_characters %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;% # expand all sentences into words
  anti_join(stop_words) %&amp;gt;% # remove bad words
  filter(!word %in% characters) %&amp;gt;% # only select characters we&amp;#39;re interested
  arrange(name) %&amp;gt;%
  separate(name, c(&amp;quot;season&amp;quot;, &amp;quot;episode&amp;quot;), sep = &amp;quot;x&amp;quot;, remove = FALSE) %&amp;gt;%
  mutate(name = factor(name, ordered = TRUE),
         season = factor(season, ordered = TRUE),
         episode = factor(episode, ordered = TRUE)) %&amp;gt;%
  filter(season != &amp;quot;07&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just to make sure, let’s look at the &lt;code&gt;tibble&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;words_per_character
# # A tibble: 88,174 x 7
#      name season episode character episode_lines_id all_lines_id      word
#     &amp;lt;ord&amp;gt;  &amp;lt;ord&amp;gt;   &amp;lt;ord&amp;gt;     &amp;lt;chr&amp;gt;            &amp;lt;int&amp;gt;        &amp;lt;int&amp;gt;     &amp;lt;chr&amp;gt;
#  1 01x01      01     01   marshall                1            1      ring
#  2 01x01      01     01   marshall                1            1     marry
#  3 01x01      01     01        ted                2            2   perfect
#  4 01x01      01     01        ted                2            2   engaged
#  5 01x01      01     01        ted                2            2       pop
#  6 01x01      01     01        ted                2            2 champagne
#  7 01x01      01     01        ted                2            2     drink
#  8 01x01      01     01        ted                2            2     toast
#  9 01x01      01     01        ted                2            2   kitchen
# 10 01x01      01     01        ted                2            2     floor
# # ... with 88,164 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perfect! One row per word, per character, per episode with the id of the line of the word.&lt;/p&gt;
&lt;p&gt;Alright, let’s get our hands dirty. First, let visualize the presence of each character in terms of words over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Filtering position of first episode of all seasons to
# position the X axis in the next plot.
first_episodes &amp;lt;-
  all_episodes %&amp;gt;%
  filter(count &amp;gt; 100, episode_number == &amp;quot;01 &amp;quot;) %&amp;gt;%
  pull(id)

words_per_character %&amp;gt;%
  split(.$name) %&amp;gt;%
  setNames(1:length(.)) %&amp;gt;%
  enframe(name = &amp;quot;episode_id&amp;quot;) %&amp;gt;%
  unnest() %&amp;gt;%
  count(episode_id, character) %&amp;gt;%
  group_by(episode_id) %&amp;gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&amp;gt;%
  ggplot(aes(as.numeric(episode_id), perc, group = character, colour = character)) +
  geom_line() +
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  scale_colour_discrete(guide = FALSE) +
  scale_x_continuous(name = &amp;quot;Seasons&amp;quot;,
                     breaks = first_episodes, labels = paste0(&amp;quot;S&amp;quot;, 1:7)) +
  scale_y_continuous(name = &amp;quot;Percentage of words per episode&amp;quot;) +
  theme_minimal() +
  facet_wrap(~ character, ncol = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ted is clearly the character with the highest number of words per episode followed by Barney. Lily and Robin, the only two women have very low presence compared to the men. In fact, if one looks closely, Lily seemed to have decreased slightly over time, having an all time low in season 4. Marshall, Lily’s partner in the show, does have much lower presence than both Barney and Ted but he has been catching up over time.&lt;/p&gt;
&lt;p&gt;We also see an interesting pattern where Barney has a lot of peaks, suggesting that in some specific episodes he gains predominance, where Ted has an overall higher level of words per episode. And when Ted has peaks, it’s usually below its trend-line.&lt;/p&gt;
&lt;p&gt;Looking at the distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# devtools::install_github(&amp;quot;clauswilke/ggjoy&amp;quot;)
library(ggjoy)

words_per_character %&amp;gt;%
  split(.$name) %&amp;gt;%
  setNames(1:length(.)) %&amp;gt;%
  enframe(name = &amp;quot;episode_id&amp;quot;) %&amp;gt;%
  unnest() %&amp;gt;%
  count(season, episode_id, character) %&amp;gt;%
  group_by(episode_id) %&amp;gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&amp;gt;%
  ggplot(aes(x = perc, y = character, fill = character)) +
  geom_joy(scale = 0.85) +
  scale_fill_discrete(guide = F) +
  scale_y_discrete(name = NULL, expand=c(0.01, 0)) +
  scale_x_continuous(name = &amp;quot;Percentage of words&amp;quot;, expand=c(0.01, 0)) +
  ggtitle(&amp;quot;Percentage of words per season&amp;quot;) +
  facet_wrap(~ season, ncol = 7) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;we see the differences much clearer. For example, we see Barney’s peaks through out every season with Season 6 seeing a clear peak of 40%. On the other hand, we see that their distributions don’t change that much over time! Suggesting that the presence of each character is very similar in all seasons. Don’t get me wrong, there are differences like Lily in Season 2 and then in Season 6, but in overall terms the previous plot suggests no increase over seasons, and this plot suggests that between seasons, there’s not a lot of change in their distributions that affects the overall mean.&lt;/p&gt;
&lt;p&gt;If you’ve watched the TV series, you’ll remember Barney always repeating one similar trademark word: legendary! Although it is a bit cumbersome for us to count the number of occurrences of that sentence once we unnested each sentence, we can at least count the number of words per character and see whether some characters have particular words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count_words &amp;lt;-
  words_per_character %&amp;gt;%
  filter(!word %in% characters) %&amp;gt;%
  count(character, word, sort = TRUE)

count_words %&amp;gt;%
  group_by(character) %&amp;gt;%
  top_n(20) %&amp;gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see that a lot of the words we capture are actually nouns or expressions which are common to everyone, such as ‘yeah’, ‘hey’ or ‘time’. We can weight down commonly used words for other words which are important but don’t get repeated a lot. We can exclude those words using &lt;code&gt;bind_tf_idf()&lt;/code&gt;, which for each character decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection or corpus of documents (see 3.3 in &lt;a href=&#34;http://tidytextmining.com/tfidf.html&#34; class=&#34;uri&#34;&gt;http://tidytextmining.com/tfidf.html&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count_words %&amp;gt;%
  bind_tf_idf(word, character, n) %&amp;gt;%
  arrange(desc(tf_idf)) %&amp;gt;%
  group_by(character) %&amp;gt;%
  top_n(20) %&amp;gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now Barney has a very distinctive word usage, one particularly sexist with words such as couger, bang and tits. Also, we see the word legendary as the thirdly repeated word, something we were expecting! On the other hand, we see Ted with things like professor (him), aunt (because of aunt Lily and such).&lt;/p&gt;
&lt;p&gt;Knowing that Ted is the main character in the series is no surprise. To finish off, we’re interested in knowing which characters are related to each other. First, let’s turn the data frame into a suitable format.&lt;/p&gt;
&lt;p&gt;Here we turn all lines to lower case and check which characters are present in the text of each dialogue. The loop will return a vector of logicals whether there was a mention of any of the characters. For simplicity I exclude all lines where there is more than 1 mention of a character, that is, 2 or more characters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lines_characters &amp;lt;-
  lines_characters %&amp;gt;%
  mutate(text = str_to_lower(text))

rows_fil &amp;lt;-
  map(characters, ~ str_detect(lines_characters$text, .x)) %&amp;gt;%
  reduce(`+`) %&amp;gt;%
  ifelse(. &amp;gt;= 2, 0, .) # excluding sentences which have 2 or more mentions for now
  # ideally we would want to choose to count the number of mentions
  # per line or randomly choose another a person that was mentioned.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the rows that have a mention of another character, we subset only those rows. Then we want know which character was mentioned in which line. I loop through each line and test which character is present in that specific dialogue line. The loop returns the actual character name for each dialogue. Because we already filtered lines that &lt;strong&gt;have&lt;/strong&gt; a character name mentioned, the loop should return a vector of the same length.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;character_relation &amp;lt;-
  lines_characters %&amp;gt;%
  filter(as.logical(rows_fil)) %&amp;gt;%
  mutate(who_said_what =
           map_chr(.$text, ~ { # loop over all each line
             who_said_what &amp;lt;- map_lgl(characters, function(.y) str_detect(.x, .y))
             # loop over each character and check whether he/she was mentioned
             # in that line
             characters[who_said_what]
             # subset the character that matched
           }))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we plot the relationship using the &lt;code&gt;ggraph&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggraph)
library(igraph)

character_relation %&amp;gt;%
  count(character, who_said_what) %&amp;gt;%
  graph_from_data_frame() %&amp;gt;%
  ggraph(layout = &amp;quot;linear&amp;quot;, circular = TRUE) +
  geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                width = 2.5, show.legend = FALSE) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A very clear pattern emerges. There is a strong relationship between Robin and Barney towards Ted. In fact, their direct relationship is very weak, but both are very well connected to Ted. On the other hand, Marshall and Lily are also reasonably connected to Ted but with a weaker link. Both of them are indeed very connected, as should be expected since they were a couple in the TV series.&lt;/p&gt;
&lt;p&gt;We also see that the weakest members of the group are Robin and Barney with only strong bonds toward Ted but no strong relationship with the other from the group. Overall, there seems to be a division: Marshall and Lily hold a somewhat close relationship with each other and towards Ted and Barney and Robin tend to be related to Ted but no one else.&lt;/p&gt;
&lt;p&gt;As a follow-up question, is this pattern of relationships the same across all seasons? We can do that very quickly by filtering each season using the previous plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cowplot)

# Loop through each season
seasons &amp;lt;- paste0(0, 1:7)

all_season_plots &amp;lt;- lapply(seasons, function(season_num) {

  set.seed(2131)
  
  character_relation %&amp;gt;%
    # Extract the season number from the `name` column
    mutate(season = str_replace_all(character_relation$name, &amp;quot;x(.*)$&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;%
    filter(season == season_num) %&amp;gt;%
    count(character, who_said_what) %&amp;gt;%
    graph_from_data_frame() %&amp;gt;%
    ggraph(layout = &amp;quot;linear&amp;quot;, circular = TRUE) +
    geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                  width = 2.5, show.legend = FALSE) +
    geom_node_text(aes(label = name), repel = TRUE) +
    theme_void()
})

# Plot all graphs side-by-side
cowplot::plot_grid(plotlist = all_season_plots, labels = seasons)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are reasonable changes for all non-Ted relationship! For example, for season 2 the relationship Marshall-Lily-Ted becomes much stronger and it disappears in season 3. Let’s remember that these results might be affected by the fact that I excluded some episodes because of low number of dialogue lines. Keeping that in mind, we also see that for season 7 the Robin-Barney relationship became much stronger (is this the season the started dating?). All in all, the relationships don’t look dramatically different from the previous plot. Everyone seems to be strongly related to Ted. The main difference is the changes in relationship between the other members of the cast.&lt;/p&gt;
&lt;p&gt;This dataset has a lot of potential and I’m sure I’ve scratched the surface of what one can do with this data. I encourage anyone interested in the topic to use the code to analyze the data further. One idea I might explore in the future is to build a model that attempts to predict who said what for all dialogue lines that didn’t have a character member. This can be done by extracting features from all sentences and using these patterns try to classify which. Any feedback is welcome, so feel free to message me at &lt;a href=&#34;mailto:cimentadaj@gmail.com&#34;&gt;cimentadaj@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>perccalc package</title>
      <link>/blog/2017-08-01-perccalc-package/perccalc-package/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-08-01-perccalc-package/perccalc-package/</guid>
      <description>&lt;p&gt;Reardon (2011) introduced a very interesting concept in which he calculates percentile differences from ordered categorical variables. He explains his procedure very much in detail in the appendix of the book chapter but no formal implementation has been yet available on the web. With this package I introduce a function that applies the procedure, following a step-by-step Stata script that Sean Reardon kindly sent me.&lt;/p&gt;
&lt;p&gt;In this vignette I show you how to use the function and match the results to the Stata code provided by Reardon himself.&lt;/p&gt;
&lt;p&gt;For this example, we’ll use a real world data set, one I’m very much familiar with: PISA. We’ll use the PISA 2012 wave for Germany because it asked parents about their income category. For this example we’ll need the packages below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(c(&amp;quot;devtools&amp;quot;, &amp;quot;matrixStats&amp;quot;, &amp;quot;tidyverse&amp;quot;))
# devtools::install_github(&amp;quot;pbiecek/PISA2012lite&amp;quot;)

library(matrixStats)
library(tidyverse)
library(haven)
library(PISA2012lite)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you haven’t installed any of the packages above, uncomment the first two lines to install them. Beware that the &lt;code&gt;PISA2012lite&lt;/code&gt; package contains the PISA 2012 data and takes a while to download.&lt;/p&gt;
&lt;p&gt;Let’s prepare the data. Below we filter only German students, select only the math test results and calculate the median of all math plausible values to get one single math score. Finally, we match each student with their corresponding income data from their parents data and their sample weights.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ger_student &amp;lt;- student2012 %&amp;gt;%
  filter(CNT == &amp;quot;Germany&amp;quot;) %&amp;gt;%
  select(CNT, STIDSTD, matches(&amp;quot;^PV*.MATH$&amp;quot;)) %&amp;gt;%
  transmute(CNT, STIDSTD,
            avg_score = rowMeans(student2012[student2012$CNT == &amp;quot;Germany&amp;quot;, paste0(&amp;quot;PV&amp;quot;, 1:5, &amp;quot;MATH&amp;quot;)]))

ger_parent &amp;lt;-
  parent2012 %&amp;gt;%
  filter(CNT == &amp;quot;Germany&amp;quot;) %&amp;gt;%
  select(CNT, STIDSTD, PA07Q01)

ger_weights &amp;lt;-
  student2012weights %&amp;gt;%
  filter(CNT == &amp;quot;Germany&amp;quot;) %&amp;gt;%
  select(CNT, STIDSTD, W_FSTUWT)

dataset_ready &amp;lt;-
  ger_student %&amp;gt;%
  left_join(ger_parent, by = c(&amp;quot;CNT&amp;quot;, &amp;quot;STIDSTD&amp;quot;)) %&amp;gt;%
  left_join(ger_weights, by = c(&amp;quot;CNT&amp;quot;, &amp;quot;STIDSTD&amp;quot;)) %&amp;gt;%
  as_tibble() %&amp;gt;%
  rename(income = PA07Q01,
         score = avg_score,
         wt = W_FSTUWT) %&amp;gt;%
  select(-CNT, -STIDSTD)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final results is this dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##      score         income       wt
##      &amp;lt;dbl&amp;gt;         &amp;lt;fctr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 439.5622 Less than &amp;lt;$A&amp;gt; 137.3068
## 2 523.1422 Less than &amp;lt;$A&amp;gt; 170.0566
## 3 291.4083 Less than &amp;lt;$A&amp;gt; 162.3794
## 4 436.6023 Less than &amp;lt;$A&amp;gt; 162.3794
## 5 367.4326 Less than &amp;lt;$A&amp;gt; 114.6644
## # ... with 5 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the minimum dataset that the function will accept. This means that it needs to have at least a categorical variable and a continuous variable (the vector of weights is optional).&lt;/p&gt;
&lt;p&gt;The package is called &lt;code&gt;perccalc&lt;/code&gt;, short for percentile calculator and we can install and load it with this code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;perccalc&amp;quot;, repo = &amp;quot;https://cran.rediris.es/&amp;quot;)
## 
## The downloaded binary packages are in
##  /var/folders/w0/pscnb7zx5y9g_qf13cxhl0_r0000gn/T//RtmpYdxHZM/downloaded_packages
library(perccalc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The package has two functions, which I’ll show some examples. The first one is called &lt;code&gt;perc_diff&lt;/code&gt; and it’s very easy to use, we just specify the data, the name of the categorical and continuous variable and the percentile difference we want.&lt;/p&gt;
&lt;p&gt;Let’s put it to use!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, percentiles = c(90, 10))
## Error: is_ordered_fct is not TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I generated that error on purpose to raise a very important requirement of the function. The categorical variable needs to be an ordered factor (categorical). It is very important because otherwise we could be calculating percentile differences of categorical variables such as married, single and widowed, which doesn’t make a lot of sense.&lt;/p&gt;
&lt;p&gt;We can turn it into an ordered factor with the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset_ready &amp;lt;-
  dataset_ready %&amp;gt;%
  mutate(income = factor(income, ordered = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it’ll work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, percentiles = c(90, 10))
## difference         se 
##   97.00706    8.74790&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can play around with other percentiles&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, percentiles = c(50, 10))
## difference         se 
##  58.776200   8.291083&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can add a vector of weights&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, how are we sure that these estimates are as accurate as the Reardon (2011) implementation? We can compare the Stata ouput using this data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Saving the dataset to a path
dataset_ready %&amp;gt;%
  write_dta(path = &amp;quot;/Users/cimentadaj/Downloads/pisa_income.dta&amp;quot;, version = 13)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running the code below using the &lt;code&gt;pisa_income.dta&lt;/code&gt;..&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;*--------
use &amp;quot;/Users/cimentadaj/Downloads/pisa_income.dta&amp;quot;, clear

tab income, gen(inc)
*--------

/*-----------------------
    Making a data set that has 
    one observation per income category
    and has mean and se(mean) in each category
    and percent of population in the category
------------------------*/

tempname memhold
tempfile results
postfile `memhold&amp;#39; income mean se_mean per using `results&amp;#39;

forv i = 1/6 {
    qui sum inc`i&amp;#39; [aw=wt]
    loc per`i&amp;#39; = r(mean)    
                                
    qui sum score if inc`i&amp;#39;==1 
                            
    if `r(N)&amp;#39;&amp;gt;0 {
        qui regress score if inc`i&amp;#39;==1 [aw=wt]
        post `memhold&amp;#39; (`i&amp;#39;) (_b[_cons]) (_se[_cons]) (`per`i&amp;#39;&amp;#39;)
                            
    }               
}
postclose `memhold&amp;#39; 

/*-----------------------
    Making income categories
    into percentiles
------------------------*/


    use `results&amp;#39;, clear

    sort income
    gen cathi = sum(per)
    gen catlo = cathi[_n-1]
    replace catlo = 0 if income==1
    gen catmid = (catlo+cathi)/2
    
    /*-----------------------
        Calculate income 
        achievement gaps
    ------------------------*/

    sort income
    
    g x1 = catmid
    g x2 = catmid^2 + ((cathi-catlo)^2)/12
    g x3 = catmid^3 + ((cathi-catlo)^2)/4

    g cimnhi = mean + 1.96*se_mean
    g cimnlo = mean - 1.96*se_mean

    reg mean x1 x2 x3 [aw=1/se_mean^2] 

    twoway (rcap cimnhi cimnlo catmid) (scatter mean catmid) ///
        (function y = _b[_cons] + _b[x1]*x + _b[x2]*x^2 + _b[x3]*x^3, ran(0 1)) 
    
    loc hi_p = 90
    loc lo_p = 10

    loc d1 = [`hi_p&amp;#39; - `lo_p&amp;#39;]/100
    loc d2 = [(`hi_p&amp;#39;)^2 - (`lo_p&amp;#39;)^2]/(100^2)
    loc d3 = [(`hi_p&amp;#39;)^3 - (`lo_p&amp;#39;)^3]/(100^3)

    lincom `d1&amp;#39;*x1 + `d2&amp;#39;*x2 + `d3&amp;#39;*x3
    loc diff`hi_p&amp;#39;`lo_p&amp;#39; = r(estimate)
    loc se`hi_p&amp;#39;`lo_p&amp;#39; = r(se)
    
    di &amp;quot;`hi_p&amp;#39;-`lo_p&amp;#39; gap:     `diff`hi_p&amp;#39;`lo_p&amp;#39;&amp;#39;&amp;quot;
    di &amp;quot;se(`hi_p&amp;#39;-`lo_p&amp;#39; gap): `se`hi_p&amp;#39;`lo_p&amp;#39;&amp;#39;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I get that the 90/10 difference is &lt;code&gt;95.22&lt;/code&gt; with a standard error of &lt;code&gt;8.45&lt;/code&gt;. Does it sound familiar?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second function of the package is called &lt;code&gt;perc_dist&lt;/code&gt; and instead of calculating the difference of two percentiles, it returns the score and standard error of every percentile. The arguments of the function are exactly the same but without the &lt;code&gt;percentiles&lt;/code&gt; argument, because this will return the whole set of percentiles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_dist(dataset_ready, income, score)
##     percentile   estimate std.error
## 1            1   3.693889  1.327722
## 2            2   7.280584  2.591314
## 3            3  10.762009  3.792189
## 4            4  14.140090  4.931759
## 5            5  17.416754  6.011441
## 6            6  20.593925  7.032650
## 7            7  23.673529  7.996804
## 8            8  26.657492  8.905323
## 9            9  29.547739  9.759628
## 10          10  32.346196 10.561142
## 11          11  35.054789 11.311287
## 12          12  37.675443 12.011489
## 13          13  40.210083 12.663175
## 14          14  42.660636 13.267774
## 15          15  45.029026 13.826714
## 16          16  47.317180 14.341427
## 17          17  49.527023 14.813345
## 18          18  51.660481 15.243900
## 19          19  53.719479 15.634527
## 20          20  55.705943 15.986660
## 21          21  57.621798 16.301735
## 22          22  59.468970 16.581186
## 23          23  61.249385 16.826450
## 24          24  62.964968 17.038960
## 25          25  64.617644 17.220150
## 26          26  66.209340 17.371453
## 27          27  67.741982 17.494296
## 28          28  69.217493 17.590108
## 29          29  70.637801 17.660310
## 30          30  72.004830 17.706321
## 31          31  73.320507 17.729551
## 32          32  74.586757 17.731404
## 33          33  75.805505 17.713275
## 34          34  76.978678 17.676548
## 35          35  78.108200 17.622596
## 36          36  79.195997 17.552774
## 37          37  80.243995 17.468422
## 38          38  81.254120 17.370860
## 39          39  82.228297 17.261387
## 40          40  83.168451 17.141273
## 41          41  84.076509 17.011761
## 42          42  84.954395 16.874063
## 43          43  85.804036 16.729352
## 44          44  86.627357 16.578762
## 45          45  87.426284 16.423385
## 46          46  88.202741 16.264261
## 47          47  88.958656 16.102380
## 48          48  89.695953 15.938677
## 49          49  90.416558 15.774024
## 50          50  91.122396 15.609233
## 51          51  91.815394 15.445050
## 52          52  92.497476 15.282149
## 53          53  93.170569 15.121139
## 54          54  93.836598 14.962558
## 55          55  94.497488 14.806873
## 56          56  95.155165 14.654486
## 57          57  95.811555 14.505735
## 58          58  96.468584 14.360901
## 59          59  97.128176 14.220213
## 60          60  97.792258 14.083859
## 61          61  98.462754 13.951996
## 62          62  99.141592 13.824764
## 63          63  99.830695 13.702302
## 64          64 100.531991 13.584764
## 65          65 101.247404 13.472342
## 66          66 101.978860 13.365285
## 67          67 102.728285 13.263927
## 68          68 103.497604 13.168712
## 69          69 104.288742 13.080222
## 70          70 105.103627 12.999208
## 71          71 105.944182 12.926622
## 72          72 106.812333 12.863647
## 73          73 107.710007 12.811728
## 74          74 108.639129 12.772604
## 75          75 109.601624 12.748328
## 76          76 110.599418 12.741293
## 77          77 111.634436 12.754239
## 78          78 112.708605 12.790256
## 79          79 113.823849 12.852771
## 80          80 114.982095 12.945522
## 81          81 116.185267 13.072509
## 82          82 117.435292 13.237935
## 83          83 118.734096 13.446126
## 84          84 120.083602 13.701443
## 85          85 121.485739 14.008184
## 86          86 122.942430 14.370488
## 87          87 124.455601 14.792251
## 88          88 126.027178 15.277052
## 89          89 127.659088 15.828107
## 90          90 129.353254 16.448235
## 91          91 131.111603 17.139861
## 92          92 132.936061 17.905030
## 93          93 134.828552 18.745443
## 94          94 136.791004 19.662499
## 95          95 138.825340 20.657348
## 96          96 140.933487 21.730939
## 97          97 143.117371 22.884074
## 98          98 145.378916 24.117446
## 99          99 147.720049 25.431683
## 100        100 150.142695 26.827377&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also add the optional set of weights and graph it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_dist(dataset_ready, income, score, wt) %&amp;gt;%
  mutate(ci_low = estimate - 1.96 * std.error,
         ci_hi = estimate + 1.96 * std.error) %&amp;gt;%
  ggplot(aes(percentile, estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_hi))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-08-01-perccalc-package/2017-08-01-perccalc-package_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Please note that for calculating the difference between two percentiles it is more accurate to use the &lt;code&gt;perc_diff&lt;/code&gt; function. The &lt;code&gt;perc_diff&lt;/code&gt; calculates the difference through a linear combination of coefficients resulting in a different standard error.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_dist(dataset_ready, income, score, wt) %&amp;gt;%
  filter(percentile %in% c(90, 10)) %&amp;gt;%
  summarize(diff = diff(estimate),
            se_diff = diff(std.error))
##       diff  se_diff
## 1 95.22852 5.679855&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;compared to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They both have the same point estimate but a different standard error.&lt;/p&gt;
&lt;p&gt;I hope this was a convincing example, I know this will be useful for me. All the intelectual ideas come from Sean Reardon and the Stata code was written by Sean Reardon, Ximena Portilla, and Jenna Finch. The R implemention is my own work.&lt;/p&gt;
&lt;p&gt;You can find the package repository &lt;a href=&#34;https://github.com/cimentadaj/perccalc&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reardon, Sean F. “The widening academic achievement gap between the rich and the poor: New evidence and possible explanations.” Whither opportunity (2011): 91-116.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>My PISA twitter bot</title>
      <link>/blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/</guid>
      <description>&lt;p&gt;I’ve long wanted to prepare a project with R related to education. I knew I’d found the idea when I read Thomas Lumley’s &lt;a href=&#34;http://notstatschat.tumblr.com/post/156007757906/a-bus-watching-bot&#34;&gt;attempt to create a Twitter bot in which he tweeted bus arrivals in New Zealand&lt;/a&gt;. Quoting him, “Is it really hard to write a bot? No. Even I can do it. And I’m old.”&lt;/p&gt;
&lt;p&gt;So I said to myself, alright, you have to create a Twitter bot but it has to be related to education. It’s an easy project which shouldn’t take a lot of your time. I then came up with this idea: what if you could randomly sample questions from the &lt;a href=&#34;http://www.oecd.org/pisa/aboutpisa/&#34;&gt;PISA databases&lt;/a&gt; and create a sort of random facts generator. The result would be one graph a day, showing a question for some random sample of countries. I figured, why not prepare a post (both for me to remember how I did it but also so others can contribute to the project) where I explained step-by-step how I did it?&lt;/p&gt;
&lt;p&gt;The repository for the project is &lt;a href=&#34;https://github.com/cimentadaj/PISAfacts_twitterBot&#34;&gt;here&lt;/a&gt;, so feel free to drop any comments or improvements. The idea is to load the &lt;a href=&#34;http://vs-web-fs-1.oecd.org/pisa/PUF_SPSS_COMBINED_CMB_STU_QQQ.zip&#34;&gt;PISA 2015 data&lt;/a&gt;, randomly pick a question that doesn’t have a lot of labels (because then it’s very difficult to plot it nicely), and based on the type of question create an appropriate graph. Of course, all of this needs to be done on the fly, without human assistance. You can follow this twitter account at &lt;span class=&#34;citation&#34;&gt;@DailyPISA_Facts&lt;/span&gt;. Let’s start!&lt;/p&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data wrangling&lt;/h2&gt;
&lt;p&gt;First we load some of the packages we’ll use and read the PISA 2015 student data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(forcats)
library(haven)
library(intsvy) # For correct estimation of PISA estimates
library(countrycode) # For countrycodes
library(cimentadaj) # devtools::install_github(&amp;quot;cimentadaj/cimentadaj&amp;quot;)
library(lazyeval)
library(ggthemes) # devtools::install_github(&amp;quot;jrnold/ggthemes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;file_name &amp;lt;- file.path(tempdir(), &amp;quot;pisa.zip&amp;quot;)

download.file(
  &amp;quot;http://vs-web-fs-1.oecd.org/pisa/PUF_SPSS_COMBINED_CMB_STU_QQQ.zip&amp;quot;,
  destfile = file_name
)

unzip(file_name, exdir = tempdir())

pisa_2015 &amp;lt;- read_spss(file.path(tempdir(), &amp;quot;CY6_MS_CMB_STU_QQQ.sav&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In my system it takes about 2 minutes. Make sure to download the zip file from the link above to download the datasets. (We could’ve downloaded the data from R to a temporary file and then delete it but since we’ll be tweeting every single day I though that having the data in my system would be a smarter move)&lt;/p&gt;
&lt;p&gt;The idea is to generate a script that can be used with all PISA datasets, so at some point we should be able not only to randomly pick question but also randomly pick PISA surveys (PISA has been implemented since the year 2000 in three year intervals). We create some places holders for the variable country name, the format of the country names and the missing labels we want to ignore for each question (I think these labels should be the same across all surveys).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country_var &amp;lt;- &amp;quot;cnt&amp;quot;
country_types &amp;lt;- &amp;quot;iso3c&amp;quot;

missing_labels &amp;lt;- c(&amp;quot;Valid Skip&amp;quot;,
                    &amp;quot;Not Reached&amp;quot;,
                    &amp;quot;Not Applicable&amp;quot;,
                    &amp;quot;Invalid&amp;quot;,
                    &amp;quot;No Response&amp;quot;)

int_data &amp;lt;- pisa_2015 # Create a safe copy of the data, since it takes about 2 mins to read.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this, I started doing some basic data manipulation. Each line is followed by a comment on why I did it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(int_data) &amp;lt;- tolower(names(int_data)) # It&amp;#39;s easier to write variable names as lower case
int_data$region &amp;lt;- countrycode(int_data[[country_var]], country_types, &amp;quot;continent&amp;quot;)
# Create a region variable to add regional colours to plots at some point.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most PISA datasets are in SPSS format, where the variable’s question has been written as a label. If you’ve used SPSS or SAS you know that labels are very common; they basically outline the question of that variable. In R, this didn’t properly exists until the &lt;code&gt;foreign&lt;/code&gt; and &lt;code&gt;haven&lt;/code&gt; package. With &lt;code&gt;read_spss()&lt;/code&gt;, each variable has now two important attributes called &lt;code&gt;label&lt;/code&gt; and &lt;code&gt;labels&lt;/code&gt;. Respectively, the first one contains the question, while the second contains the value labels (assuming the file to be read has these labels). This information will be vital to our PISA bot. In fact, this script works only if the data has these two attributes. If you’re feeling particularly adventurous, you can fork this repository and make the script work also with metadata!&lt;/p&gt;
&lt;p&gt;Have a look at the country names in &lt;code&gt;int_data[[country_var]][1:10]&lt;/code&gt;. They’re all written as 3-letter country codes. But to our luck, the &lt;code&gt;labels&lt;/code&gt; attribute has the correct names with the 3-letter equivalent. We can save these attributes and recode the 3-letter country name to long names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Saving country names to change 3 letter country name to long country names
country_labels &amp;lt;- attr(int_data[[country_var]], &amp;quot;labels&amp;quot;)

# Reversing the 3-letter code to names so I can search for countries
# in a lookup table
country_names &amp;lt;- reverse_name(country_labels)

# Lookup 3-letter code and change them for long country names
int_data[, country_var] &amp;lt;- country_names[int_data[[country_var]]]
attr(int_data[[country_var]], &amp;quot;labels&amp;quot;) &amp;lt;- country_labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next thing I’d like to do is check which variables will be valid, i.e. those which have a &lt;code&gt;labels&lt;/code&gt; attribute, have 2 or more &lt;code&gt;labels&lt;/code&gt; aside from the &lt;code&gt;missing&lt;/code&gt; category of labels and are not either characters or factors (remember that all variables should be numeric with an attribute that contains the labels; character columns are actually invalid here). This will give me the list of variables that I’ll be able to use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset_vars &amp;lt;- 
  int_data %&amp;gt;%
  map_lgl(function(x)
    !is.null(attr(x, &amp;quot;labels&amp;quot;)) &amp;amp;&amp;amp;
    length(setdiff(names(attr(x, &amp;quot;labels&amp;quot;)), missing_labels)) &amp;gt;= 2 &amp;amp;&amp;amp;
    !typeof(x) %in% c(&amp;quot;character&amp;quot;, &amp;quot;factor&amp;quot;)) %&amp;gt;%
  which()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great, we have our vector of valid columns.&lt;/p&gt;
&lt;p&gt;The next steps are fairly straight forward. I randomply sample one of those indexes (which have the variale name as a &lt;code&gt;names&lt;/code&gt; attribute, check &lt;code&gt;subset_vars&lt;/code&gt;), together with the &lt;code&gt;cnt&lt;/code&gt; and &lt;code&gt;region&lt;/code&gt; variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;valid_df_fun &amp;lt;- function(data, vars_select) {
  data %&amp;gt;%
  select_(&amp;quot;cnt&amp;quot;, &amp;quot;region&amp;quot;, sample(names(vars_select), 1)) %&amp;gt;%
  as.data.frame()
}

valid_df &amp;lt;- valid_df_fun(int_data, subset_vars)
random_countries &amp;lt;- unique(valid_df$cnt) # To sample unique countries later on&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also need to check how many labels we have, aside from the &lt;code&gt;missing&lt;/code&gt; labels. In any case, if those unique labels have more than 5, we need to resample a new variable. It’s difficult to understand a plot with that many labels. We need to make our plots as simple and straightforward as possible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var_labels &amp;lt;- attr(valid_df[[names(valid_df)[3]]], &amp;#39;labels&amp;#39;) # Get labels

# Get unique labels
valid_labels &amp;lt;- function(variable_label, miss) {
  variable_label %&amp;gt;%
    names() %&amp;gt;%
    setdiff(miss)
}

len_labels &amp;lt;- length(valid_labels(var_labels, missing_labels)) # length of unique labels

# While the length of the of the labels is &amp;gt; 4, sample a new variable.
while (len_labels &amp;gt; 4) {
  valid_df &amp;lt;- valid_df_fun(int_data, subset_vars)
  var_labels &amp;lt;- attr(valid_df[[names(valid_df)[3]]], &amp;#39;labels&amp;#39;) # Get labels
  len_labels &amp;lt;- length(valid_labels(var_labels, missing_labels))
}

# Make 100% sure we get the results:
stopifnot(len_labels &amp;lt;= 4)

(labels &amp;lt;- reverse_name(var_labels)) 
# Reverse vector names to objects and viceversa for 
# later recoding.

var_name &amp;lt;- names(valid_df)[3]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before estimating the &lt;code&gt;PISA&lt;/code&gt; proportions, I want to create a record of all variables that have been used. Whenever a graph has something wrong we wanna know which variable it was, so we can reproduce the problem and fix it later in the future.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_var &amp;lt;- paste(var_name, Sys.Date(), sep = &amp;quot; - &amp;quot;)
write_lines(new_var, path = &amp;quot;./all_variables.txt&amp;quot;, append = T) 
# I create an empty .txt file to write the vars&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now comes the estimation section. Using the &lt;code&gt;pisa.table&lt;/code&gt; function from the package &lt;code&gt;intsvy&lt;/code&gt; we can correctly estimate the population proportions of any variable for any valid country. This table will be the core data behind our plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;try_df &amp;lt;-
  valid_df %&amp;gt;%
  filter(!is.na(region)) %&amp;gt;%
  pisa.table(var_name, data = ., by = &amp;quot;cnt&amp;quot;) %&amp;gt;%
  filter(complete.cases(.))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check out the contents of &lt;code&gt;try_df&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##          cnt st123q03na Freq Percentage Std.err.
## 1  Australia          1  327       2.34        0
## 2  Australia          2  962       6.90        0
## 3  Australia          3 5681      40.73        0
## 4  Australia          4 6979      50.03        0
## 5    Austria          1  151       2.18        0
## 6    Austria          2  391       5.65        0
## 7    Austria          3 1532      22.13        0
## 8    Austria          4 4849      70.04        0
## 9    Belgium          1  168       1.82        0
## 10   Belgium          2  568       6.15        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! To finish with the data, we simply need one more thing: to recode the value labels with the &lt;code&gt;labels&lt;/code&gt; vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;try_df[var_name] &amp;lt;- labels[try_df[, var_name]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Awesome. We have the data ready, more or less. Let’s produce a dirty plot to check how long the title is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;title_question &amp;lt;- attr(valid_df[, var_name], &amp;#39;label&amp;#39;)

ggplot(try_df, aes_string(names(try_df)[2], &amp;quot;Percentage&amp;quot;)) +
  geom_col() +
  xlab(title_question)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-03-08-my-pisa-twitter-bot/2017-03-08-my-pisa-twitter-bot_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Because the question is randomly sampled, you might be getting a short title. Rerun the script and eventually you’ll get a long one.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So, the question &lt;em&gt;might&lt;/em&gt; have two problems. The wording is a bit confusing (something we can’t really do anything about because that’s how it’s written in the questionnaire) and it’s too long. For the second problem I created a function that cuts the title in an arbitrary cutoff point (based on experimental tests on how many letters fit into a ggplot coordinate plane) but it makes sure that the cutoff is not in the middle of a word, i.e. it searches for the closest end of a word.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Section: Get the title
cut &amp;lt;- 60 # Arbitrary cutoff

# This function accepts a sentence (or better, a title) and cuts it between
# the start and cutoff arguments (just as substr).
# But if the cutoff is not an empty space it will search +-1 index by
# index from the cutoff point until it reaches
# the closest empty space. It will return from start to the new cutoff
sentence_cut &amp;lt;- function(sentence, start, cutoff) {
  
  if (nchar(sentence) &amp;lt;= cutoff) return(substr(sentence, start, cutoff))
  
  excerpt &amp;lt;- substr(sentence, start, cutoff)
  actual_val &amp;lt;- cutoff
  neg_val &amp;lt;- pos_val &amp;lt;- actual_val
  
  if (!substr(excerpt, actual_val, actual_val) == &amp;quot; &amp;quot;) {
    
    expr &amp;lt;- c(substr(sentence, neg_val, neg_val) == &amp;quot; &amp;quot;, substr(sentence, pos_val, pos_val) == &amp;quot; &amp;quot;)
    
    while (!any(expr)) {
      neg_val &amp;lt;- neg_val - 1
      pos_val &amp;lt;- pos_val + 1
      
      expr &amp;lt;- c(substr(sentence, neg_val, neg_val) == &amp;quot; &amp;quot;, substr(sentence, pos_val, pos_val) == &amp;quot; &amp;quot;)
    }
    
    cutoff &amp;lt;- ifelse(which(expr) == 1, neg_val, pos_val)
    excerpt &amp;lt;- substr(sentence, start, cutoff)
    return(excerpt)
    
  } else {
    
    return(excerpt)
    
  }
}

# How many lines in ggplot2 should this new title have? Based on the cut off
sentence_vecs &amp;lt;- ceiling(nchar(title_question) / cut)

# Create an empty list with the length of `lines` of the title.
# In this list I&amp;#39;ll paste the divided question and later paste them together
list_excerpts &amp;lt;- replicate(sentence_vecs, vector(&amp;quot;character&amp;quot;, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just to make sure our function works, let’s do some quick tests. Let’s create the sentence &lt;code&gt;This is my new sentence&lt;/code&gt; and subset from index &lt;code&gt;1&lt;/code&gt; to index &lt;code&gt;17&lt;/code&gt;. Index &lt;code&gt;17&lt;/code&gt; is the letter &lt;code&gt;e&lt;/code&gt; from the word &lt;code&gt;sentence&lt;/code&gt;, so we should cut the sentence to the closest space, in our case, &lt;code&gt;This is my new&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sentence_cut(&amp;quot;This is my new sentence&amp;quot;, 1, 17)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;This is my new &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A more complicated test using &lt;code&gt;I want my sentence to be cut where no word is still running&lt;/code&gt;. Let’s pick from index &lt;code&gt;19&lt;/code&gt;, which is the space between &lt;code&gt;sentence&lt;/code&gt; and &lt;code&gt;to&lt;/code&gt;, the index &lt;code&gt;27&lt;/code&gt;, which is the &lt;code&gt;u&lt;/code&gt; of &lt;code&gt;cut&lt;/code&gt;. Because the length to a space &lt;code&gt;-1 and +1&lt;/code&gt; is the same both ways, the function always picks the shortest length as a defensive mechanism to long titles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sentence_cut(&amp;quot;I want my sentence to be cut where no word is still running&amp;quot;, 19, 27)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot; to be &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the function ready, we have to automate the process so that the first line is cut, then the second line should start where the first line left off and so on.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (list_index in seq_along(list_excerpts)) {
  
  non_empty_list &amp;lt;- Filter(f = function(x) !(is_empty(x)), list_excerpts)
  
  # If this is the first line, the start should 1, otherwise the sum of all characters
  # of previous lines
  start &amp;lt;- ifelse(list_index == 1, 1, sum(map_dbl(non_empty_list, nchar)))
  
  # Because start gets updated every iteration, simply cut from start to start + cut
  # The appropriate exceptions are added when its the first line of the plot.
  list_excerpts[[list_index]] &amp;lt;-
    sentence_cut(title_question, start, ifelse(list_index == 1, cut, start + cut))
}

final_title &amp;lt;- paste(list_excerpts, collapse = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above loop gives you a list with the title separate into N lines based on the cutoff point. For the ggplot title, we finish by collapsing the separate titles with the &lt;code&gt;\n&lt;/code&gt; as the separator.&lt;/p&gt;
&lt;p&gt;So, I wrapped all of this into this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;label_cutter &amp;lt;- function(variable_labels, cut) {
  
  variable_label &amp;lt;- unname(variable_labels)
  
  # This function accepts a sentence (or better, a title) and cuts it between
  # the start and cutoff arguments ( just as substr). But if the cutoff is not an empty space
  # it will search +-1 index by index from the cutoff point until it reaches
  # the closest empty space. It will return from start to the new cutoff
  sentence_cut &amp;lt;- function(sentence, start, cutoff) {
    
    if (nchar(sentence) &amp;lt;= cutoff) return(substr(sentence, start, cutoff))
    
    excerpt &amp;lt;- substr(sentence, start, cutoff)
    actual_val &amp;lt;- cutoff
    neg_val &amp;lt;- pos_val &amp;lt;- actual_val
    
    if (!substr(excerpt, actual_val, actual_val) == &amp;quot; &amp;quot;) {
      
      expr &amp;lt;- c(substr(sentence, neg_val, neg_val) == &amp;quot; &amp;quot;, substr(sentence, pos_val, pos_val) == &amp;quot; &amp;quot;)
      
      while (!any(expr)) {
        neg_val &amp;lt;- neg_val - 1
        pos_val &amp;lt;- pos_val + 1
        
        expr &amp;lt;- c(substr(sentence, neg_val, neg_val) == &amp;quot; &amp;quot;, substr(sentence, pos_val, pos_val) == &amp;quot; &amp;quot;)
      }
      
      cutoff &amp;lt;- ifelse(which(expr) == 1, neg_val, pos_val)
      excerpt &amp;lt;- substr(sentence, start, cutoff)
      return(excerpt)
      
    } else {
      
      return(excerpt)
      
    }
  }
  
  # How many lines should this new title have? Based on the cut off
  sentence_vecs &amp;lt;- ceiling(nchar(variable_label) / cut)
  
  # Create an empty list with the amount of lines for the excerpts
  # to be stored.
  list_excerpts &amp;lt;- replicate(sentence_vecs, vector(&amp;quot;character&amp;quot;, 0))
  
  for (list_index in seq_along(list_excerpts)) {
    
    non_empty_list &amp;lt;- Filter(f = function(x) !(is_empty(x)), list_excerpts)
    
    # If this is the first line, the start should 1, otherwise the sum of all characters
    # of previous lines
    start &amp;lt;- ifelse(list_index == 1, 1, sum(map_dbl(non_empty_list, nchar)))
    
    # Because start gets updated every iteration, simply cut from start to start + cut
    # The appropriate exceptions are added when its the first line of the plot.
    list_excerpts[[list_index]] &amp;lt;-
      sentence_cut(variable_label, start, ifelse(list_index == 1, cut, start + cut))
  }
  
  final_title &amp;lt;- paste(list_excerpts, collapse = &amp;quot;\n&amp;quot;)
  final_title
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function accepts a string and a cut off point. It will automatically create new lines if needed and return the separated title based on the cutoff point. We apply this function over the title and the labels, to make sure everything is clean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_title &amp;lt;- label_cutter(title_question, 60)
labels &amp;lt;- map_chr(labels, label_cutter, 35)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, as I’ve outlined above, each question should have less then four labels. I though that it might be a good idea if I created different graphs for different number of labels. For example, for the two label questions, I thought a simple dot plot might be a good idea —— the space between the dots will sum up to one making it quite intuitive. However, for three and four labels, I though of a cutomized dotplot.&lt;/p&gt;
&lt;p&gt;At the time I was writing this bot I was learning object oriented programming, so I said to myself, why not create a generic function that generates different plots for different labels? First, I need to assign the data frame the appropriate class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;label_class &amp;lt;-
  c(&amp;quot;2&amp;quot; = &amp;quot;labeltwo&amp;quot;, &amp;#39;3&amp;#39; = &amp;quot;labelthree&amp;quot;, &amp;#39;4&amp;#39; = &amp;quot;labelfour&amp;quot;)[as.character(len_labels)]

class(try_df) &amp;lt;- c(class(try_df), label_class)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The generic function, together with its cousin functions, are located in the &lt;code&gt;ggplot_funs.R&lt;/code&gt; script in the PISA bot repository linked in the beginning.&lt;/p&gt;
&lt;p&gt;The idea is simple. Create a generic function that dispatches based on the class of the object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pisa_graph &amp;lt;- function(data, y_title, fill_var) UseMethod(&amp;quot;pisa_graph&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pisa_graph.labeltwo &amp;lt;- function(data, y_title, fill_var) {
  
  dots &amp;lt;- setNames(list(interp(~ fct_reorder2(x, y, z),
                               x = quote(cnt),
                               y = as.name(fill_var),
                               z = quote(Percentage))), &amp;quot;cnt&amp;quot;)
  # To make sure we can randomly sample a number lower than the length
  unique_cnt &amp;lt;- length(unique(data$cnt))
  
  data %&amp;gt;%
    filter(cnt %in% sample(unique(cnt), ifelse(unique_cnt &amp;gt;= 15, 15, 10))) %&amp;gt;%
    mutate_(.dots = dots) %&amp;gt;%
    ggplot(aes(cnt, Percentage)) +
    geom_point(aes_string(colour = fill_var)) +
    labs(y = y_title, x = NULL) +
    scale_colour_discrete(name = NULL) +
    guides(colour = guide_legend(nrow = 1)) +
    scale_y_continuous(limits = c(0, 100),
                       breaks = seq(0, 100, 20),
                       labels = paste0(seq(0, 100, 20), &amp;quot;%&amp;quot;)) +
    coord_flip() +
    theme_minimal() +
    theme(legend.position = &amp;quot;top&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the graph for the &lt;code&gt;labeltwo&lt;/code&gt; class. Using a work around for non-standard evaluation, I reorder the &lt;code&gt;x&lt;/code&gt; axis. This took me some time to understand but it’s very easy once you’ve written two or three expressions. Create a list with the formula (this might be for &lt;code&gt;mutate&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt; or whatever &lt;code&gt;tidyverse&lt;/code&gt; function) and &lt;strong&gt;rename&lt;/strong&gt; the placeholders in the formula with the appropriate names. Make sure to name that list object with the new variable name you want for this variable. So, for my example, we’re creating a new variable called &lt;code&gt;cnt&lt;/code&gt; that will be the same variable reordered by the &lt;code&gt;fill_var&lt;/code&gt; and the &lt;code&gt;Percentage&lt;/code&gt; variable.&lt;/p&gt;
&lt;p&gt;After this, I just built a usual &lt;code&gt;ggplot2&lt;/code&gt; object (although notice that I used &lt;code&gt;mutate_&lt;/code&gt; instead of &lt;code&gt;mutate&lt;/code&gt; for the non-standard evaluation).&lt;/p&gt;
&lt;p&gt;If you’re interested in learning more about standard and non-standard evaluation, I found these resources very useful (&lt;a href=&#34;http://www.carlboettiger.info/2015/02/06/fun-standardizing-non-standard-evaluation.html&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://adv-r.had.co.nz/Computing-on-the-language.html&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/lazyeval/vignettes/lazyeval.html&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The generic for &lt;code&gt;labelthree&lt;/code&gt; and &lt;code&gt;labelfour&lt;/code&gt; are pretty much the same as the previous plot but using a slightly different &lt;code&gt;geom&lt;/code&gt;. Have a look at the original file &lt;a href=&#34;https://raw.githubusercontent.com/cimentadaj/PISAfacts_twitterBot/master/ggplot_funs.R&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We’ll, we’re almost there. After this, we simply, &lt;code&gt;source&lt;/code&gt; the &lt;code&gt;ggplot_funs.R&lt;/code&gt; script and produce the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;https://raw.githubusercontent.com/cimentadaj/PISAfacts_twitterBot/master/ggplot_funs.R&amp;quot;)
pisa_graph(data = try_df,
             y_title = final_title,
             fill_var = var_name)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-03-08-my-pisa-twitter-bot/2017-03-08-my-pisa-twitter-bot_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;file &amp;lt;- tempfile()
ggsave(file, device = &amp;quot;png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-the-twitter-bot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting the twitter bot&lt;/h2&gt;
&lt;p&gt;The final part is automating the twitter bot. I followed &lt;a href=&#34;https://www.r-bloggers.com/programming-a-twitter-bot-and-the-rescue-from-procrastination/&#34;&gt;this&lt;/a&gt; and &lt;a href=&#34;http://www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/&#34;&gt;this&lt;/a&gt;. I won’t go into the specifics because I probably wouldn’t do justice to the the second post, but you have to create your account on Twitter, this will give you some keys that make sure you’re the right person. &lt;em&gt;You need to write these key-value pairs as environment variables&lt;/em&gt; (follow the second post) and then delete them from your R script (they’re secret! You shouldn’t keep them on your script but on some folder on your computer). Finally, make sure you identify your twitter account and make your first tweet!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(twitteR) # devtools::install_github(&amp;quot;geoffjentry/twitteR&amp;quot;)
setwd(&amp;quot;./folder_with_my_credentials/&amp;quot;)

api_key             &amp;lt;- Sys.getenv(&amp;quot;twitter_api_key&amp;quot;)
api_secret          &amp;lt;- Sys.getenv(&amp;quot;twitter_api_secret&amp;quot;)
access_token        &amp;lt;- Sys.getenv(&amp;quot;twitter_access_token&amp;quot;)
access_token_secret &amp;lt;- Sys.getenv(&amp;quot;twitter_access_token_secret&amp;quot;)
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)

tweet(&amp;quot;&amp;quot;, mediaPath = file)
unlink(file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it! The last line should create the&lt;code&gt;tweet&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automating-the-bot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Automating the bot&lt;/h2&gt;
&lt;p&gt;The only thing left to do is automate this to run every day. I’ll explain how I did it for OSx by following &lt;a href=&#34;http://www.techradar.com/how-to/computing/apple/terminal-101-creating-cron-jobs-1305651&#34;&gt;this&lt;/a&gt; tutorial. You can find a Windows explanation in step 3 &lt;a href=&#34;https://www.r-bloggers.com/programming-a-twitter-bot-and-the-rescue-from-procrastination/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, we need to figure out the specific time we want to schedule the script. We define the time by filling out five stars:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;*****&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first asterisk is for specifying the minute of the run (0-59)&lt;/li&gt;
&lt;li&gt;The second asterisk is for specifying the hour of the run (0-23)&lt;/li&gt;
&lt;li&gt;The third asterisk is for specifying the day of the month for the run (1-31)&lt;/li&gt;
&lt;li&gt;The fourth asterisk is for specifying the month of the run (1-12)&lt;/li&gt;
&lt;li&gt;The fifth asterisk is for specifying the day of the week (where Sunday is equal to 0, up to Saturday is equal to 6)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Taken from &lt;a href=&#34;http://www.techradar.com/how-to/computing/apple/terminal-101-creating-cron-jobs-1305651&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For example, let’s say we wanted to schedule the script for &lt;code&gt;3:00 pm&lt;/code&gt; every day, then the combination would be &lt;code&gt;0 15 * * *&lt;/code&gt;. If we wanted something every &lt;code&gt;15&lt;/code&gt; minutes, then &lt;code&gt;15 * * * *&lt;/code&gt; would do it. If we wanted to schedule the script for Mondays and Wednesdays at &lt;code&gt;15:00&lt;/code&gt; and &lt;code&gt;17:00&lt;/code&gt; respectively, then we would write &lt;code&gt;0 15,17 * * 1,3&lt;/code&gt;. In this last example the &lt;code&gt;* *&lt;/code&gt; are the placeholders for day of the month and month.&lt;/p&gt;
&lt;p&gt;In my example, I want the script to run every weekday at &lt;code&gt;9:30&lt;/code&gt; am, so my equivalent would be &lt;code&gt;30 9 * * 1-5&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To begin, we type &lt;code&gt;env EDITOR=nano crontab -e&lt;/code&gt; in the &lt;code&gt;terminal&lt;/code&gt; to initiate the &lt;code&gt;cron&lt;/code&gt; file that will run the script. Next, type our time schedule followed by the command that will run the script in R. The command is &lt;code&gt;RScript&lt;/code&gt;. However, because your terminal might not know where &lt;code&gt;RScript&lt;/code&gt; is we need to type the directory to where RScript is. Type &lt;code&gt;which RScript&lt;/code&gt; in the terminal and you shall get something like &lt;code&gt;/usr/local/bin/RScript&lt;/code&gt;. Then the expression would be something like &lt;code&gt;30 9 * * 1-5 /usr/local/bin/RScript path_to_your/script.R&lt;/code&gt;. See &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/218012917-How-to-run-R-scripts-from-the-command-line&#34;&gt;here&lt;/a&gt; for the &lt;code&gt;RScript&lt;/code&gt; explanation.&lt;/p&gt;
&lt;p&gt;The whole sequence would be like this:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;env EDITOR=nano crontab -e
30 9 * * 1-5 /usr/local/bin/RScript path_to_your/script.R&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To save the file, press Control + O (to write out the file), then enter to accept the file name, then press Control + X (to exit nano). If all went well, then you should see “crontab: installing new crontab” without anything after that.&lt;/p&gt;
&lt;p&gt;Aaaaaand that’s it! You now have a working script that will be run from Monday to Friday at 9:30 am. This script will read the PISA data, pick a random variable, make a graph and tweet it. You can follow this twitter account at &lt;span class=&#34;citation&#34;&gt;[@DailyPISA_Facts]&lt;/span&gt;(&lt;a href=&#34;https://twitter.com/DailyPISA_Facts&#34; class=&#34;uri&#34;&gt;https://twitter.com/DailyPISA_Facts&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Hope this was useful!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cognitive inequality around the world – Shiny app</title>
      <link>/blog/2016-12-12-cognitive-inequality-around-the-world--shiny-app/cognitive-inequality-around-the-world--shiny-app/</link>
      <pubDate>Mon, 12 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-12-12-cognitive-inequality-around-the-world--shiny-app/cognitive-inequality-around-the-world--shiny-app/</guid>
      <description>&lt;p&gt;For the last month I’ve been working on this massive dataset that combines all PISA, TIMSS and PIRLS surveys into one major database. It has over 3 million students and over 2,000 variables, including student background and school and teacher information. I started playing around with it and ending up doing this: &lt;a href=&#34;https://cimentadaj.shinyapps.io/shiny/&#34; class=&#34;uri&#34;&gt;https://cimentadaj.shinyapps.io/shiny/&lt;/a&gt;. Feel free to check it out and drop any comments below.&lt;/p&gt;
&lt;p&gt;If you want to contribute, &lt;a href=&#34;https://github.com/cimentadaj/Inequality_Shinyapp&#34;&gt;this&lt;/a&gt; is the Github repository. I plan to keep adding some stuff to the app, including new surveys and automatic plot downloading, so don’t forget to check it out.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
