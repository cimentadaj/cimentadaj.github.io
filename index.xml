<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jorge Cimentada</title>
    <link>/</link>
    <description>Recent content on Jorge Cimentada</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Thu, 05 Apr 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Login in, scraping and hidden fields</title>
      <link>/blog/2018-04-05-login-in-scraping-and-hidden-fields/login-in-scraping-and-hidden-fields/</link>
      <pubDate>Thu, 05 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-04-05-login-in-scraping-and-hidden-fields/login-in-scraping-and-hidden-fields/</guid>
      <description>&lt;p&gt;Lightning post. Earlier today I was trying to scrape the emails from all the PhD candidates in my program and I had to log in from our ‘Aula Global’. I did so using &lt;code&gt;httr&lt;/code&gt; but something was off: I introduced both my username and password but the website did not log in. Apparently, when loging in through &lt;code&gt;POST&lt;/code&gt;, sometimes there’s a thing call hidden fields that you need to fill out! I would’ve never though about this. Below is a case study, that excludes my credentials.&lt;/p&gt;
&lt;p&gt;The first thing we have to do is identify the &lt;code&gt;POST&lt;/code&gt; method and the inputs to the request. Using Google Chrome, go to the website &lt;a href=&#34;https://sso.upf.edu/CAS/index.php/login?service=https%3A%2F%2Faulaglobal.upf.edu%2Flogin%2Findex.php&#34;&gt;https://sso.upf.edu/CAS/index.php/login?service=https%3A%2F%2Faulaglobal.upf.edu%2Flogin%2Findex.php&lt;/a&gt; and then on the Google Chrome menu go to -&amp;gt; Settings -&amp;gt; More tools -&amp;gt; Developer tools. Here we have the complete html of the website.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We identify the POST method and the URL&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- &lt;img src=&#34;/img/post_method.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt; --&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/post_method.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;It’s the branch with &lt;code&gt;form&lt;/code&gt; that has &lt;code&gt;method=&#39;post&#39;&lt;/code&gt;.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open the &lt;code&gt;POST&lt;/code&gt; branch and find all fields. We can see the two ‘hidden’ fields.&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/hidden_fields.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;Below the &lt;code&gt;form&lt;/code&gt; tag, we see two &lt;code&gt;input&lt;/code&gt; tags set to hidden, there they are! Even though we want to login, we also have to provide the two hidden fields. Take note of both their &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; tags.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Dive deeper down the branch and find other fields. In our case, username and password.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For username:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/username.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;For password:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/password.png&#34; /&gt;

&lt;/div&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Write down the field names with the correspoding values.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_fields &amp;lt;-
  list(
    adAS_username = &amp;quot;private&amp;quot;,
    adAS_password = &amp;quot;private&amp;quot;,
    adAS_i18n_theme = &amp;#39;en&amp;#39;,
    adAS_mode = &amp;#39;authn&amp;#39;
  )&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load our packages and our URL’s&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(httr)
library(xml2)

login &amp;lt;- &amp;quot;https://sso.upf.edu/CAS/index.php/login?service=https%3A%2F%2Faulaglobal.upf.edu%2Flogin%2Findex.php&amp;quot;
website &amp;lt;- &amp;quot;https://aulaglobal.upf.edu/user/index.php?page=0&amp;amp;perpage=5000&amp;amp;mode=1&amp;amp;accesssince=0&amp;amp;search&amp;amp;roleid=5&amp;amp;contextid=185837&amp;amp;id=9829&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Login using all of our fields.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;upf &amp;lt;- handle(&amp;quot;https://aulaglobal.upf.edu&amp;quot;)

access &amp;lt;- POST(login,
               body = all_fields,
               handle = upf)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note how I set the &lt;code&gt;handle&lt;/code&gt;. If the website you want to visit and the website that hosts the login information have the same root of the URL (&lt;code&gt;aulaglobal.upf.edu&lt;/code&gt; for example), then you can avoid using &lt;code&gt;handle&lt;/code&gt; (it’s done behind the scenes). In my case, I set the &lt;code&gt;handle&lt;/code&gt; to the same root URL of the website I WANT to visit after I log in (because they have different root URL’s). This way the cookies and login information from the login are preserved through out the session.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Request the information from the website you’re interested&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emails &amp;lt;- GET(website, handle = upf)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Scrape away!&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_emails &amp;lt;-
  read_html(emails) %&amp;gt;% 
  xml_ns_strip() %&amp;gt;% 
  xml_find_all(&amp;quot;//table//a&amp;quot;) %&amp;gt;% 
  as_list() %&amp;gt;% 
  unlist() %&amp;gt;% 
  str_subset(&amp;quot;.+@upf.edu$&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unfortunately you won’t be able to reproduce this script because you don’t have a log information unless you belong to the same PhD program as I do. However, I hope you find the hidden fields explanation useful, I’m sure I will come back to this in the near future for reference!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ess is now essurvey</title>
      <link>/blog/2018-03-26-ess-is-now-essurvey/ess-is-now-essurvey/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-03-26-ess-is-now-essurvey/ess-is-now-essurvey/</guid>
      <description>&lt;p&gt;My &lt;code&gt;ess&lt;/code&gt; package has be renamed to &lt;code&gt;essurvey&lt;/code&gt;. For quite some time I’ve been pondering whether I should change the name. All of this comes from a dicussion we had on the &lt;a href=&#34;http://r.789695.n4.nabble.com/R-pkgs-Release-of-ess-0-0-1-td4746540.html&#34;&gt;R-pkg mailing list&lt;/a&gt; where many R users suggested that the name was unfortunate given that Emacs Speaks Statistics (ESS) has a long precedence in the R community and the names are very similar. Later on, when submitting the package to &lt;a href=&#34;https://ropensci.org/&#34;&gt;rOpensci&lt;/a&gt;, Jim Hester &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/201#issuecomment-372304003&#34;&gt;raised the fact once again&lt;/a&gt;, without being aware of the previous email thread.&lt;/p&gt;
&lt;p&gt;Considering that I was already changing some of the functionalities of the package due to the &lt;a href=&#34;https://github.com/ropensci/onboarding/issues/201&#34;&gt;rOpensci review&lt;/a&gt;, I decided to change the package name and republish an improved version of &lt;code&gt;ess&lt;/code&gt; as &lt;code&gt;essurvey 1.0.0&lt;/code&gt;. &lt;code&gt;essurvey&lt;/code&gt; is now on CRAN and the repository has been moved to rOpensci’s &lt;a href=&#34;https://github.com/ropensci/essurvey&#34;&gt;github account&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The new package is mostly similar although there are now some deprecated functions and new features. Below are the main changes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;You can login &lt;strong&gt;once&lt;/strong&gt; using &lt;code&gt;set_email(&amp;quot;your_email&amp;quot;)&lt;/code&gt; and avoid rewriting your email in every call to the &lt;code&gt;ess_*&lt;/code&gt; functions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All &lt;code&gt;ess_*&lt;/code&gt; functions have been deprecated in favour of similar &lt;code&gt;import_*&lt;/code&gt; functions. For example:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ess_rounds(1:7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;becomes..&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;import_rounds(1:7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But that’s the same you would say. The only difference is that with &lt;code&gt;ess_rounds&lt;/code&gt; you could download data in Stata, SPSS or SAS formats directly. For that, there’s now the &lt;code&gt;download_*&lt;/code&gt; functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;download_rounds(
  1:5,
  output_dir = getwd(),
  format = &amp;quot;spss&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of the above applies to &lt;code&gt;ess_country&lt;/code&gt; and the &lt;code&gt;ess_all_*&lt;/code&gt; functions. There’s also some other minor changes you can checkout in the &lt;a href=&#34;https://github.com/ropensci/essurvey/blob/master/NEWS.md&#34;&gt;NEWS&lt;/a&gt; file. If you haven’t tried &lt;code&gt;essurvey&lt;/code&gt;, you can visit the package website for more detailed examples at &lt;a href=&#34;https://ropensci.github.io/essurvey/&#34; class=&#34;uri&#34;&gt;https://ropensci.github.io/essurvey/&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RSelenium and scraping Catalan educational data</title>
      <link>/blog/2018-03-22-rselenium-and-scraping-catalan-educational-data/rselenium-and-scraping-catalan-educational-data/</link>
      <pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-03-22-rselenium-and-scraping-catalan-educational-data/rselenium-and-scraping-catalan-educational-data/</guid>
      <description>&lt;p&gt;Yesterday I found this public dataset on schools from Barcelona and their performance on tests on 6th grade. I wanted to scrape them to investigate the relationship between performance and schools that receive special government funds for social integration. I found this dataset &lt;a href=&#34;https://view-awesome-table.com/-L4lo3r-JA2iaWk1puUT/view&#34;&gt;here&lt;/a&gt; but it was different from the types of websites I usually scape (&lt;code&gt;html&lt;/code&gt; or &lt;code&gt;xml&lt;/code&gt;). Although the website has some &lt;code&gt;html&lt;/code&gt; the engine swiping the schools is actually based on &lt;code&gt;Javascript&lt;/code&gt;. Well, that’s a job for &lt;code&gt;RSelenium&lt;/code&gt;, an R package that allows you to browse a website with R.&lt;/p&gt;
&lt;p&gt;The process was actually much easier than I thought using Docker. I follow the answer of setting docker from &lt;a href=&#34;https://stackoverflow.com/questions/45395849/cant-execute-rsdriver-connection-refused&#34;&gt;this&lt;/a&gt; post. Note that this is for Windows 10.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://download.docker.com/win/stable/DockerToolbox.exe&#34;&gt;install docker&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;run it, restart computer as requested&lt;/li&gt;
&lt;li&gt;pull image by running in command line: &lt;code&gt;docker pull selenium/standalone-firefox&lt;/code&gt; (or chrome instead of firefox) or in R &lt;code&gt;shell(&#39;docker pull selenium/standalone-firefox&#39;)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;start server by running in command line: &lt;code&gt;docker run -d -p 4445:4444 selenium/standalone-firefox&lt;/code&gt; or in R &lt;code&gt;shell(&#39;docker run -d -p 4445:4444 selenium/standalone-firefox&#39;)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Then run &lt;code&gt;remDr &amp;lt;- remoteDriver(remoteServerAddr = &amp;quot;localhost&amp;quot;, port = 4445L, browserName = &amp;quot;firefox&#39;&amp;quot;)&lt;/code&gt;. The doc suggests something different with a virtual machine but i couldn’t get it to work. Replacing &lt;code&gt;&amp;quot;localhost&amp;quot;&lt;/code&gt; with the &lt;code&gt;ip&lt;/code&gt; the your docker server provides.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I used &lt;code&gt;chrome&lt;/code&gt; for all of the above and got this working just fine in no time!&lt;/p&gt;
&lt;p&gt;Now that we got that down, I scraped the data with not much hassle.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Load packages and create empty data frame to fill out (I looked at the website to get the columns)&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RSelenium)
library(xml2)
library(tidyverse)

the_df &amp;lt;-
  as_tibble(set_names(rerun(4, character()),
                      c(&amp;quot;school_name&amp;quot;, &amp;quot;complexity&amp;quot;, &amp;quot;social_fund&amp;quot;, &amp;quot;score_6th&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Open the website with &lt;code&gt;RSelenium&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;remDr &amp;lt;- remoteDriver(remoteServerAddr = &amp;quot;192.168.99.100&amp;quot;,
                      port = 4445L,
                      browserName = &amp;quot;chrome&amp;quot;)

remDr$open()
remDr$navigate(&amp;quot;https://view-awesome-table.com/-L4lo3r-JA2iaWk1puUT/view&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point you can use &lt;code&gt;remDr$screenshot(display = TRUE)&lt;/code&gt; to print a screenshot of the website that you’re at.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Define a function that clicks one time on the swiping key on the right, scrapes the table and turns it into a &lt;code&gt;tibble&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;navigate_click &amp;lt;- function() {
  webElem &amp;lt;- remDr$findElement(using = &amp;quot;class name&amp;quot;,
                               &amp;quot;google-visualization-table-div-page&amp;quot;)
  
  Sys.sleep(0.5)
  webElem$clickElement()
  
  remDr$getPageSource()[[1]] %&amp;gt;% 
    read_xml() %&amp;gt;%
    xml_ns_strip() %&amp;gt;%
    xml_find_all(xpath = &amp;#39;//td&amp;#39;) %&amp;gt;%
    xml_text() %&amp;gt;%
    set_names(c(&amp;quot;school_name&amp;quot;, &amp;quot;complexity&amp;quot;, &amp;quot;social_fund&amp;quot;, &amp;quot;score_6th&amp;quot;)) %&amp;gt;%
    as.list() %&amp;gt;% as_tibble()
}&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Run that function &lt;code&gt;160&lt;/code&gt; times (# of schools in that data) and bind all of these datasets together&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;complete_df &amp;lt;-
  map(1:160, ~ navigate_click()) %&amp;gt;%
  bind_rows()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Aaaaandddd, we got our nicely formatted dataset ready for some analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;complete_df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 160 x 4
##    school_name                   complexity   social_fund score_6th   
##    &amp;lt;chr&amp;gt;                         &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;       
##  1 Escuela Collaso i Gil         Muy alta     52%         Bajo        
##  2 Escuela Ruben Darío           Muy alta     66%         Bajo        
##  3 Escuela Castella              Muy alta     25%         Mediano-bajo
##  4 Escuela Drassanes             Muy alta     41%         Bajo        
##  5 Escuela Milà i Fontanals      Muy alta     49%         Bajo        
##  6 Escuela Baixeras              Mediana-alta 24%         Bajo        
##  7 Escuela Cervantes             Mediana-alta 38%         Mediano-alto
##  8 Escuela Parc de la Ciutadella Mediana-baja 15%         Mediano-bajo
##  9 Escuela Pere Vila             Alta         30%         Mediano-alto
## 10 Escuela Alexandre Galí        Alta         27%         Bajo        
## # ... with 150 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PS: If they ever remove that dataset from the website this post might not work in the future, but at least there’s a traceback on how to user docker with &lt;code&gt;RSelenium&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ess 0.1.1 is out!</title>
      <link>/blog/2018-03-04-ess-011-is-out/ess-0-1-1-is-out/</link>
      <pubDate>Sun, 04 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-03-04-ess-011-is-out/ess-0-1-1-is-out/</guid>
      <description>&lt;p&gt;The new version of the ess package is out! &lt;code&gt;ess 0.1.1&lt;/code&gt; fixes some bugs and inconsistencies across the package and has one important new feature and a change that breaks backward compatibility. You can see all changes &lt;a href=&#34;https://cimentadaj.github.io/ess/news/index.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Install the latest version &lt;code&gt;0.1.1&lt;/code&gt; from CRAN with &lt;code&gt;install.packages(&amp;quot;ess&amp;quot;)&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;new-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New features&lt;/h2&gt;
&lt;p&gt;When downloading any round(s) from the European Social Survey the files are always accompanied by a script that recodes values like 6, 7, 8 and 9 or 96, 97, 98 and 99 to missings, depending on the question. This is a bit tricky because a question with a scale from 1 to 5 will have 6 to 9 as missing values and a question with a scale from 1 to 10 will have the missing values set as 96 to 99. The new &lt;code&gt;remove_missings()&lt;/code&gt; function removes all missing values from all questions.&lt;/p&gt;
&lt;p&gt;For example…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(ess)

clean_df &amp;lt;-
  ess_rounds(1, &amp;quot;your_email@gmail.com&amp;quot;) %&amp;gt;%
  recode_missings()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… will set all the missing categories to NA. That is, the 6 to 9 and 96 to 99 categories on the specific questions. It gives you flexibility in recoding specific categories such as ‘Don’t Know’, ‘Refusal’ or both.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;another_clean_df &amp;lt;-
  ess_rounds(1, &amp;quot;your_email@gmail.com&amp;quot;) %&amp;gt;%
  recode_missings(c(&amp;quot;Refusal&amp;quot;, &amp;quot;No answer&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See &lt;code&gt;?recode_missings&lt;/code&gt; for the missing categories that are available for recode.&lt;/p&gt;
&lt;p&gt;However, I do not advise recoding missing values right away if you’re exploring the dataset. If you want to manually recode missing values you can use the &lt;code&gt;recode_numeric_missing()&lt;/code&gt; and &lt;code&gt;recode_strings_missing&lt;/code&gt; correspondingly on numeric and string variables. They work the same as &lt;code&gt;recode_missings&lt;/code&gt; but accept a vector of class labelled, the class of each of the columns that returns the &lt;code&gt;ess_*&lt;/code&gt; functions.&lt;/p&gt;
&lt;p&gt;For example&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;another_clean_df$tvtot &amp;lt;-
  recode_numeric_missing(
    another_clean_df$tvtot,
    &amp;quot;Don&amp;#39;t know&amp;quot;
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;works for recoding the “Don’t know” category. By default all missing values are chosen.&lt;/p&gt;
&lt;p&gt;Note that both sets of functions &lt;strong&gt;only&lt;/strong&gt; work with labelled numeric vectors from the &lt;code&gt;haven&lt;/code&gt; package. If you use the &lt;code&gt;ess&lt;/code&gt; package that’s taken care of. If you download the data manually, you must read it with the &lt;code&gt;haven&lt;/code&gt; package for these functions to work.&lt;/p&gt;
&lt;p&gt;There are also two new &lt;code&gt;show_*&lt;/code&gt; functions, namely &lt;code&gt;show_themes&lt;/code&gt; and &lt;code&gt;show_rounds_country&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The first one returns all available themes…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show_themes()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Ageism&amp;quot;                            
##  [2] &amp;quot;Citizen involvement&amp;quot;               
##  [3] &amp;quot;Democracy&amp;quot;                         
##  [4] &amp;quot;Economic morality&amp;quot;                 
##  [5] &amp;quot;Family work and well-being&amp;quot;        
##  [6] &amp;quot;Gender, Household&amp;quot;                 
##  [7] &amp;quot;Health and care&amp;quot;                   
##  [8] &amp;quot;Human values&amp;quot;                      
##  [9] &amp;quot;Immigration&amp;quot;                       
## [10] &amp;quot;Justice&amp;quot;                           
## [11] &amp;quot;Media and social trust&amp;quot;            
## [12] &amp;quot;Personal ... well-being&amp;quot;           
## [13] &amp;quot;Politics&amp;quot;                          
## [14] &amp;quot;Public attitudes to climate change&amp;quot;
## [15] &amp;quot;Social inequalities in health&amp;quot;     
## [16] &amp;quot;Socio demographics&amp;quot;                
## [17] &amp;quot;Subjective well-being...&amp;quot;          
## [18] &amp;quot;Timing of life&amp;quot;                    
## [19] &amp;quot;Welfare attitudes&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… but doesn’t haven a corresponding &lt;code&gt;ess_*&lt;/code&gt; function. This means that it works purely for descriptive purposes.&lt;/p&gt;
&lt;p&gt;Additionaly, &lt;code&gt;show_rounds_country&lt;/code&gt; returns all countries that participated in a give round.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show_rounds_country(rounds = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Austria&amp;quot;        &amp;quot;Belgium&amp;quot;        &amp;quot;Czech Republic&amp;quot; &amp;quot;Denmark&amp;quot;       
##  [5] &amp;quot;Estonia&amp;quot;        &amp;quot;Finland&amp;quot;        &amp;quot;France&amp;quot;         &amp;quot;Germany&amp;quot;       
##  [9] &amp;quot;Greece&amp;quot;         &amp;quot;Hungary&amp;quot;        &amp;quot;Iceland&amp;quot;        &amp;quot;Ireland&amp;quot;       
## [13] &amp;quot;Italy&amp;quot;          &amp;quot;Luxembourg&amp;quot;     &amp;quot;Netherlands&amp;quot;    &amp;quot;Norway&amp;quot;        
## [17] &amp;quot;Poland&amp;quot;         &amp;quot;Portugal&amp;quot;       &amp;quot;Slovakia&amp;quot;       &amp;quot;Slovenia&amp;quot;      
## [21] &amp;quot;Spain&amp;quot;          &amp;quot;Sweden&amp;quot;         &amp;quot;Switzerland&amp;quot;    &amp;quot;Turkey&amp;quot;        
## [25] &amp;quot;Ukraine&amp;quot;        &amp;quot;United Kingdom&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You could use this to see which countries participated in all rounds. For example..&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_countries &amp;lt;-
  map(show_rounds(), ~ show_rounds_country(.x)) %&amp;gt;%
  reduce(intersect)

all_countries&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Belgium&amp;quot;        &amp;quot;Finland&amp;quot;        &amp;quot;France&amp;quot;         &amp;quot;Germany&amp;quot;       
##  [5] &amp;quot;Ireland&amp;quot;        &amp;quot;Netherlands&amp;quot;    &amp;quot;Norway&amp;quot;         &amp;quot;Poland&amp;quot;        
##  [9] &amp;quot;Slovenia&amp;quot;       &amp;quot;Sweden&amp;quot;         &amp;quot;Switzerland&amp;quot;    &amp;quot;United Kingdom&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;breaking-changes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Breaking changes&lt;/h2&gt;
&lt;p&gt;Finally, there is one change that breaks backward compatability. All the &lt;code&gt;ess_*&lt;/code&gt; functions always used to return a list, regardless of the number of rounds that were requested. Now, &lt;code&gt;ess_*&lt;/code&gt; functions return a &lt;code&gt;tibble&lt;/code&gt; whenever it is request only one round and a list when more than one round is requested.&lt;/p&gt;
&lt;p&gt;For example&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ess_rounds(1, &amp;quot;your_email@gmail.com&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will return a tibble but…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ess_rounds(1:3, &amp;quot;your_email@gmail.com&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;…will return a list with each tibble in a slot.&lt;/p&gt;
&lt;p&gt;For more concrete examples check out the new website of the ess &lt;a href=&#34;https://cimentadaj.github.io/ess/&#34;&gt;here&lt;/a&gt;. If you have any ideas for features or find a bug, please report &lt;a href=&#34;https://github.com/cimentadaj/ess/issues&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What time should I ride my bike?</title>
      <link>/blog/2018-02-12-what-time-should-i-ride-my-bike/what-time-should-i-ride-my-bike/</link>
      <pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-02-12-what-time-should-i-ride-my-bike/what-time-should-i-ride-my-bike/</guid>
      <description>&lt;p&gt;For a few months now I’ve started developing a project on which I download live bicycle usage from the API of Bicing, the public bicycle service from the city of Barcelona. Before I started analyzing the data I wanted to harvest a reasonable amount of data to be able to get a representative sample of bicycle usage.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;The first thing I did was to set up my Virtual Private Server (VPS) and set a &lt;code&gt;cron&lt;/code&gt; job to email me every day after the scraping of the data is done. Check out a detailed tutorial on how to this &lt;a href=&#34;blog/2017-12-01-how-long-should-i-wait-for-my-bike/how-long-should-i-wait-for-my-bike/index.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The second thing I did was to set up a MySQL database in my VPS and develop a program that interacts with the Barcelona Public Bicycle System API and feeds the database on a daily basis. Check out a detailed tutorial on how I did it &lt;a href=&#34;blog/2018-01-31-scraping-at-scale-daily-scraping-to-your-database/scraping-at-scale-daily-scraping-to-your-database/index.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I left this program grabing biclycle data of a station close to my house only in the mornings and evenings (moments I used the bicycle) for the last 3 months. This is my first attempt to analyze this data. Please take this as a work in progress as I develop more fine-grained understanding of the data.&lt;/p&gt;
&lt;p&gt;Here I load the libraries and connect to the database in my VPS. Note how I hide the IP of the server and the password by grabbing it as environment variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(DBI)
library(RMySQL)
library(caret)
library(viridis)
library(tidyverse)


password &amp;lt;- Sys.getenv(&amp;quot;password&amp;quot;)
host &amp;lt;- Sys.getenv(&amp;quot;host&amp;quot;)

con &amp;lt;- dbConnect(MySQL(),
                 dbname = &amp;quot;bicing&amp;quot;, # in &amp;quot;&amp;quot; quotes
                 user = &amp;quot;cimentadaj&amp;quot;, # in &amp;quot;&amp;quot; quotes
                 password = password,
                 host = host) # ip of my server&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let’s grab the data with a simple query. Let’s get some columns:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;slots&lt;/code&gt; is the number of available slots in the station&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bikes&lt;/code&gt; is the number of available bikes in the station&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These two columns are exact opposites. If the station can hold 20 bicycles and there are 8 slots available, then there’s 12 bicycles availables.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;status&lt;/code&gt; is the status of the station. Whether &lt;code&gt;OPN&lt;/code&gt; or &lt;code&gt;CLOSED&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;time&lt;/code&gt; is the specific date/time at which that row was returned from the API.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There’s an additional column named &lt;code&gt;error_msg&lt;/code&gt; that has the error message if the API couldn’t retrieve the data. Let’s use only those which were scraped correctly. Let’s write that query and grab the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;query &amp;lt;- 
&amp;quot;SELECT slots, bikes, status, time
 FROM bicing_station
 WHERE hour(time) IN (&amp;#39;7&amp;#39;, &amp;#39;8&amp;#39;, &amp;#39;9&amp;#39;, &amp;#39;10&amp;#39;, &amp;#39;18&amp;#39;, &amp;#39;19&amp;#39;, &amp;#39;20&amp;#39;)
 AND error_msg IS NULL;&amp;quot;

bicing &amp;lt;-
  dbGetQuery(con, query) %&amp;gt;%
  as_tibble() %&amp;gt;% 
  mutate(time = lubridate::ymd_hms(time),
         slots = as.numeric(slots),
         bikes = as.numeric(slots))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Awesome. Now we have our data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bicing&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 46,399 x 4
##    slots bikes status time               
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dttm&amp;gt;             
##  1   10.   10. OPN    2017-12-11 08:01:16
##  2   10.   10. OPN    2017-12-11 08:02:12
##  3   10.   10. OPN    2017-12-11 08:03:04
##  4   10.   10. OPN    2017-12-11 08:04:04
##  5   10.   10. OPN    2017-12-11 08:05:04
##  6    9.    9. OPN    2017-12-11 08:06:04
##  7    8.    8. OPN    2017-12-11 08:07:04
##  8    8.    8. OPN    2017-12-11 08:08:05
##  9    7.    7. OPN    2017-12-11 08:09:04
## 10    8.    8. OPN    2017-12-11 08:10:04
## # ... with 46,389 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check if there’s any cases in which the station was not open.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bicing %&amp;gt;%
  filter(status != &amp;quot;OPN&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 0 x 4
## # ... with 4 variables: slots &amp;lt;dbl&amp;gt;, bikes &amp;lt;dbl&amp;gt;, status &amp;lt;chr&amp;gt;,
## #   time &amp;lt;dttm&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Empty rows, alright, station has worked fine.&lt;/p&gt;
&lt;p&gt;Let’s explore the number of bikes comparing between mornings/evenings&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary_time &amp;lt;-
  bicing %&amp;gt;% 
  group_by(hour = as.factor(lubridate::hour(time))) %&amp;gt;% 
  summarize(Average = mean(bikes, na.rm = TRUE),
            Median = median(bikes, na.rm = TRUE)) %&amp;gt;% 
  gather(type, value, -hour)

bicing %&amp;gt;%
  mutate(hour = as.factor(lubridate::hour(time))) %&amp;gt;%
  ggplot(aes(hour, bikes)) +
  geom_jitter(alpha = 1/8) +
  geom_point(data = summary_time,
             aes(y = value, colour = type), size = 3) +
  theme_bw() +
  labs(x = &amp;quot;Hour of the day (24H)&amp;quot;,
       y = &amp;quot;# of available bikes&amp;quot;,
       title = &amp;quot;Mornings have greater bicycle usage than evenings&amp;quot;,
       subtitle = &amp;quot;But number of bikes can vary betwen 0 and 20 in the morning&amp;quot;) +
  scale_colour_manual(name = &amp;quot;Types&amp;quot;, values = c(&amp;#39;Average&amp;#39; = &amp;#39;red&amp;#39;, &amp;#39;Median&amp;#39; = &amp;#39;blue&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is a bit revealing. Some take aways:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Mornings have greater number of bikes but they also have high variability. For example, look at the 8 AM category. Even though the average is at around 7 bikes, it’s also very likely that there’s 0 bikes as well as 20 bikes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;As time passes, more outliers appear in the distribution. We can infer this both from the overall distribution and the average and the mean are farther away from each other.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is probably related to how Bicing fills out the stations (a few times a days a truck with bicycles passes by the station and fills them out). I think this is beginning to tell a story although perhaps it’s too early: usage in the mornings is heavy and very dynamic but as the day passes by more a more bikes are taken (either by the Bicing team or by citizens).&lt;/p&gt;
&lt;p&gt;This gives no clear clue to the layman citizen: if it’s 8 AM, how likely am I find bikes? Let’s inspect further.&lt;/p&gt;
&lt;p&gt;Logically, the next question is: does this differ by day of the week? Bloew I plot the average number of bikes per day/hour combination. In addition we’d also want to plot some sort of uncertainty indicator like the standard deviation. However, because it’s very common for bikes to be close to 7-10 bikes as average and below, I plot the uncertainty as the percentage of times that the station has over 10 bikes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary_time &amp;lt;-
  bicing %&amp;gt;% 
  group_by(hour = as.factor(lubridate::hour(time)),
           day = as.factor(lubridate::wday(time, label = TRUE, week_start = TRUE))) %&amp;gt;% 
  summarize(Average = mean(bikes, na.rm = TRUE),
            Variation = mean(bikes &amp;gt; 10, na.rm = TRUE)) %&amp;gt;% 
  gather(type, value, -hour, -day)

p1 &amp;lt;- 
  summary_time %&amp;gt;% 
  filter(type == &amp;quot;Average&amp;quot;) %&amp;gt;% 
  ggplot(aes(hour, day, fill = value)) + 
  geom_tile() +
  scale_fill_viridis(name = &amp;quot;Avg # of bikes&amp;quot;) +
  labs(x = &amp;#39;Hour of the day (24H)&amp;#39;,
       y = &amp;#39;Day of the week&amp;#39;,
       title = &amp;#39;Average number of bikes has a workin week/end of week divide&amp;#39;,
       subtitle = &amp;#39;Thu and Wed seem to have high peaks at 8, Sun and Sat have peaks at 10&amp;#39;)

p2 &amp;lt;-
  summary_time %&amp;gt;% 
  filter(type == &amp;quot;Variation&amp;quot;) %&amp;gt;% 
  ggplot(aes(hour, day, fill = value)) + 
  geom_tile() +
  scale_fill_viridis(name = &amp;#39;% of times \n station has &amp;gt; 10 bikes&amp;#39;) +
  labs(x = &amp;#39;Hour of the day (24H)&amp;#39;,
       y = &amp;#39;Day of the week&amp;#39;,
       title = &amp;#39;Variability reflects same pattern as average # of bikes&amp;#39;,
       subtitle = &amp;#39;Thu and Wed seem to have &amp;gt; 10 bikes often at 8, Sun and Sat have peaks at 10&amp;#39;)

p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similarly, we can see whether there’s a clear morning/evening divide by looking at the percentage of bikes for every day in the evening and the morning.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bicing %&amp;gt;%
  mutate(hour = lubridate::hour(time),
         day = lubridate::wday(time, label = TRUE, week_start = TRUE),
         morning_evening = ifelse(hour &amp;lt;= 10, &amp;quot;Morning&amp;quot;, &amp;quot;Evening&amp;quot;)) %&amp;gt;%
  ggplot(aes(day, bikes, fill = morning_evening)) +
  geom_col(position = &amp;quot;fill&amp;quot;) +
  labs(x = &amp;quot;Day of the week&amp;quot;,
       y = &amp;quot;% of bikes&amp;quot;,
       title = &amp;#39;# of bikes increases linearly through out the week&amp;#39;,
       fill = &amp;#39;Time of day&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Alright, so we got that down. So far:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;There’s more taking and droping happening in the first few days of the week than in the rest&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Weekdays and weekends have different patterns in bike usage; namely that bike usage is higher in earlier in the week than in the weekends.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;More bikes are taken in the early days of the weeks than in the latter parts.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Following the previous conclusion, I had the itching of figuring out the rate at which bicycles are taken out by hour. This we can depart from the total or average number of bikes, to actual rate of picking/droping bikes. This can help to pinpoint specific times at which we should avoid going or droping a bike.&lt;/p&gt;
&lt;p&gt;What’s the rate at which bicycles are being taken out by hour? At which time is the station emptying out quicker?&lt;/p&gt;
&lt;p&gt;I’ve computed a metric that calculates the percentage of minutes that there’s changes in the station.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;intm_df &amp;lt;-
  bicing %&amp;gt;%
  mutate(hour = as.factor(lubridate::hour(time)),
         day = lubridate::wday(time, label = TRUE, week_start = TRUE)) %&amp;gt;%
  group_by(hour, day) %&amp;gt;%
  mutate(future_bike = lag(bikes)) %&amp;gt;%
  summarize(avg_movement = mean(bikes != future_bike, na.rm = TRUE) * 60) %&amp;gt;%
  ungroup()

intm_df %&amp;gt;% 
  ggplot(aes(hour, avg_movement, colour = day, group = day)) +
  geom_line(size = 1.2) +
  facet_wrap(~ day, ncol = 4) +
  theme_bw() +
  labs(x = &amp;#39;Hour of the day (24H)&amp;#39;,
       y = &amp;quot;Minutes per hour with a bicycle change&amp;quot;,
       title = &amp;#39;Weekdays have much greater bicycle usage than weekends&amp;#39;,
       subtitle = &amp;quot;Wed has the busiest hour of the week at 8AM; There&amp;#39;s activity 25 minutes out of the 60 minutes.&amp;quot;) +
  theme(legend.position = &amp;#39;none&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is very interesting! This plot reverses some of the findings from before. First off, we see that there’s high variability between hours: there’s no point in looking at averages within an hour because a lot happens between minutes. For example, at 7AM and 10 AM during working days there’s very little activity regardless of the day. On the contrary, 8AM and 9AM have high bycicle usage through the working days.&lt;/p&gt;
&lt;p&gt;This makes sense, it’s the time that people usually go to work. Also as expected, bicycle usage is low on weekend mornings and increases linearly through out the day. All in all, 8/9 AM on working days seems to be the time to avoid bicycles if you can! Eventually, in another post, I plan to investigate whether there’s minute-to-minute patterns for 8/9 AM on working days. For example, is there more activity closer to certain minutes? Like half past the hour or at exactly the hour.&lt;/p&gt;
&lt;p&gt;Also, it seems that evenings are busy even on working days, specially on Thursdays but have very little bicycle usage on Fridays! Perhaps Catalans are ready to party and travel on the metro. On my follow up post, I also plan to see whether these patterns hold by season. I would expect summer and winter to have strong seasonal patterns.&lt;/p&gt;
&lt;p&gt;To begin the conclusion, when are the moments when the station is empty? This will trigger me to avoid picking bicycles on those specific times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bicing %&amp;gt;%
  filter(bikes == 0) %&amp;gt;%
  mutate(time_day = as.numeric(lubridate::hm(str_extract(time, &amp;quot;[0-9]{2}:[0-9]{2}&amp;quot;)))) %&amp;gt;% 
  ggplot(aes(x = time_day)) +
  geom_histogram()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Bicing people probably prepare very well because it’s mostly empty in the evenings&lt;/p&gt;
&lt;p&gt;The next step in the analysis is to start making predictions in waiting time. That will be the topic of my next post, in which I start to develop a modelling approach to predict the time you’ll have to have wait until a bicycle arrives or leaves. As a very simple exercise, I wanted to predict check whether I can predict when the station will be empty? I tried a simple logistic regression just to check.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;empty_bicycle &amp;lt;-
  mutate(bicing,
         empty = ifelse(bikes == 0, 1, 0),
         hour = as.character(lubridate::hour(time)),
         day = lubridate::wday(time),
         day = as.character(day)) %&amp;gt;%
  select(-(1:4))

training_rows &amp;lt;- createDataPartition(empty_bicycle$empty, 1, p = 0.8)[[1]]

training &amp;lt;- empty_bicycle[training_rows, ]
test &amp;lt;- empty_bicycle[-training_rows, ]

mod1 &amp;lt;- glm(empty ~ . + day:hour, data = training, family = &amp;quot;binomial&amp;quot;)

pred1 &amp;lt;- predict(mod1, newdata = test, type = &amp;#39;response&amp;#39;)

pred_empty &amp;lt;- rbinom(length(pred1), 1, prob = pred1)

confusionMatrix(test$empty, pred_empty, positive = &amp;quot;1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 6693 1162
##          1 1103  321
##                                          
##                Accuracy : 0.7559         
##                  95% CI : (0.747, 0.7646)
##     No Information Rate : 0.8402         
##     P-Value [Acc &amp;gt; NIR] : 1.000          
##                                          
##                   Kappa : 0.0762         
##  Mcnemar&amp;#39;s Test P-Value : 0.223          
##                                          
##             Sensitivity : 0.21645        
##             Specificity : 0.85852        
##          Pos Pred Value : 0.22542        
##          Neg Pred Value : 0.85207        
##              Prevalence : 0.15982        
##          Detection Rate : 0.03459        
##    Detection Prevalence : 0.15346        
##       Balanced Accuracy : 0.53749        
##                                          
##        &amp;#39;Positive&amp;#39; Class : 1              
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model is terrible at predicting the emptyness of the stations as it can only predict 20% of the time. A few strategies I could check out to improve accuracy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Feature engineer when the bicing team picks up bicycles (because they leave them empty)&lt;/li&gt;
&lt;li&gt;Add more information on weather and public holidays from public API’s&lt;/li&gt;
&lt;li&gt;Because the cell that contains empty stations has very few cases, it might be useful to resample that sample until it reaches a similar sample size as the other cells. This might give greater certainty and I assume that there’s not a lot of variability in the pattern of empty stations, so it should be representative.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, other classification models are certainly warranted. One good alternative would be a random forest, as it takes into consideration specific thresholds in the time of day when prunning the trees.&lt;/p&gt;
&lt;p&gt;However, we also need to be aware that a model is as good as the data that’s being fit. Perhaps, we just need better data!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rewriting duplicated</title>
      <link>/blog/2018-02-06-rewriting-duplicated/rewriting-duplicated/</link>
      <pubDate>Tue, 06 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-02-06-rewriting-duplicated/rewriting-duplicated/</guid>
      <description>&lt;p&gt;Lightning post. I got very confused earlier today on how to use &lt;code&gt;duplicated&lt;/code&gt;. Basically, I didn’t know if it was picking only one duplicate or many of the duplicates at the same time. I figure it out but I still was a bit confused so I decided to rewrite the function from scratch. Below you can see it. Please post any other solutions or feedback.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dupl_identifier &amp;lt;- function(vec, where) {
  intm &amp;lt;- x %in% vec
  pos &amp;lt;- which(intm)
  intm[where(pos)] &amp;lt;- FALSE
  intm
}

my_duplicated &amp;lt;- function(x, fromLast = FALSE) {
  
  where &amp;lt;- ifelse(!fromLast, min, max)
  repeated &amp;lt;- names(which(table(x) &amp;gt; 1))
  
  if (length(repeated) == 0) return(rep(FALSE, length(x)))
  
  val &amp;lt;- lapply(repeated, dupl_identifier, where)
  final &amp;lt;- as.logical(Reduce(`+`, val))
  
  final
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- sample(1:10, 100, replace = TRUE)

identical(my_duplicated(x),
          duplicated(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- sample(c(1:100, NA), 100, replace = TRUE)

identical(my_duplicated(x),
          duplicated(x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Cleaning in-door positioning data</title>
      <link>/blog/2018-02-03-predicting-location-via-indoor-positioning-systems/predicting-location-via-indoor-positioning-systems/</link>
      <pubDate>Sat, 03 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-02-03-predicting-location-via-indoor-positioning-systems/predicting-location-via-indoor-positioning-systems/</guid>
      <description>&lt;p&gt;I’ve just started reading the wonderful book &lt;a href=&#34;http://rdatasciencecases.org/&#34;&gt;Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving&lt;/a&gt;. I’ve just begun the first chapter and I wanted to document some of the things I found interesting. In this post I’ll walkthrough the example on how to transform a text file with GPS locations into a well formatted rectangular dataset. For a detailed explanation see their book, which I highly recommend buying.&lt;/p&gt;
&lt;p&gt;Note: When it makes senses/it’s possible, I always try to find an equivalent tidyverse solution to everything they do in the book.&lt;/p&gt;
&lt;p&gt;This is the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

ex_file &amp;lt;- read_lines(&amp;quot;http://rdatasciencecases.org/Data/offline.final.trace.txt&amp;quot;)
ex_file[1:4]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;# timestamp=2006-02-11 08:31:58&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                 
## [2] &amp;quot;# usec=250&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                                      
## [3] &amp;quot;# minReadings=110&amp;quot;                                                                                                                                                                                                                                                                                                                                                                                                                                               
## [4] &amp;quot;t=1139643118358;id=00:02:2D:21:0F:33;pos=0.0,0.0,0.0;degree=0.0;00:14:bf:b1:97:8a=-38,2437000000,3;00:14:bf:b1:97:90=-56,2427000000,3;00:0f:a3:39:e1:c0=-53,2462000000,3;00:14:bf:b1:97:8d=-65,2442000000,3;00:14:bf:b1:97:81=-65,2422000000,3;00:14:bf:3b:c7:c6=-66,2432000000,3;00:0f:a3:39:dd:cd=-75,2412000000,3;00:0f:a3:39:e0:4b=-78,2462000000,3;00:0f:a3:39:e2:10=-87,2437000000,3;02:64:fb:68:52:e6=-88,2447000000,1;02:00:42:55:31:00=-84,2457000000,1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some lines are comments and the 4th line is the actual data. Basically, everything that is &lt;code&gt;something=&lt;/code&gt; is the name of the column and columns are separated by a &lt;code&gt;;&lt;/code&gt;. Now, within each column there can also be several values like in the column &lt;code&gt;pos&lt;/code&gt; where numbers are separated by a comma.&lt;/p&gt;
&lt;p&gt;First, let’s separate everything now that we know all of the delimiters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tokens &amp;lt;- str_split(ex_file[4], pattern = &amp;quot;[;=,]&amp;quot;)[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the documentation we know that the first 4 columns are constant in every line. The remaining columns can vary by each line, which is why they decide to transform the data into stacked/long format. So each unique &lt;code&gt;id&lt;/code&gt; will be repeate the number of times that there’s MAC columns (the columns that vary).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- matrix(tokens[-(1:10)], ncol = 4, byrow = TRUE)
# We got the MAC in a long format, now we have to get unique id
# of each of the macs (along with time and other vars) to be repeated
# the number of rows that tmp has


# There we go
tmp_two &amp;lt;- matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, byrow = TRUE)

mat &amp;lt;- cbind(tmp_two, tmp)
mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1]            [,2]                [,3]  [,4]  [,5]  [,6] 
##  [1,] &amp;quot;1139643118358&amp;quot; &amp;quot;00:02:2D:21:0F:33&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot;
##  [2,] &amp;quot;1139643118358&amp;quot; &amp;quot;00:02:2D:21:0F:33&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot;
##  [3,] &amp;quot;1139643118358&amp;quot; &amp;quot;00:02:2D:21:0F:33&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot;
##  [4,] &amp;quot;1139643118358&amp;quot; &amp;quot;00:02:2D:21:0F:33&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot;
##  [5,] &amp;quot;1139643118358&amp;quot; &amp;quot;00:02:2D:21:0F:33&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot;
##  [6,] &amp;quot;1139643118358&amp;quot; &amp;quot;00:02:2D:21:0F:33&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot;
##  [7,] &amp;quot;1139643118358&amp;quot; &amp;quot;00:02:2D:21:0F:33&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot;
##  [8,] &amp;quot;1139643118358&amp;quot; &amp;quot;00:02:2D:21:0F:33&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot;
##  [9,] &amp;quot;1139643118358&amp;quot; &amp;quot;00:02:2D:21:0F:33&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot;
## [10,] &amp;quot;1139643118358&amp;quot; &amp;quot;00:02:2D:21:0F:33&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot;
## [11,] &amp;quot;1139643118358&amp;quot; &amp;quot;00:02:2D:21:0F:33&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot; &amp;quot;0.0&amp;quot;
##       [,7]                [,8]  [,9]         [,10]
##  [1,] &amp;quot;00:14:bf:b1:97:8a&amp;quot; &amp;quot;-38&amp;quot; &amp;quot;2437000000&amp;quot; &amp;quot;3&amp;quot;  
##  [2,] &amp;quot;00:14:bf:b1:97:90&amp;quot; &amp;quot;-56&amp;quot; &amp;quot;2427000000&amp;quot; &amp;quot;3&amp;quot;  
##  [3,] &amp;quot;00:0f:a3:39:e1:c0&amp;quot; &amp;quot;-53&amp;quot; &amp;quot;2462000000&amp;quot; &amp;quot;3&amp;quot;  
##  [4,] &amp;quot;00:14:bf:b1:97:8d&amp;quot; &amp;quot;-65&amp;quot; &amp;quot;2442000000&amp;quot; &amp;quot;3&amp;quot;  
##  [5,] &amp;quot;00:14:bf:b1:97:81&amp;quot; &amp;quot;-65&amp;quot; &amp;quot;2422000000&amp;quot; &amp;quot;3&amp;quot;  
##  [6,] &amp;quot;00:14:bf:3b:c7:c6&amp;quot; &amp;quot;-66&amp;quot; &amp;quot;2432000000&amp;quot; &amp;quot;3&amp;quot;  
##  [7,] &amp;quot;00:0f:a3:39:dd:cd&amp;quot; &amp;quot;-75&amp;quot; &amp;quot;2412000000&amp;quot; &amp;quot;3&amp;quot;  
##  [8,] &amp;quot;00:0f:a3:39:e0:4b&amp;quot; &amp;quot;-78&amp;quot; &amp;quot;2462000000&amp;quot; &amp;quot;3&amp;quot;  
##  [9,] &amp;quot;00:0f:a3:39:e2:10&amp;quot; &amp;quot;-87&amp;quot; &amp;quot;2437000000&amp;quot; &amp;quot;3&amp;quot;  
## [10,] &amp;quot;02:64:fb:68:52:e6&amp;quot; &amp;quot;-88&amp;quot; &amp;quot;2447000000&amp;quot; &amp;quot;1&amp;quot;  
## [11,] &amp;quot;02:00:42:55:31:00&amp;quot; &amp;quot;-84&amp;quot; &amp;quot;2457000000&amp;quot; &amp;quot;1&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There we go. We have a stacked matrix with all the variables we need. Let’s wrap the line maker into a function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;processLine &amp;lt;- function(x) {
  tokens &amp;lt;- str_split(x, pattern = &amp;quot;[;=,]&amp;quot;)[[1]]
  
  # We got the MAC in a long format, now we have to get unique id
  # of each of the macs (along with time and other vars) to be repeated
  # the number of rows that tmp has
  tmp &amp;lt;- matrix(tokens[-(1:10)], ncol = 4, byrow = TRUE)
  
  # There we go
  tmp_two &amp;lt;- matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, byrow = TRUE)
  
  mat &amp;lt;- cbind(tmp_two, tmp)
  mat
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s apply it to a few sample rows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- map(ex_file[4:20], processLine)

offline &amp;lt;- as.data.frame(do.call(&amp;quot;rbind&amp;quot;, tmp))
head(offline)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              V1                V2  V3  V4  V5  V6                V7  V8
## 1 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:8a -38
## 2 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:90 -56
## 3 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:0f:a3:39:e1:c0 -53
## 4 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:8d -65
## 5 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:b1:97:81 -65
## 6 1139643118358 00:02:2D:21:0F:33 0.0 0.0 0.0 0.0 00:14:bf:3b:c7:c6 -66
##           V9 V10
## 1 2437000000   3
## 2 2427000000   3
## 3 2462000000   3
## 4 2442000000   3
## 5 2422000000   3
## 6 2432000000   3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Good! Now we can apply it to all lines, excluding of course the ones which are commented out!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- map(ex_file[!str_sub(ex_file, 1, 1) == &amp;quot;#&amp;quot;], processLine)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix

## Warning in matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, :
## data length exceeds size of matrix&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Aha.. so there’s a few warnings? What’s happening? If we ran the previous with &lt;code&gt;options(error, warn = 2)&lt;/code&gt; we would see that it looks like there are some anomalous cases where there’s no MAC information. We either fill out those values with NA’s or we simply exclude them. Because working with the MAC’s is of utmost importance for the analysis, we drop it to save memory. We redefine our function so that if there’s only the 10 starting values it returns a NULL.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;processLine &amp;lt;- function(x) {
  tokens &amp;lt;- str_split(x, pattern = &amp;quot;[;=,]&amp;quot;)[[1]]
  
  # We exclude rows where there&amp;#39;s no MAC information
  if (length(tokens) == 10) return(NULL)
  
  # We got the MAC in a long format, now we have to get unique id
  # of each of the macs (along with time and other vars) to be repeated
  # the number of rows that tmp has
  tmp &amp;lt;- matrix(tokens[-(1:10)], ncol = 4, byrow = TRUE)
  
  # There we go
  tmp_two &amp;lt;- matrix(tokens[c(2, 4, 6:8, 10)], nrow = nrow(tmp), ncol = 6, byrow = TRUE)
  
  mat &amp;lt;- cbind(tmp_two, tmp)
  mat
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And apply it now..&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- map(ex_file[!str_sub(ex_file, 1, 1) == &amp;quot;#&amp;quot;], processLine)

offline &amp;lt;- as_tibble(do.call(&amp;quot;rbind&amp;quot;, tmp))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Good, let’s set warnings back: &lt;code&gt;options(error = recover, warn = 1)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;To finish off let’s set some names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(offline) &amp;lt;- c(&amp;quot;time&amp;quot;, &amp;quot;scanMac&amp;quot;, &amp;quot;posX&amp;quot;, &amp;quot;posY&amp;quot;, &amp;quot;posZ&amp;quot;,
                    &amp;quot;orientation&amp;quot;, &amp;quot;mac&amp;quot;, &amp;quot;signal&amp;quot;, &amp;quot;channel&amp;quot;, &amp;quot;type&amp;quot;)

offline&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1,181,628 x 10
##    time   scanMac posX  posY  posZ  orientation mac   signal channel type 
##    &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;
##  1 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -38    243700~ 3    
##  2 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -56    242700~ 3    
##  3 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -53    246200~ 3    
##  4 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -65    244200~ 3    
##  5 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -65    242200~ 3    
##  6 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:1~ -66    243200~ 3    
##  7 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -75    241200~ 3    
##  8 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -78    246200~ 3    
##  9 11396~ 00:02:~ 0.0   0.0   0.0   0.0         00:0~ -87    243700~ 3    
## 10 11396~ 00:02:~ 0.0   0.0   0.0   0.0         02:6~ -88    244700~ 1    
## # ... with 1,181,618 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;— &lt;strong&gt;BONUS&lt;/strong&gt; —&lt;/p&gt;
&lt;p&gt;Just wanted to try to get the data in a wide format where each MAC indicator is a column rather than stacked.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define the MAC colums as wide. Because each MAC columns
# has three associated values, I stack them up so there should
# be three rows pero every MAC column
right_col &amp;lt;- tokens[-(1:10)]

right_names &amp;lt;- seq(1, length(right_col), by = 4)

mac_tibble &amp;lt;-
  matrix(right_col[-right_names], nrow = 3, ncol = length(right_names),
         dimnames = list(NULL, right_col[right_names])) %&amp;gt;%
  as_tibble() %&amp;gt;%
  add_column(mac_indicators = c(&amp;quot;signal&amp;quot;, &amp;quot;chanel&amp;quot;, &amp;quot;type&amp;quot;),
             .before = 1)

# Define the first four columns
left_col &amp;lt;- tokens[1:10]

left_names &amp;lt;- seq(1, length(left_col), by = 2)

left_tibble &amp;lt;-
  matrix(left_col[-left_names], nrow = 3, ncol = length(left_names), byrow = TRUE,
         dimnames = list(NULL, left_col[left_names])) %&amp;gt;%
  as_tibble()

# Bind both dfs
mat &amp;lt;- bind_cols(left_tibble, mac_tibble)
mat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 17
##   t         id         pos   `0.0` degree mac_indicators `00:14:bf:b1:97:~
##   &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;            
## 1 11396431~ 00:02:2D:~ 0.0   0.0   0.0    signal         -38              
## 2 11396431~ 00:02:2D:~ 0.0   0.0   0.0    chanel         2437000000       
## 3 11396431~ 00:02:2D:~ 0.0   0.0   0.0    type           3                
## # ... with 10 more variables: `00:14:bf:b1:97:90` &amp;lt;chr&amp;gt;,
## #   `00:0f:a3:39:e1:c0` &amp;lt;chr&amp;gt;, `00:14:bf:b1:97:8d` &amp;lt;chr&amp;gt;,
## #   `00:14:bf:b1:97:81` &amp;lt;chr&amp;gt;, `00:14:bf:3b:c7:c6` &amp;lt;chr&amp;gt;,
## #   `00:0f:a3:39:dd:cd` &amp;lt;chr&amp;gt;, `00:0f:a3:39:e0:4b` &amp;lt;chr&amp;gt;,
## #   `00:0f:a3:39:e2:10` &amp;lt;chr&amp;gt;, `02:64:fb:68:52:e6` &amp;lt;chr&amp;gt;,
## #   `02:00:42:55:31:00` &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s wrap it into a function excluding those which dont have MAC values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;processLine &amp;lt;- function(x) {
  tokens &amp;lt;- str_split(x, pattern = &amp;quot;[;=,]&amp;quot;)[[1]]
  
  if (length(tokens) == 10) return(NULL) # exclude non-MAC lines
  
  right_col &amp;lt;- tokens[-(1:10)]
  
  right_names &amp;lt;- seq(1, length(right_col), by = 4)
  
  mac_tibble &amp;lt;-
    matrix(right_col[-right_names], nrow = 3, ncol = length(right_names),
           dimnames = list(NULL, right_col[right_names]))

  # Define the first four columns
  left_col &amp;lt;- tokens[1:10]
  
  left_names &amp;lt;- seq(1, length(left_col), by = 2)
  
  left_tibble &amp;lt;-
    matrix(left_col[-left_names], nrow = 3, ncol = length(left_names), byrow = TRUE,
           dimnames = list(NULL, left_col[left_names]))

  # Bind both dfs
  mat &amp;lt;- cbind(left_tibble, mac_tibble)
  mat
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s apply it to each line:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tmp &amp;lt;- map(ex_file[!str_sub(ex_file, 1, 1) == &amp;quot;#&amp;quot;], processLine)

# Interestingly, applying as_tibble instead of as.data.frame is
# very slow. So I opt for data frame and then convert the binded
# df to a tibble
final_data &amp;lt;-
  bind_rows(map(tmp, as.data.frame, stringsAsFactors = FALSE)) %&amp;gt;%
  as_tibble() %&amp;gt;%
  add_column(mac_indicators = rep(c(&amp;quot;signal&amp;quot;, &amp;quot;chanel&amp;quot;, &amp;quot;type&amp;quot;), length(unique(.$t))),
             .after = &amp;quot;degree&amp;quot;)

final_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 438,222 x 40
##    t         id        pos   `0.0` degree mac_indicators `00:14:bf:b1:97:~
##    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;            
##  1 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
##  2 11396431~ 00:02:2D~ 0.0   0.0   0.0    chanel         2437000000       
##  3 11396431~ 00:02:2D~ 0.0   0.0   0.0    type           3                
##  4 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
##  5 11396431~ 00:02:2D~ 0.0   0.0   0.0    chanel         2437000000       
##  6 11396431~ 00:02:2D~ 0.0   0.0   0.0    type           3                
##  7 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
##  8 11396431~ 00:02:2D~ 0.0   0.0   0.0    chanel         2437000000       
##  9 11396431~ 00:02:2D~ 0.0   0.0   0.0    type           3                
## 10 11396431~ 00:02:2D~ 0.0   0.0   0.0    signal         -38              
## # ... with 438,212 more rows, and 33 more variables:
## #   `00:14:bf:b1:97:90` &amp;lt;chr&amp;gt;, `00:0f:a3:39:e1:c0` &amp;lt;chr&amp;gt;,
## #   `00:14:bf:b1:97:8d` &amp;lt;chr&amp;gt;, `00:14:bf:b1:97:81` &amp;lt;chr&amp;gt;,
## #   `00:14:bf:3b:c7:c6` &amp;lt;chr&amp;gt;, `00:0f:a3:39:dd:cd` &amp;lt;chr&amp;gt;,
## #   `00:0f:a3:39:e0:4b` &amp;lt;chr&amp;gt;, `00:0f:a3:39:e2:10` &amp;lt;chr&amp;gt;,
## #   `02:64:fb:68:52:e6` &amp;lt;chr&amp;gt;, `02:00:42:55:31:00` &amp;lt;chr&amp;gt;,
## #   `00:04:0e:5c:23:fc` &amp;lt;chr&amp;gt;, `00:30:bd:f8:7f:c5` &amp;lt;chr&amp;gt;, `1.0` &amp;lt;chr&amp;gt;,
## #   `2.0` &amp;lt;chr&amp;gt;, `3.0` &amp;lt;chr&amp;gt;, `4.0` &amp;lt;chr&amp;gt;, `5.0` &amp;lt;chr&amp;gt;, `6.0` &amp;lt;chr&amp;gt;,
## #   `7.0` &amp;lt;chr&amp;gt;, `8.0` &amp;lt;chr&amp;gt;, `9.0` &amp;lt;chr&amp;gt;, `10.0` &amp;lt;chr&amp;gt;, `11.0` &amp;lt;chr&amp;gt;,
## #   `12.0` &amp;lt;chr&amp;gt;, `13.0` &amp;lt;chr&amp;gt;, `00:e0:63:82:8b:a9` &amp;lt;chr&amp;gt;,
## #   `02:37:fd:3b:54:b5` &amp;lt;chr&amp;gt;, `02:2e:58:22:f1:ac` &amp;lt;chr&amp;gt;,
## #   `02:42:1c:4e:b5:c0` &amp;lt;chr&amp;gt;, `02:0a:3d:06:94:88` &amp;lt;chr&amp;gt;,
## #   `02:5c:e0:50:49:de` &amp;lt;chr&amp;gt;, `02:4f:99:43:30:cd` &amp;lt;chr&amp;gt;,
## #   `02:b7:00:bb:a9:35` &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There we go! It’s a bit refreshing to work on datasets that are not pre-cleaned for you.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scraping at scale: daily scraping to your database</title>
      <link>/blog/2018-01-31-scraping-at-scale-daily-scraping-to-your-database/scraping-at-scale-daily-scraping-to-your-database/</link>
      <pubDate>Wed, 31 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/2018-01-31-scraping-at-scale-daily-scraping-to-your-database/scraping-at-scale-daily-scraping-to-your-database/</guid>
      <description>&lt;p&gt;I’ve been working on a personal project to gather daily data from public bicycles in Barcelona to create a historical timeline of a few stations. Since the data is only available live, I had to scrape the data and store in a database daily. This is a short tutorial showing the steps I had to take to setup a database on my remote server and connect both from my local computer as well as from my server. I also show the R script that scrapes data, connects to the server and appends the information every day for a certain amouint of time.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: This worked for my Digital Ocean droplet 512 MB and 20 GB disk with Ubuntu 16.04.3 x64.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let’s get to it. It’s better to do &lt;em&gt;ALL&lt;/em&gt; of this as a user in your server but remember to append &lt;code&gt;sudo&lt;/code&gt; to everything. Nonetheless, beware of problems like the ones I encountered. For example, when installing R packages that where ran by &lt;code&gt;cron&lt;/code&gt; in a script, if installed through a non-root user the packages were said to be &lt;code&gt;&#39;not installed&#39;&lt;/code&gt; (when I fact running the script separately was fine). However, when I installed the packages logged in as root the packages were installed successfully.&lt;/p&gt;
&lt;div id=&#34;setting-up-the-data-base&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting up the data base&lt;/h2&gt;
&lt;p&gt;All steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-r-on-ubuntu-16-04-2&#34;&gt;Install R&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-mysql-on-ubuntu-16-04&#34;&gt;Install MySQL&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Type &lt;code&gt;mysql -u root -p&lt;/code&gt; to log in to MySQL&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Follow these steps to create an empty table within a database&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;CREATE DATABASE bicing;
USE bicing;
CREATE TABLE bicing_station (id VARCHAR(30), slots VARCHAR(30), bikes VARCHAR(30), status VARCHAR(30), time VARCHAR(30), error VARCHAR(30));&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-set-up-a-remote-database-to-optimize-site-performance-with-mysql&#34;&gt;This&lt;/a&gt; is an outdated guide by Digital Ocean which might be helpful. Some of the steps below are taken from that guide.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Alter &lt;code&gt;sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf&lt;/code&gt; and change &lt;code&gt;bind-address&lt;/code&gt; to have the ‘0.0.0.0’ This is so your server can listen to IP’s from outside the localhost network.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create two users to access the data base: a user from your local computer and a user from your server.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;mysql -u root -p # Log in to MySQL. -u stands for user and -p for password&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;/* Create user for local computer. Note that when username and ip are in &amp;#39;&amp;#39; they need to be in those quotes. Also, the ip address you can find easily by writing what&amp;#39;s my ip in Google*/

CREATE USER &amp;#39;username&amp;#39;@&amp;#39;ip_address_of_your_computer&amp;#39; IDENTIFIED BY &amp;#39;password&amp;#39;;
GRANT ALL ON bicing.* TO username@ip_address_of_your_computer;

/* Create user for server. For this user don&amp;#39;t change localhost as that already specifies that it belongs to the same computer. */

CREATE USER &amp;#39;username&amp;#39;@&amp;#39;localhost&amp;#39; IDENTIFIED BY &amp;#39;password&amp;#39;;
GRANT ALL ON bicing.* TO username@localhost;

/* Make sure the privileges are isntalled */
FLUSH PRIVILEGES;

quit /* To quit MySQL*/&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Test whether the access worked for both users&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Login from your server. Replace username for your username 
# -u stands for user and -p will ask for your password 
mysql -u username -h localhost -p


# Login from your LOCAL computer. Replace username for your username and your_server_ip from the server&amp;#39;s IP
mysql -u username -h your_server_ip -p&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Now install &lt;code&gt;odbc&lt;/code&gt; in your Ubuntu server. I follow &lt;a href=&#34;I%20followed%20this:%20https://askubuntu.com/questions/800216/installing-ubuntu-16-04-lts-how-to-install-odbc&#34;&gt;this&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo mkdir mysql &amp;amp;&amp;amp; cd mysql

# Download odbc in mysql folder
sudo wget https://dev.mysql.com/get/Downloads/Connector-ODBC/5.3/mysql-connector-odbc-5.3.9-linux-ubuntu16.04-x86-64bit.tar.gz

# Unzip it and copy it somewhere.
sudo tar -xvf mysql-connector-odbc-5.3.9-linux-ubuntu16.04-x86-64bit.tar.gz 
sudo cp mysql/mysql-connector-odbc-5.3.9-linux-ubuntu16.04-x86-64bit/lib/libmyodbc5a.so /usr/lib/x86_64-linux-gnu/odbc/
# If the odbc folder doesn&amp;#39;t exists, create it with mkdir /usr/lib/x86_64-linux-gnu/odbc/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note: you might need to change the url’s and directories to a &lt;strong&gt;newer&lt;/strong&gt; version of &lt;code&gt;odbc&lt;/code&gt; so don’t simply copy and paste the links from below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create and update the &lt;code&gt;odbc&lt;/code&gt; settings.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo touch /etc/odbcinst.ini

sudo nano /etc/odbcinst.ini

# And add

[MySQL Driver]
Description = MySQL
Driver = /usr/lib/x86_64-linux-gnu/odbc/libmyodbc5a.so
Setup = /usr/lib/x86_64-linux-gnu/odbc/libodbcmyS.so
FileUsage = 1

# close the nano
# And continue

sudo touch /etc/odbc.ini

sudo nano /etc/odbc.ini

# and add

[MySQL]
Description           = MySQL connection to database
Driver                = MySQL Driver
Database              = dbname
Server                = 127.0.0.1
User                  = root
Password              = password
Port                  = 3306
Socket                = /var/run/mysqld/mysqld.sock

# Change Database to your database name
# The password to your root password

# Finally, run

sudo ln -s /var/run/mysqld/mysqld.sock /tmp/mysql.sock

# to move the socket to the folder where the DBI pkgs
# search for it

# Finish by

sudo service mysql restart;

# to restart mysql server&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;connecting-to-the-database-locally-and-remotely&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Connecting to the database locally and remotely&lt;/h2&gt;
&lt;p&gt;From my local computer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(DBI)
library(RMySQL)

con &amp;lt;- dbConnect(MySQL(), # If the database changed, change this
                 host = your_server_ip, # in &amp;quot;&amp;quot; quotes.
                 dbname = &amp;quot;bicing&amp;quot;,
                 user = username, # remember to change to your username (in quotes)
                 password = password, # remember to change to your password (in quotes)
                 port = 3306)

dbListTables(con)

bike_stations &amp;lt;- dbReadTable(con, &amp;quot;bicing_station&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From R in the server&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;con &amp;lt;- dbConnect(RMySQL::MySQL(),
                 dbname = &amp;quot;bicing&amp;quot;,
                 user = username, # remember to change to your username (in quotes)
                 password = password, # remember to change to your password (in quotes)
                 port = 3306)

dbListTables(con)

bike_stations &amp;lt;- dbReadTable(con, &amp;quot;bicing_station&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That did it for me. Now I could connect to the database from R from my local computer and from the server itself.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;scraping-automatically&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scraping automatically&lt;/h2&gt;
&lt;p&gt;So far you should have a database in your server which you can connect locally and remotely. I assume you have a working script that can actually add/retrieve information from the remote database. Here I will explain how to set up the scraping to run automatically as a &lt;code&gt;cron&lt;/code&gt; job and get a final email with the summary of the scrape.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Create a text file to save the output of the scraping with &lt;code&gt;sudo touch scrape_log.txt&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write &lt;code&gt;cron -e&lt;/code&gt; logged in as your non-root user.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At the bottom of the interactive &lt;code&gt;cron&lt;/code&gt; specify these options:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;SHELL=/bin/bash # the path to the predetermined program to run cron jobs. Default bash

PATH=bla/bla/bla # PATH I’m not sure what’s for but I pasted the output of echo $PATH.

HOME= your/dr/ofinteres # Path to the directory where the scripts will be executed (where the script is)

MAILTO=&amp;quot;your@email.com&amp;quot; # Your email to receive emails

# The actual cron jobs. Below each job I explain them
30-59 11 * * * /usr/bin/Rscript scrape_bicing.R &amp;gt;&amp;gt;scrape_log.txt 2&amp;gt;&amp;amp;1

# Run this cron job from 11:30 to 11:59 every day (*), every month (*), every year(*): 30-59 11 * * *

# Use R to run the script: /usr/bin/Rscript
# You can find this directory with which Rscript

# Execute the file scrape_bicing.R (which is looked for in the HOME variable specified above)
# &amp;gt;&amp;gt;scrape_log.txt 2&amp;gt;&amp;amp;1: Save the output to scrape_log.txt (which we created) and DON&amp;#39;T send an email
# because we don&amp;#39;t want to received 29 emails.

00 12 * * * /bin/bash sql_query.sh
# Instead of receiving 29 emails, run a query the minute after the scraping ends
# to filter how many rows were added between 11:30 and 11:59
# By default it will send the result of the query to your email&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great but what does &lt;code&gt;scrape_bicing.R&lt;/code&gt; have?&lt;/p&gt;
&lt;p&gt;The script should do something along the lines of:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries
library(httr)
library(DBI)
library(RMySQL)

# The url of your api
api_url &amp;lt;- &amp;quot;bla/bla&amp;quot;

# Wrap GET so that whenever the request fails it returns an R error
my_GET &amp;lt;- function(x, config = list(), ...) {
  stop_for_status(GET(url = x, config = config, ...))
}

# If it can&amp;#39;t connect to the API will throw an R error
test_bike &amp;lt;- my_GET(api_url)


## Do API calls here
## I assume the result is a data.frame or something like that
## It should have the same column names as the SQL database.

# Establish the connection to the database.
# This script is run within the server, so the connection
# should not specify the server ip, it assumes it&amp;#39;s
# the localhost

con &amp;lt;- dbConnect(MySQL(),
                 dbname = database_name, # in &amp;quot;&amp;quot; quotes
                 user = your_user, # in &amp;quot;&amp;quot; quotes
                 password = your_password, # in &amp;quot;&amp;quot; quotes
                 port = 3306)

# Append the table
write_success &amp;lt;-
  dbWriteTable(conn = con, # connection from above
              &amp;quot;table name&amp;quot;, # name of the table to append (in quotes)
              api output, # data frame from the API output
              append = TRUE, row.names = FALSE) # to append instead of overwrite and ignore row.names

# Write your results to the database. In my API call
# I considered many possible errors and coded the request
# very defensively, running the script many times under certain
# scenarios (no internet, getting different results).
# If you get unexpected results from your API request then this step will
# not succeed.


# If the append was successfull, write_success should be TRUE
if (write_success) print(&amp;quot;Append success&amp;quot;) else print(&amp;quot;No success&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Something to keep in mind, by default you can connect from the your local computer to the remote DB by port 3306. This port can be closed if you’re in a public internet network or a network connection from a university. If you can’t connect, make you sort this out with the personnel from that network (it happened to me with my university network).&lt;/p&gt;
&lt;p&gt;What does &lt;code&gt;sql_query.sh&lt;/code&gt; have?&lt;/p&gt;
&lt;p&gt;A very simple SQL query:&lt;/p&gt;
&lt;pre class=&#34;sql&#34;&gt;&lt;code&gt;read PASS &amp;lt; pw.txt /* Read the password from a pw.txt file you create with your user pasword*/

mysql -uroot -p$PASS database_name -e &amp;quot;SELECT id, error_msg, COUNT(*) AS count FROM bicing_station WHERE time &amp;gt;= CONCAT(CURDATE(),&amp;#39; &amp;#39;,&amp;#39;11:30:00&amp;#39;) AND time &amp;lt;= CONCAT(CURDATE(),&amp;#39; &amp;#39;,&amp;#39;12:00:00&amp;#39;) GROUP BY id, error_msg;&amp;quot;

/*
mysql: run mysql

-uroot: specify your mysql username (note there are no spaces)

-p$PASS: -p is for password and $PASS is the variable with the password

database_name: is the data base name

-e: is short for -execute a query

The remaining is the query to execute. I would make sure the query
works by running this same line in the server interactively.

What this query means is to get the counts of the id and error messages
where the time is between the scheduele cron of the API request.

This way I get a summary of the error messages and how many lines were
appended between the time the script should&amp;#39;ve started and should&amp;#39;ve ended
*/
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As stated in the first line of the code chunk, create a text file with your password. You can do so with &lt;code&gt;echo &amp;quot;Your SQL username password&amp;quot; &amp;gt;&amp;gt; pw.txt&lt;/code&gt;. That should allow PASS to read in the password just fine.&lt;/p&gt;
&lt;p&gt;And that should be it! Make sure you run each of these steps separately so that they work on it’s own and you don’get weird errors. This workflow will now run &lt;code&gt;cron&lt;/code&gt; jobs at whatever time you set it, return the output to a text file (in case something bad happens and you want to look at the log) and run a query after it finishes so that you only get one email with a summary of API requests.&lt;/p&gt;
&lt;p&gt;Hope this was helpful!&lt;/p&gt;
&lt;p&gt;PS:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/a-basic-mysql-tutorial&#34;&gt;Basic MySQL tutorial&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I use SQL Workbench to run queries from my local computer&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Brief Analysis of Independent/Unionist Vote in Catalonia</title>
      <link>/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/brief-analysis-of-independent-unionist-vote-in-catalonia/</link>
      <pubDate>Thu, 14 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/brief-analysis-of-independent-unionist-vote-in-catalonia/</guid>
      <description>&lt;div id=&#34;catalan-elections&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Catalan elections&lt;/h2&gt;
&lt;p&gt;On a train from Barcelona-Madrid I started working with an &lt;code&gt;R&lt;/code&gt; package called &lt;code&gt;ggrides&lt;/code&gt;. To my surprise, the package contains one dataset that documents the change in independent/unionist vote for all Catalan municipalities from 1980 to 2015. This is very cool! Needless to say, I left what I was doing and started to dive into the dataset.&lt;/p&gt;
&lt;p&gt;The data looks like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggridges)
Catalan_elections&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20,764 x 4
##    Municipality        Year Option Percent
##    &amp;lt;chr&amp;gt;              &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 Abella de la Conca  1980 Indy      68.4
##  2 Abella de la Conca  1984 Indy      95.7
##  3 Abella de la Conca  1988 Indy      89.4
##  4 Abella de la Conca  1992 Indy      81.7
##  5 Abella de la Conca  1995 Indy      80.0
##  6 Abella de la Conca  1999 Indy      74.7
##  7 Abella de la Conca  2003 Indy      84.4
##  8 Abella de la Conca  2006 Indy      73.2
##  9 Abella de la Conca  2010 Indy      75.9
## 10 Abella de la Conca  2012 Indy      84.0
## # ... with 20,754 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Very straight forward. It’s the ‘Indy’ or Independent vote and the ‘Unionist’ vote from 1980 until 2015. The data is complete for nearly all Municipalities, meaning that the data is available for all years. Only a handful (~ 40) do not have data starting from 1980.&lt;/p&gt;
&lt;p&gt;Basically, I wanted to answer one question: has the indepence vote grown over time? This question is of general interest considering that the topic is being hotly debated in the media and next week new Catalan elections will be held in a scenario never seen before; after independent parties proclaimed unilateral independece and the government seized control of Catalunya. The elections are predicted to be very contested with Independent parties losing some votes.&lt;/p&gt;
&lt;p&gt;With that said, let’s dive into the data!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load my libraries
library(scales)
library(tidyverse)

# Change abbreviated Indy and Unionist to long names
Catalan_elections$Option &amp;lt;- with(Catalan_elections, ifelse(Option == &amp;quot;Indy&amp;quot;, &amp;quot;Independent&amp;quot;, &amp;quot;Unionist&amp;quot;))


# Summarize the median independence/unionist vote for
# all municipalities on the first/last year recorded
avg_pl &amp;lt;-
  Catalan_elections %&amp;gt;%
  group_by(Municipality, Option) %&amp;gt;%
  summarize(first_year = first(Percent, Year),
            last_year = last(Percent, Year)) %&amp;gt;%
  group_by(Option) %&amp;gt;%
  summarize(first_year = median(first_year, na.rm = TRUE),
            last_year = median(last_year, na.rm = TRUE)) %&amp;gt;%
  mutate(id = 1:nrow(.)) %&amp;gt;%
  gather(year, value, -id, -Option)

# Summarize the indy/unionist vote for
# the first/last year for Barcelona
bcn_pl &amp;lt;-
  Catalan_elections %&amp;gt;%
  filter(Municipality == &amp;quot;Barcelona&amp;quot;) %&amp;gt;%
  group_by(Municipality, Option) %&amp;gt;%
  summarize(first_year = first(Percent, Year),
            last_year = last(Percent, Year)) %&amp;gt;%
  mutate(id = 1:nrow(.)) %&amp;gt;%
  gather(year, value, ends_with(&amp;quot;year&amp;quot;))

# Create a base parallel plot with both
# unionist/independence votes pooled
base_plot &amp;lt;-
  Catalan_elections %&amp;gt;%
  group_by(Municipality, Option) %&amp;gt;%
  summarize(first_year = first(Percent, Year),
            last_year = last(Percent, Year)) %&amp;gt;%
  mutate(id = paste0(Municipality, &amp;quot;_&amp;quot;, Option)) %&amp;gt;%
  gather(year, value, ends_with(&amp;quot;year&amp;quot;)) %&amp;gt;%
  ggplot(aes(year, value)) +
  geom_point(alpha = 0.1, size = 2) +
  geom_line(aes(group = id), alpha = 0.1)

# Add the median summary line for both indy/unionist
median_plot &amp;lt;-
  base_plot +
  geom_point(data = avg_pl, aes(year, value),
            colour = &amp;quot;red&amp;quot;, alpha = 0.5, size = 2) +
  geom_line(data = avg_pl, aes(year, value, group = id),
            colour = &amp;quot;red&amp;quot;, alpha = 0.5, size = 2)

# Add the change of Barcelona for both indy/unionist vote
bcn_plot &amp;lt;-
  median_plot +
  geom_point(data = bcn_pl, aes(year, value),
             colour = &amp;quot;blue&amp;quot;, alpha = 0.5, size = 2) +
  geom_line(data = bcn_pl, aes(year, value, group = id),
            colour = &amp;quot;blue&amp;quot;, alpha = 0.5, size = 2)


# Separate the plot for indy/unionist in different
# panels and add pretty options
pretty_plot &amp;lt;-
  bcn_plot +
  scale_x_discrete(name = NULL,
                   labels = c(&amp;quot;1980&amp;quot;,
                              &amp;quot;2015&amp;quot;)) +
  scale_y_continuous(name = &amp;quot;% of votes in favour of:&amp;quot;,
                     breaks = seq(0, 100, 20),
                     labels = percent(seq(0, 1, 0.2))) +
  facet_wrap(~ Option, strip.position = &amp;quot;bottom&amp;quot;) +
  labs(
    title = &amp;quot;Independence/Unionist vote in Catalonia in three decades&amp;quot;,
    subtitle = &amp;quot;Red line is the median change for all municipalities - Blue line is Barcelona&amp;quot;,
    caption = &amp;quot;Data collected by @marcbeldata - Plot and analisis by @cimentadaj&amp;quot;
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, family = &amp;quot;Arial-BoldMT&amp;quot;),
    plot.subtitle = element_text(size = 12, color = &amp;quot;#666666&amp;quot;),
    plot.caption = element_text(size = 12, color = &amp;quot;#666666&amp;quot;),
    strip.text.x = element_text(size = 14),
    axis.title.y = element_text(size = 16),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 14)
  )
# Final plot
pretty_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/2017-12-14-brief-analysis-of-independent-unionist-vote-in-catalonia_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I was very surprised with this plot. On the left panel we can see the increase of independence votes in the last 35 years. The red line is the median change for all municipalities. There is a huge average increase from around 49% to over 70%. In fact, it’s not just an artifact of mean/median with big variance. If we look at the bulk of the distribution on the right line and then the left line, we see an upwards shift in the whole distribution.&lt;/p&gt;
&lt;p&gt;On the other hand, the unionist vote seems to have decreased! The left/right distributions seem to be very similar (but it looks like the distribution of the right line has some outliers shifting upwards). But remember something: these are all municipalities. Municipalities might have 1000 citizen or even less! Consider the lonely town in a mountain with 50 people voting for independent parties: that’s also a municipality.&lt;/p&gt;
&lt;p&gt;It is for this reason that we need to pay attention to places like Barcelona, which have over 1 million residents and definately weigh in more in proportion. And that’s where the interesting thing about this plot arises: the Barcelona change is practically the same. Not only have the votes increased very very similarly for both sides, but they’re also at the same level of support. Both blue lines look pretty much identical.&lt;/p&gt;
&lt;p&gt;Don’t forget: small differences &lt;strong&gt;can&lt;/strong&gt; make a difference, specially in elections. Perhaps they &lt;strong&gt;are&lt;/strong&gt; different but we need to take a closer look.&lt;/p&gt;
&lt;p&gt;Let’s plot the independence/unionist evolution only for Barcelona.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot for indy/unionist vote over time only for Barcelona
Catalan_elections %&amp;gt;%
  filter(Municipality == &amp;quot;Barcelona&amp;quot;) %&amp;gt;%
  ggplot(aes(Year, Percent, group = Option, colour = Option)) +
  geom_line(alpha = 0.5, size = 2) +
  scale_x_continuous(name = NULL) +
  scale_colour_discrete(name = NULL) +
  scale_y_continuous(name = &amp;quot;% of votes in favour of:&amp;quot;,
                     lim = c(0, 100),
                     breaks = seq(0, 100, 20),
                     labels = percent(seq(0, 1, 0.2))) +
  labs(
    title = &amp;quot;Overtime votes for independence/unionist in Barcelona&amp;quot;,
    caption = &amp;quot;Data collected by @marcbeldata - Plot and analisis by @cimentadaj&amp;quot;
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, family = &amp;quot;Arial-BoldMT&amp;quot;),
    plot.subtitle = element_text(size = 12, color = &amp;quot;#666666&amp;quot;),
    plot.caption = element_text(size = 12, color = &amp;quot;#666666&amp;quot;),
    legend.position = &amp;quot;top&amp;quot;,
    legend.text = element_text(size = 14),
    strip.text.x = element_text(size = 14),
    axis.title.y = element_text(size = 16),
    axis.text.y = element_text(size = 12),
    axis.text.x = element_text(size = 14)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-12-14-brief-analysis-of-independentunionist-vote-in-catalonia/2017-12-14-brief-analysis-of-independent-unionist-vote-in-catalonia_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Despite most municipalities are for independence, Barcelona, by only a small margin, has a majority of people voting for unionist parties. Be aware that these votes are not ‘referendums’ carried out every year. These are votes towards independence/unionist parties, which is a different thing. Also note that these are not predictions/forecasts, so they don’t have uncertainty intervals or margins of errors. This is empirical data from voter turnout.&lt;/p&gt;
&lt;p&gt;I also tried other big municipalities such as Sabadell and found that unionism trumps over independence much strongly. Yet in others like Lleida, Independence seems to be on top. For a look at specific municipalities, &lt;a href=&#34;http://marcbeldata.github.io/ggjoy-Catalan-vote-2015/&#34;&gt;check the post by Marc Belzunces&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-note-on-next-weeks-elections&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A note on next week’s elections&lt;/h2&gt;
&lt;p&gt;This data takes us as far as 2015. Catalonia has suffered dramatic changes since 2015 specially due to the independence movement. These data are most likely not a good representation of what’s gonna happen next week. Big municipalities have usually been majority unionists according to polls, but the differences are tiny and we’ve seen dramatic changes with independence parties proclaiming unilateral independence. There are good attempts at predicting catalan elections (&lt;a href=&#34;https://politica.elpais.com/politica/2017/12/07/ratio/1512647178_322229.html&#34;&gt;in Spanish&lt;/a&gt;) so tune in next week to see what happens.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How long should I wait for my bike?</title>
      <link>/blog/2017-12-01-how-long-should-i-wait-for-my-bike/how-long-should-i-wait-for-my-bike/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-12-01-how-long-should-i-wait-for-my-bike/how-long-should-i-wait-for-my-bike/</guid>
      <description>&lt;p&gt;I’ve just started a project which I’m very excited about. Everyday I take my bike to work and most days I have one of two problems. First, whenever I get to my station there are no bikes available; no problem, there’s an app that shows the closest stations with bikes available. The problem is that these stations might be far and sometimes I’m relucant to walk that much. I’d love for bicing to give me some time estimation until a new bike arrives.&lt;/p&gt;
&lt;p&gt;Second, whenever you’re trying to return a bike the station might not have any parking spaces available. Similarly, it would be very cool if bicing (the public bicycle company) gave me an estimate of how much time I should wait until a new bike will be taken. I started thinking on how I could implement this and started looking for bicing data online. To my surprise, bicing actually releases their &lt;strong&gt;live&lt;/strong&gt; data as a json! But for this type of estimation I need historical data. I want to know the pattern usage of the station and use that information for the prediction.&lt;/p&gt;
&lt;p&gt;With that idea in mind, I got to work. I needed to set up my Virtual Private Server (VPS) to pull the data from the bicing API everyday. Because this is still a work in progress, I will only describe here how I set my VPS to scrape the bicing API everyday and how I set &lt;code&gt;cron&lt;/code&gt; to send me an email after every scrape.&lt;/p&gt;
&lt;p&gt;I have a VPS from &lt;a href=&#34;https://www.digitalocean.com/&#34;&gt;Digital Ocean&lt;/a&gt; with an Ubuntu OS and 512 mb of RAM and 2 GB of hard disk. That’s enough for this task because the data should not be very big, even in the long run. In any case you can adjust for your VPS to have more memory/ram without losing information. Assuming you have &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-r-on-ubuntu-16-04-2&#34;&gt;R installed in your Ubuntu VPS&lt;/a&gt; with your favorite packages, then make sure your script works by running &lt;code&gt;Rscript path/to/your/script.R&lt;/code&gt;. It might be better to type &lt;code&gt;which Rscript&lt;/code&gt; in the terminal and paste the path to the executable, similar to &lt;code&gt;/usr/bin/Rscript path/to/your/script.R&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;My workflow is as follows: I first create an empty dataset saved as &lt;code&gt;.rds&lt;/code&gt; and my script reads the data, scrapes the bicing data and then saves the data by appending both the empty and the scraped data. It finishes by saving the same &lt;code&gt;.rds&lt;/code&gt; for a later scrape. I tested this very thoroughly to make sure the script wouldn’t fail and I always get the expected data.&lt;/p&gt;
&lt;p&gt;All good so far, right? This took me no time. The hard problem came when setting the &lt;code&gt;cron&lt;/code&gt; job, which is a way of scheduling tasks in OSx and Ubuntu. For an explanation of how &lt;code&gt;cron&lt;/code&gt; works, check out how I set &lt;a href=&#34;blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/index.html&#34;&gt;my PISA twitter bot&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, make sure you have &lt;code&gt;cron&lt;/code&gt; &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-use-cron-to-automate-tasks-on-a-vps&#34;&gt;installed&lt;/a&gt;. I followed &lt;strong&gt;a lot&lt;/strong&gt; of tutorials and dispered information. What worked for me perhaps does not work for you, but here it is.&lt;/p&gt;
&lt;p&gt;Type &lt;code&gt;crontab -e&lt;/code&gt; and the cron interface should appear. The lines starting with &lt;code&gt;#&lt;/code&gt; are coments, so scroll down until the end of the comments. First we have to set a few environmental variables that &lt;code&gt;cron&lt;/code&gt; uses to execute your script. I followed &lt;a href=&#34;http://krisjordan.com/essays/timesaving-crontab-tips&#34;&gt;these tips&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;When I finished my crontab looked like this:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;SHELL=/bin/bash
PATH=/home/cimentadaj/bin:/home/cimentadaj/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
HOME=/home/cimentadaj/bicycle
MAILTO=my_email # set email here!
15,50 16  * * * /usr/bin/Rscript scrape_bicing.R&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;SHELL is the path to the pre-determined program to run on the cron job. Be default I set it to bash (but it could be anything else you want).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PATH I’m not sure what’s for but I pasted the output of &lt;code&gt;echo $PATH&lt;/code&gt;, as the tips suggested.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;HOME is the root directory where the script will be executed, I set it to where the script is (or where your project is at).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;MAILTO is the email where I will get the cron job alert when it finishes.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;15,50 16  * * * /usr/bin/Rscript scrape_bicing.R&lt;/code&gt; is the schedule, program and script to run. Here I set arbitrary times, so the the script is scheduled to run at &lt;code&gt;16:15&lt;/code&gt; and &lt;code&gt;16:50&lt;/code&gt; every day, every month and every year. I will run using &lt;code&gt;Rscript&lt;/code&gt; and the name of the script to run.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;WARNING:&lt;/strong&gt; remember that the &lt;code&gt;cron&lt;/code&gt; is set relative to the time of where your server is. Mine did not have the same timezone of where I lived, so I had to set the &lt;code&gt;cron&lt;/code&gt; one hour before of my actual time. Use &lt;code&gt;date&lt;/code&gt; to print the time of your VPS.&lt;/p&gt;
&lt;p&gt;Even after this, the &lt;code&gt;cron&lt;/code&gt; job was still not running. Nothing, no email, no log, no change in the data. I then figured out that Ubuntu systems have some &lt;a href=&#34;https://serverfault.com/a/754104&#34;&gt;pecularities&lt;/a&gt; when it comes to &lt;code&gt;cron&lt;/code&gt;. So I went to &lt;code&gt;./etc/&lt;/code&gt; and renamed every &lt;code&gt;cron.&lt;/code&gt; file for &lt;code&gt;cron-&lt;/code&gt; with &lt;code&gt;rename &#39;s/cron./cron-/g&#39; *&lt;/code&gt;, thanks to this &lt;a href=&#34;https://stackoverflow.com/a/20657563/3617958&#34;&gt;answer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Run again and it worked! Great. However, I didn’t receive an email stating that the &lt;code&gt;cron&lt;/code&gt; job finished. I looked up many solutions and ended up installing &lt;code&gt;ssmtp&lt;/code&gt; which is a library for sending emails from terminal. I won’t bore you with the details. Here are the steps I took:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install &lt;code&gt;ssmtp&lt;/code&gt; with &lt;code&gt;sudo apt-get update&lt;/code&gt; and &lt;code&gt;sudo apt-get install ssmtp&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Edit &lt;code&gt;ssmtp.conf&lt;/code&gt; with &lt;code&gt;sudo nano /etc/ssmtp/ssmtp.conf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s the config that worked for me using &lt;code&gt;gmail&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;# Config file for sSMTP sendmail
#
# The person who gets all mail for userids &amp;lt; 1000
# Make this empty to disable rewriting.
root=your_email@gmail.com

# The place where the mail goes. The actual machine name is required no 
# MX records are consulted. Commonly mailhosts are named mail.domain.com
mailhub=smtp.gmail.com:587

AuthUser=your_email@gmail.com
AuthPass=your_password
UseTLS=YES
UseSTARTTLS=yes
TLS_CA_FILE=/etc/ssl/certs/ca-certificates.crt

# Where will the mail seem to come from?
#rewriteDomain=gmail.com

# The full hostname
hostname=your_host_name

# Are users allowed to set their own From: address?
# YES - Allow the user to specify their own From: address
# NO - Use the system generated From: address
#FromLineOverride=YES&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Three caveats that took me a lot of time to figure out.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;First, &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-use-google-s-smtp-server&#34;&gt;some docs&lt;/a&gt; say you should use another port in &lt;code&gt;mailhub&lt;/code&gt;, but &lt;code&gt;587&lt;/code&gt; worked for me.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;TLS_CA_FILE&lt;/code&gt;: make sure that &lt;a href=&#34;https://askubuntu.com/questions/342484/etc-pki-tls-certs-ca-bundle-crt-not-found&#34;&gt;this file exists&lt;/a&gt;! For Ubuntu/Debian the file is at &lt;code&gt;/etc/ssl/certs/ca-certificates.crt&lt;/code&gt; while on other platforms it might be in &lt;code&gt;/etc/pki/tls/certs/ca-bundle.crt&lt;/code&gt;. Note the different file names!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;hostname&lt;/code&gt; should be the result of typing &lt;code&gt;hostname&lt;/code&gt; in your server.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lastly, I also added the line &lt;code&gt;root:your_EMAIL_@gmail.com:smtp.gmail.com:587&lt;/code&gt; with &lt;code&gt;sudo nano /etc/ssmtp/revaliases&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;After an entire day figuring out all this information, the &lt;code&gt;cron&lt;/code&gt; job worked! I now set my &lt;code&gt;cron&lt;/code&gt; job and whenever it finished I receive an email directly showing the log of the script.&lt;/p&gt;
&lt;p&gt;I wrote this primarily for me not to forget any of this, but it might be useful for other people.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An introduction to the ess package</title>
      <link>/blog/2017-11-23-an-introduction-to-the-ess-package/an-introduction-to-the-ess-package/</link>
      <pubDate>Thu, 23 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-11-23-an-introduction-to-the-ess-package/an-introduction-to-the-ess-package/</guid>
      <description>&lt;p&gt;The &lt;code&gt;ess&lt;/code&gt; package is designed to download the ESS data as easily as possible. It has a few helper functions to download rounds, rounds for a selected country and to show which rounds/countries are available. In this tutorial I will walk you through how these functions work.&lt;/p&gt;
&lt;p&gt;Before using the package it is necessary for you to sign up at &lt;a href=&#34;http://www.europeansocialsurvey.org/&#34; class=&#34;uri&#34;&gt;http://www.europeansocialsurvey.org/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s do it together.&lt;/p&gt;
&lt;p&gt;When you enter the website go the to topmost left corner and click on &lt;code&gt;Sign in/Register&lt;/code&gt;. Under the email box, click on &lt;code&gt;New user?&lt;/code&gt; and fill out your personal information. Click on &lt;code&gt;Register&lt;/code&gt; and check your email inbox. You should’ve received an email from the ESS with an activation link. Click on that link and voila! We’re ready to go.&lt;/p&gt;
&lt;p&gt;We can install and load the package with this code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;ess&amp;quot;, dependencies = TRUE)
library(ess)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;download-country-rounds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Download country rounds&lt;/h2&gt;
&lt;p&gt;First things first, do we know if Spain participated in the European Social Survey? &lt;code&gt;ess&lt;/code&gt; has &lt;code&gt;show_countries()&lt;/code&gt; that automatically searchers for all countries that participated. The nice thing is that these (an all other functions from the package) interactively check this information on the website, so any changes should be also visible immediately in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show_countries()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Spain is there! But which rounds did Spain participate? For that, the usual way would be to visit &lt;a href=&#34;http://www.europeansocialsurvey.org/data/country_index.html&#34; class=&#34;uri&#34;&gt;http://www.europeansocialsurvey.org/data/country_index.html&lt;/a&gt; and look for it. &lt;code&gt;ess&lt;/code&gt; provides the function &lt;code&gt;show_country_rounds()&lt;/code&gt; which returns all the available rounds from that website.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show_country_rounds(&amp;quot;Spain&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Remember to type exactly the same name provided by &lt;code&gt;show_countries()&lt;/code&gt; because these functions are case sensitive. How do we download this data?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;your_email &amp;lt;- &amp;quot;your email here&amp;quot;

spain_seven &amp;lt;- ess_country(
  country = &amp;quot;Spain&amp;quot;,
  rounds = 7,
  your_email = your_email
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That easy! Now you have &lt;code&gt;spain_seven&lt;/code&gt; with the 7th round for Spain. If you wanted to download more rounds, you can specify them in the rounds section.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spain_three &amp;lt;- ess_country(
  country = &amp;quot;Spain&amp;quot;,
  rounds = c(1, 3, 5),
  your_email = your_email
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you’re interested in downloading all available waves from the start, use &lt;code&gt;ess_all_cntrounds()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ess_all_cntrounds(&amp;quot;Spain&amp;quot;, your_email)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;download-complete-rounds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Download complete rounds&lt;/h2&gt;
&lt;p&gt;What about specific rounds for all countries? &lt;code&gt;ess&lt;/code&gt; provides the same set of functions: &lt;code&gt;show_rounds()&lt;/code&gt; for available rounds, &lt;code&gt;ess_rounds()&lt;/code&gt; for specific rounds and &lt;code&gt;ess_all_rounds()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;show_rounds()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s grab the first three rounds, although this might take a bit more time than for country rounds!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;three_rounds &amp;lt;-
  ess_rounds(
  c(1, 3),
  your_email
)

three_rounds[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, you can download all available rounds with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_rounds &amp;lt;- ess_all_rounds(your_email)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;download-for-stata&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Download for Stata&lt;/h2&gt;
&lt;p&gt;To download Stata files you can use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ess_country(
  &amp;quot;Spain&amp;quot;,
  1:2,
  your_email,
  only_download = TRUE,
  output_dir = &amp;quot;./ess&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;only_download&lt;/code&gt; argument makes sure that it won’t return anything in R, and &lt;code&gt;output_dir&lt;/code&gt; will be where the data is saved. If you supply a non existent directory it will create it on the fly.&lt;/p&gt;
&lt;p&gt;rounds can be downloaded in the same way with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ess_rounds(
  1:2,
  your_email,
  only_download = TRUE,
  output_dir = &amp;quot;./ess&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That easy! &lt;code&gt;ess&lt;/code&gt; will continue to evolve in the future and there are some of the features already in the to-do list.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Add a &lt;code&gt;*_themes()&lt;/code&gt; family of function for topics; see &lt;a href=&#34;http://www.europeansocialsurvey.org/data/module-index.html&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Download data in SPSS and SAS format&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stata files (as well as SPSS and SAS) need to be pre-processed before reading into R (ex: run a do file before reading into R)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The repository and development version of the package can be found at &lt;a href=&#34;https://github.com/cimentadaj/ess&#34; class=&#34;uri&#34;&gt;https://github.com/cimentadaj/ess&lt;/a&gt; and please report any bugs/issues/improvements &lt;a href=&#34;https://github.com/cimentadaj/ess/issues&#34;&gt;here&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Thanks.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>1..2..3..check!</title>
      <link>/blog/2017-11-18-123check/1-2-3-check/</link>
      <pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-11-18-123check/1-2-3-check/</guid>
      <description>&lt;p&gt;1..2..3..check! This is my first post using &lt;a href=&#34;https://cran.r-project.org/web/packages/blogdown/index.html&#34;&gt;blogdown&lt;/a&gt;. I migrated my website from &lt;code&gt;Jekyll&lt;/code&gt; to &lt;code&gt;Hugo&lt;/code&gt; and although it took me around 2 days to tweak everything to where I wanted it, the process wasn’t so bad after all. As a celebration, I though of doing a quick analysis!&lt;/p&gt;
&lt;p&gt;I live in Barcelona, a city known for sunny weather, great football and for wanting to become an independent state. In fact, just recently there was an unsuccessful attempt to break parts with the Spanish nation. Without delving too much into it, I searched for any question related to nationalism into the European Social Survey and downloaded the last available wave using the &lt;a href=&#34;https://cran.r-project.org/web/packages/ess/index.html&#34;&gt;ess&lt;/a&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ess)
library(cimentadaj)
library(tidyverse)

spain_df &amp;lt;- ess_country(&amp;quot;Spain&amp;quot;, 7, &amp;quot;your_email@gmail.com&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;spain_df&lt;/code&gt; is now a data frame containing the 7th ESS round. Next we have to recode the autonomous communities which are in a ESS format. We’re interested in two variables, &lt;code&gt;region&lt;/code&gt; and &lt;code&gt;fclcntr&lt;/code&gt;, the second one asking whether the person feels closer to Spain.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note: did you notice the comunidades &lt;code&gt;tibble&lt;/code&gt; there? I pasted that with no effort with &lt;a href=&#34;https://cran.r-project.org/web/packages/datapasta/index.html&#34;&gt;datapasta&lt;/a&gt;! If you’re using Rstudio, just copy the table from your source and use Shift + CMD + T (on a mac) to paste it as a very nice tibble.&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;comunidades &amp;lt;- tibble::tribble(
  ~ round,                      ~ country,
  &amp;quot;ES11&amp;quot;,                      &amp;quot;Galicia&amp;quot;,
  &amp;quot;ES12&amp;quot;,      &amp;quot;Asturias&amp;quot;,
  &amp;quot;ES13&amp;quot;,                   &amp;quot;Cantabria&amp;quot;,
  &amp;quot;ES21&amp;quot;,                  &amp;quot;País Vasco&amp;quot;,
  &amp;quot;ES22&amp;quot;,  &amp;quot;Navarra&amp;quot;,
  &amp;quot;ES23&amp;quot;,                    &amp;quot;La Rioja&amp;quot;,
  &amp;quot;ES24&amp;quot;,                      &amp;quot;Aragón&amp;quot;,
  &amp;quot;ES30&amp;quot;,         &amp;quot;Madrid&amp;quot;,
  &amp;quot;ES41&amp;quot;,             &amp;quot;Castilla y León&amp;quot;,
  &amp;quot;ES42&amp;quot;,          &amp;quot;Castilla-La Mancha&amp;quot;,
  &amp;quot;ES43&amp;quot;,                 &amp;quot;Extremadura&amp;quot;,
  &amp;quot;ES51&amp;quot;,                    &amp;quot;Cataluña&amp;quot;,
  &amp;quot;ES52&amp;quot;,        &amp;quot;Valenciana&amp;quot;,
  &amp;quot;ES53&amp;quot;,               &amp;quot;Illes Balears&amp;quot;,
  &amp;quot;ES61&amp;quot;,                   &amp;quot;Andalucía&amp;quot;,
  &amp;quot;ES62&amp;quot;,            &amp;quot;Región de Murcia&amp;quot;,
  &amp;quot;ES63&amp;quot;,    &amp;quot;Ceuta&amp;quot;,
  &amp;quot;ES64&amp;quot;,  &amp;quot;Melilla&amp;quot;,
  &amp;quot;ES70&amp;quot;,                    &amp;quot;Canarias&amp;quot;
)

var_recode &amp;lt;- reverse_name(attr(spain_df$fclcntr, &amp;quot;labels&amp;quot;))

ready_df &amp;lt;-
  spain_df %&amp;gt;%
  transmute(com_aut = deframe(comunidades)[region],
         close_cnt = factor(var_recode[fclcntr],
                            levels = var_recode[1:4],
                            ordered = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next up we calculate the percentage of respondents within each category and within each region and visualize it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_table &amp;lt;-
  ready_df %&amp;gt;%
  count(com_aut, close_cnt) %&amp;gt;%
  group_by(com_aut) %&amp;gt;%
  mutate(perc = (n / n())) %&amp;gt;%
  filter(!is.na(com_aut), !is.na(close_cnt))


perc_table %&amp;gt;%
  ggplot(aes(close_cnt, perc)) +
  geom_col() +
  facet_wrap(~ com_aut) +
  labs(
    x = &amp;quot;How close do you feel to Spain?&amp;quot;,
    y = &amp;quot;Percentage&amp;quot;
  ) +
  ggtitle(label = &amp;quot;Closeness to Spain by autonomous communities&amp;quot;) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 90),
    plot.title = element_text(size = 16, family = &amp;quot;Arial-BoldMT&amp;quot;),
    plot.subtitle = element_text(size = 14, color = &amp;quot;#666666&amp;quot;),
    plot.caption = element_text(size = 10, color = &amp;quot;#666666&amp;quot;)
  ) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-11-18-123check/2017-11-18-1-2-3-check_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Catalonia does seem to be the region with the highest share of respondents saying that they don’t feel close to Spain, although the vast majority does say they feel very or just close to Spain. On the other hand, Andalucia does comply with stereotypes! They certainly feel very close to the Spanish identity.&lt;/p&gt;
&lt;p&gt;My &lt;code&gt;blogdown&lt;/code&gt; workflow is very easy:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create a post with my function &lt;code&gt;cimentadaj::my_new_post&lt;/code&gt; which is a wrapper around &lt;code&gt;blogdown::new_post&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run &lt;code&gt;blogdown::serve_site&lt;/code&gt; to have a realtime visual of how my blog post is being rendered&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Write blogpost&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run &lt;code&gt;blogdown::build_site&lt;/code&gt;. This can take long if you posts that takea long time to compile&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Push to github (although this is more complicated because I have two branches, one for developing content and the other for pushing to the website. Maybe I’ll write a post about this once)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bloggin with &lt;code&gt;blogdown&lt;/code&gt; was so easy that I think I’m gonna start bloggin more now…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD thesis template with Sweave and knitr</title>
      <link>/blog/2017-10-24-phd-thesis-template-with-sweave-and-knitr/phd-thesis-template-with-sweave-and-knitr/</link>
      <pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-10-24-phd-thesis-template-with-sweave-and-knitr/phd-thesis-template-with-sweave-and-knitr/</guid>
      <description>&lt;p&gt;Writing my thesis with &lt;code&gt;Sweave&lt;/code&gt; and &lt;code&gt;knitr&lt;/code&gt; was very nice at the beginning, but then I began running into problems when I wanted to combine different chapters into one single document. Most of the problems were related to having each chapter be compilable on its own with separate bibliographies, among other things. I wrote a detailed guide on how I did it and you can read it &lt;a href=&#34;https://cimentadaj.github.io/phd_thesis/thesis_template_example/2017-10-24-thesis-template.html&#34;&gt;here&lt;/a&gt;. I’d love some feedback as workflow is still very rudimentary. You can post a comment on this post or email me at &lt;a href=&#34;mailto:cimentadaj@gmail.com&#34;&gt;cimentadaj@gmail.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Hope it’s useful.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scraping and visualizing How I Met Your Mother</title>
      <link>/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</link>
      <pubDate>Mon, 16 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/</guid>
      <description>&lt;p&gt;How I Met Your Mother (HIMYM from here after) is a television series very similar to the classical ‘Friends’ series from the 90’s. Following the release of the &lt;a href=&#34;http://tidytextmining.com/&#34;&gt;tidy text&lt;/a&gt; book I was looking for a project in which I could apply these skills. I decided I would scrape all the transcripts from HIMYM and analyze patterns between characters. This post really took me to the limit in terms of web scraping and pattern matching, which was specifically what I wanted to improve in the first place. Let’s begin!&lt;/p&gt;
&lt;p&gt;My first task was whether there was any consistency in the URL’s that stored the transcripts. If you ever watched HIMYM, we know there’s around nine seasons, each one with about 22 episodes. This makes about 200 episodes give or take. It would be a big pain to manually write down 200 complicated URL’s. Luckily, there is a way of finding the 200 links without writing them down manually.&lt;/p&gt;
&lt;p&gt;First, we create the links for the 9 websites that contain all episodes (1 through season 9)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(tidyverse)
library(stringr)
library(tidytext)

main_url &amp;lt;- &amp;quot;http://transcripts.foreverdreaming.org&amp;quot;
all_pages &amp;lt;- paste0(&amp;quot;http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;amp;start=&amp;quot;, seq(0, 200, 25))
characters &amp;lt;- c(&amp;quot;ted&amp;quot;, &amp;quot;lily&amp;quot;, &amp;quot;marshall&amp;quot;, &amp;quot;barney&amp;quot;, &amp;quot;robin&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each of the URL’s of &lt;code&gt;all_pages&lt;/code&gt; contains all episodes for that season (so around 22 URL’s). I also picked the characters we’re gonna concentrate for now. From here the job is very easy. We create a function that reads each link and parses the section containing all links for that season. We can do that using &lt;a href=&#34;http://selectorgadget.com/.&#34;&gt;SelectorGadget&lt;/a&gt; to find the section we’re interested in. We then search for the &lt;code&gt;href&lt;/code&gt; attribute to grab all links in that attribute and finally create a tibble with each episode together with it’s link.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;episode_getter &amp;lt;- function(link) {
  title_reference &amp;lt;-
    link %&amp;gt;%
    read_html() %&amp;gt;%
    html_nodes(&amp;quot;.topictitle&amp;quot;) # Get the html node name with &amp;#39;selector gadget&amp;#39;
  
  episode_links &amp;lt;-
    title_reference %&amp;gt;%
    html_attr(&amp;quot;href&amp;quot;) %&amp;gt;%
    gsub(&amp;quot;^.&amp;quot;, &amp;quot;&amp;quot;, .) %&amp;gt;%
    paste0(main_url, .) %&amp;gt;%
    setNames(title_reference %&amp;gt;% html_text()) %&amp;gt;%
    enframe(name = &amp;quot;episode_name&amp;quot;, value = &amp;quot;link&amp;quot;)
  
  episode_links
}

all_episodes &amp;lt;- map_df(all_pages, episode_getter) # loop over all seasons and get all episode links
all_episodes$id &amp;lt;- 1:nrow(all_episodes)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There we go! Now we have a very organized &lt;code&gt;tibble&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_episodes
# # A tibble: 208 x 3
#    episode_name                   link                                  id
#    &amp;lt;chr&amp;gt;                          &amp;lt;chr&amp;gt;                              &amp;lt;int&amp;gt;
#  1 01x01 - Pilot                  http://transcripts.foreverdreamin~     1
#  2 01x02 - Purple Giraffe         http://transcripts.foreverdreamin~     2
#  3 01x03 - Sweet Taste of Liberty http://transcripts.foreverdreamin~     3
#  4 01x04 - Return of the Shirt    http://transcripts.foreverdreamin~     4
#  5 01x05 - Okay Awesome           http://transcripts.foreverdreamin~     5
#  6 01x06 - Slutty Pumpkin         http://transcripts.foreverdreamin~     6
#  7 01x07 - Matchmaker             http://transcripts.foreverdreamin~     7
#  8 01x08 - The Duel               http://transcripts.foreverdreamin~     8
#  9 01x09 - Belly Full of Turkey   http://transcripts.foreverdreamin~     9
# 10 01x10 - The Pineapple Incident http://transcripts.foreverdreamin~    10
# # ... with 198 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The remaining part is to actually scrape the text from each episode. We can work that out for a single episode and then turn that into a function and apply for all episodes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;episode_fun &amp;lt;- function(file) {
  
  file %&amp;gt;%
    read_html() %&amp;gt;%
    html_nodes(&amp;quot;.postbody&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    str_split(&amp;quot;\n|\t&amp;quot;) %&amp;gt;%
    .[[1]] %&amp;gt;%
    data_frame(text = .) %&amp;gt;%
    filter(str_detect(text, &amp;quot;&amp;quot;), # Lots of empty spaces
           !str_detect(text, &amp;quot;^\\t&amp;quot;), # Lots of lines with \t to delete
           !str_detect(text, &amp;quot;^\\[.*\\]$&amp;quot;), # Text that start with brackets
           !str_detect(text, &amp;quot;^\\(.*\\)$&amp;quot;), # Text that starts with parenthesis
           str_detect(text, &amp;quot;^*.:&amp;quot;), # I want only lines with start with dialogue (:)
           !str_detect(text, &amp;quot;^ad&amp;quot;)) # Remove lines that start with ad (for &amp;#39;ads&amp;#39;, the link of google ads)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above function reads each episode, turns the html text into a data frame and organizes it clearly for text analysis. For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;episode_fun(all_episodes$link[15])
# # A tibble: 195 x 1
#    text                                                                   
#    &amp;lt;chr&amp;gt;                                                                  
#  1 Ted from 2030: Kids, something you might not know about your Uncle Mar~
#  2 &amp;quot;Ted: You don&amp;#39;t have to shout out \&amp;quot;poker\&amp;quot; when you win.&amp;quot;             
#  3 Marshall: I know. It&amp;#39;s just fun to say.                                
#  4 &amp;quot;Ted from 2030: We all finally agreed Marshall should be running our g~
#  5 &amp;quot;Marshall: It&amp;#39;s called \&amp;quot;Marsh-gammon.\&amp;quot; It combines all the best feat~
#  6 Robin: Backgammon, obviously.                                          
#  7 &amp;quot;Marshall: No. Backgammon sucks. I took the only good part of backgamm~
#  8 Lily: I&amp;#39;m so excited Victoria&amp;#39;s coming.                                
#  9 Robin: I&amp;#39;m going to go get another round.                              
# 10 Ted: Okay, I want to lay down some ground rules for tonight. Barney, I~
# # ... with 185 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have a data frame with only dialogue for each character. We need to apply that function to each episode and &lt;code&gt;bind&lt;/code&gt; everything together. We first apply the function to every episode.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_episodes$text &amp;lt;- map(all_episodes$link, episode_fun)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;text&lt;/code&gt; list-column is an organized list with text for each episode. However, manual inspection of some episodes actually denotes a small error that limits our analysis greatly. Among the main interests of this document is to study relationships and presence between characters. For that, we need each line of text to be accompanied by the character who said it. Unfortunately, some of these scripts don’t have that.&lt;/p&gt;
&lt;p&gt;For example, check any episode from season &lt;a href=&#34;http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;amp;start=175&#34;&gt;8&lt;/a&gt; and &lt;a href=&#34;http://transcripts.foreverdreaming.org/viewforum.php?f=177&amp;amp;start=200&#34;&gt;9&lt;/a&gt;. The writer didn’t write the dialogue and just rewrote the lines. There’s nothing we can do so far to improve that and we’ll be excluding these episodes. This pattern is also present in random episodes like in season 4 or season 6. We can exclude chapters based on the number of lines we parsed. On average, each of these episodes has about 200 lines of dialogue. Anything significantly lower, like 30 or 50 lines, is an episode which doesn’t have a lot of dialogue.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_episodes$count &amp;lt;- map_dbl(all_episodes$text, nrow)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can extend the previous &lt;code&gt;tibble&lt;/code&gt; to be a bit more organized by separating the episode-season column into separate season and episo numbers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_episodes &amp;lt;-
  all_episodes %&amp;gt;%
  separate(episode_name, c(&amp;quot;season&amp;quot;, &amp;quot;episode&amp;quot;), &amp;quot;-&amp;quot;, extra = &amp;quot;merge&amp;quot;) %&amp;gt;%
  separate(season, c(&amp;quot;season&amp;quot;, &amp;quot;episode_number&amp;quot;), sep = &amp;quot;x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! We now have a very organized &lt;code&gt;tibble&lt;/code&gt; with all the information we need. Next step is to actually break down the lines into words and start looking for general patterns. We can do that by looping through all episodes that have over 100 lines (just an arbitrary threshold) and unnesting each line for each &lt;strong&gt;valid&lt;/strong&gt; character.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lines_characters &amp;lt;-
  map(filter(all_episodes, count &amp;gt; 100) %&amp;gt;% pull(text), ~ { 
    # only loop over episodes that have over 100 lines
    .x %&amp;gt;%
      separate(text, c(&amp;quot;character&amp;quot;, &amp;quot;text&amp;quot;), sep = &amp;quot;:&amp;quot;, extra = &amp;#39;merge&amp;#39;) %&amp;gt;%
      # separate character dialogue from actual dialogo
      unnest_tokens(character, character) %&amp;gt;%
      filter(str_detect(character, paste0(paste0(&amp;quot;^&amp;quot;, characters, &amp;quot;$&amp;quot;), collapse = &amp;quot;|&amp;quot;))) %&amp;gt;%
      # only count the lines of our chosen characters
      mutate(episode_lines_id = 1:nrow(.))
  }) %&amp;gt;%
  setNames(filter(all_episodes, count &amp;gt; 100) %&amp;gt;% # name according to episode
             unite(season_episode, season, episode_number, sep = &amp;quot;x&amp;quot;) %&amp;gt;%
             pull(season_episode)) %&amp;gt;%
  enframe() %&amp;gt;%
  unnest() %&amp;gt;%
  mutate(all_lines_id = 1:nrow(.))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, our text is sort of ready. Let’s remove some bad words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;words_per_character &amp;lt;-
  lines_characters %&amp;gt;%
  unnest_tokens(word, text) %&amp;gt;% # expand all sentences into words
  anti_join(stop_words) %&amp;gt;% # remove bad words
  filter(!word %in% characters) %&amp;gt;% # only select characters we&amp;#39;re interested
  arrange(name) %&amp;gt;%
  separate(name, c(&amp;quot;season&amp;quot;, &amp;quot;episode&amp;quot;), sep = &amp;quot;x&amp;quot;, remove = FALSE) %&amp;gt;%
  mutate(name = factor(name, ordered = TRUE),
         season = factor(season, ordered = TRUE),
         episode = factor(episode, ordered = TRUE)) %&amp;gt;%
  filter(season != &amp;quot;07&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just to make sure, let’s look at the &lt;code&gt;tibble&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;words_per_character
# # A tibble: 88,174 x 7
#    name     season episode character episode_lines_id all_lines_id word   
#    &amp;lt;ord&amp;gt;    &amp;lt;ord&amp;gt;  &amp;lt;ord&amp;gt;   &amp;lt;chr&amp;gt;                &amp;lt;int&amp;gt;        &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;  
#  1 &amp;quot;01x01 &amp;quot; 01     &amp;quot;01 &amp;quot;   marshall                 1            1 ring   
#  2 &amp;quot;01x01 &amp;quot; 01     &amp;quot;01 &amp;quot;   marshall                 1            1 marry  
#  3 &amp;quot;01x01 &amp;quot; 01     &amp;quot;01 &amp;quot;   ted                      2            2 perfect
#  4 &amp;quot;01x01 &amp;quot; 01     &amp;quot;01 &amp;quot;   ted                      2            2 engaged
#  5 &amp;quot;01x01 &amp;quot; 01     &amp;quot;01 &amp;quot;   ted                      2            2 pop    
#  6 &amp;quot;01x01 &amp;quot; 01     &amp;quot;01 &amp;quot;   ted                      2            2 champa~
#  7 &amp;quot;01x01 &amp;quot; 01     &amp;quot;01 &amp;quot;   ted                      2            2 drink  
#  8 &amp;quot;01x01 &amp;quot; 01     &amp;quot;01 &amp;quot;   ted                      2            2 toast  
#  9 &amp;quot;01x01 &amp;quot; 01     &amp;quot;01 &amp;quot;   ted                      2            2 kitchen
# 10 &amp;quot;01x01 &amp;quot; 01     &amp;quot;01 &amp;quot;   ted                      2            2 floor  
# # ... with 88,164 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Perfect! One row per word, per character, per episode with the id of the line of the word.&lt;/p&gt;
&lt;p&gt;Alright, let’s get our hands dirty. First, let visualize the presence of each character in terms of words over time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Filtering position of first episode of all seasons to
# position the X axis in the next plot.
first_episodes &amp;lt;-
  all_episodes %&amp;gt;%
  filter(count &amp;gt; 100, episode_number == &amp;quot;01 &amp;quot;) %&amp;gt;%
  pull(id)

words_per_character %&amp;gt;%
  split(.$name) %&amp;gt;%
  setNames(1:length(.)) %&amp;gt;%
  enframe(name = &amp;quot;episode_id&amp;quot;) %&amp;gt;%
  unnest() %&amp;gt;%
  count(episode_id, character) %&amp;gt;%
  group_by(episode_id) %&amp;gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&amp;gt;%
  ggplot(aes(as.numeric(episode_id), perc, group = character, colour = character)) +
  geom_line() +
  geom_smooth(method = &amp;quot;lm&amp;quot;) +
  scale_colour_discrete(guide = FALSE) +
  scale_x_continuous(name = &amp;quot;Seasons&amp;quot;,
                     breaks = first_episodes, labels = paste0(&amp;quot;S&amp;quot;, 1:7)) +
  scale_y_continuous(name = &amp;quot;Percentage of words per episode&amp;quot;) +
  theme_minimal() +
  facet_wrap(~ character, ncol = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ted is clearly the character with the highest number of words per episode followed by Barney. Lily and Robin, the only two women have very low presence compared to the men. In fact, if one looks closely, Lily seemed to have decreased slightly over time, having an all time low in season 4. Marshall, Lily’s partner in the show, does have much lower presence than both Barney and Ted but he has been catching up over time.&lt;/p&gt;
&lt;p&gt;We also see an interesting pattern where Barney has a lot of peaks, suggesting that in some specific episodes he gains predominance, where Ted has an overall higher level of words per episode. And when Ted has peaks, it’s usually below its trend-line.&lt;/p&gt;
&lt;p&gt;Looking at the distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# devtools::install_github(&amp;quot;clauswilke/ggjoy&amp;quot;)
library(ggjoy)

words_per_character %&amp;gt;%
  split(.$name) %&amp;gt;%
  setNames(1:length(.)) %&amp;gt;%
  enframe(name = &amp;quot;episode_id&amp;quot;) %&amp;gt;%
  unnest() %&amp;gt;%
  count(season, episode_id, character) %&amp;gt;%
  group_by(episode_id) %&amp;gt;%
  mutate(total_n = sum(n),
         perc = round(n / total_n, 2)) %&amp;gt;%
  ggplot(aes(x = perc, y = character, fill = character)) +
  geom_joy(scale = 0.85) +
  scale_fill_discrete(guide = F) +
  scale_y_discrete(name = NULL, expand=c(0.01, 0)) +
  scale_x_continuous(name = &amp;quot;Percentage of words&amp;quot;, expand=c(0.01, 0)) +
  ggtitle(&amp;quot;Percentage of words per season&amp;quot;) +
  facet_wrap(~ season, ncol = 7) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;we see the differences much clearer. For example, we see Barney’s peaks through out every season with Season 6 seeing a clear peak of 40%. On the other hand, we see that their distributions don’t change that much over time! Suggesting that the presence of each character is very similar in all seasons. Don’t get me wrong, there are differences like Lily in Season 2 and then in Season 6, but in overall terms the previous plot suggests no increase over seasons, and this plot suggests that between seasons, there’s not a lot of change in their distributions that affects the overall mean.&lt;/p&gt;
&lt;p&gt;If you’ve watched the TV series, you’ll remember Barney always repeating one similar trademark word: legendary! Although it is a bit cumbersome for us to count the number of occurrences of that sentence once we unnested each sentence, we can at least count the number of words per character and see whether some characters have particular words.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count_words &amp;lt;-
  words_per_character %&amp;gt;%
  filter(!word %in% characters) %&amp;gt;%
  count(character, word, sort = TRUE)

count_words %&amp;gt;%
  group_by(character) %&amp;gt;%
  top_n(20) %&amp;gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we see that a lot of the words we capture are actually nouns or expressions which are common to everyone, such as ‘yeah’, ‘hey’ or ‘time’. We can weight down commonly used words for other words which are important but don’t get repeated a lot. We can exclude those words using &lt;code&gt;bind_tf_idf()&lt;/code&gt;, which for each character decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection or corpus of documents (see 3.3 in &lt;a href=&#34;http://tidytextmining.com/tfidf.html&#34; class=&#34;uri&#34;&gt;http://tidytextmining.com/tfidf.html&lt;/a&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;count_words %&amp;gt;%
  bind_tf_idf(word, character, n) %&amp;gt;%
  arrange(desc(tf_idf)) %&amp;gt;%
  group_by(character) %&amp;gt;%
  top_n(20) %&amp;gt;%
  ggplot(aes(reorder(word, n), n)) +
  geom_col(alpha = 0.8) +
  coord_flip() +
  facet_wrap(~ character, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now Barney has a very distinctive word usage, one particularly sexist with words such as couger, bang and tits. Also, we see the word legendary as the thirdly repeated word, something we were expecting! On the other hand, we see Ted with things like professor (him), aunt (because of aunt Lily and such).&lt;/p&gt;
&lt;p&gt;Knowing that Ted is the main character in the series is no surprise. To finish off, we’re interested in knowing which characters are related to each other. First, let’s turn the data frame into a suitable format.&lt;/p&gt;
&lt;p&gt;Here we turn all lines to lower case and check which characters are present in the text of each dialogue. The loop will return a vector of logicals whether there was a mention of any of the characters. For simplicity I exclude all lines where there is more than 1 mention of a character, that is, 2 or more characters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lines_characters &amp;lt;-
  lines_characters %&amp;gt;%
  mutate(text = str_to_lower(text))

rows_fil &amp;lt;-
  map(characters, ~ str_detect(lines_characters$text, .x)) %&amp;gt;%
  reduce(`+`) %&amp;gt;%
  ifelse(. &amp;gt;= 2, 0, .) # excluding sentences which have 2 or more mentions for now
  # ideally we would want to choose to count the number of mentions
  # per line or randomly choose another a person that was mentioned.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the rows that have a mention of another character, we subset only those rows. Then we want know which character was mentioned in which line. I loop through each line and test which character is present in that specific dialogue line. The loop returns the actual character name for each dialogue. Because we already filtered lines that &lt;strong&gt;have&lt;/strong&gt; a character name mentioned, the loop should return a vector of the same length.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;character_relation &amp;lt;-
  lines_characters %&amp;gt;%
  filter(as.logical(rows_fil)) %&amp;gt;%
  mutate(who_said_what =
           map_chr(.$text, ~ { # loop over all each line
             who_said_what &amp;lt;- map_lgl(characters, function(.y) str_detect(.x, .y))
             # loop over each character and check whether he/she was mentioned
             # in that line
             characters[who_said_what]
             # subset the character that matched
           }))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we plot the relationship using the &lt;code&gt;ggraph&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggraph)
library(igraph)

character_relation %&amp;gt;%
  count(character, who_said_what) %&amp;gt;%
  graph_from_data_frame() %&amp;gt;%
  ggraph(layout = &amp;quot;linear&amp;quot;, circular = TRUE) +
  geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                width = 2.5, show.legend = FALSE) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A very clear pattern emerges. There is a strong relationship between Robin and Barney towards Ted. In fact, their direct relationship is very weak, but both are very well connected to Ted. On the other hand, Marshall and Lily are also reasonably connected to Ted but with a weaker link. Both of them are indeed very connected, as should be expected since they were a couple in the TV series.&lt;/p&gt;
&lt;p&gt;We also see that the weakest members of the group are Robin and Barney with only strong bonds toward Ted but no strong relationship with the other from the group. Overall, there seems to be a division: Marshall and Lily hold a somewhat close relationship with each other and towards Ted and Barney and Robin tend to be related to Ted but no one else.&lt;/p&gt;
&lt;p&gt;As a follow-up question, is this pattern of relationships the same across all seasons? We can do that very quickly by filtering each season using the previous plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cowplot)

# Loop through each season
seasons &amp;lt;- paste0(0, 1:7)

all_season_plots &amp;lt;- lapply(seasons, function(season_num) {

  set.seed(2131)
  
  character_relation %&amp;gt;%
    # Extract the season number from the `name` column
    mutate(season = str_replace_all(character_relation$name, &amp;quot;x(.*)$&amp;quot;, &amp;quot;&amp;quot;)) %&amp;gt;%
    filter(season == season_num) %&amp;gt;%
    count(character, who_said_what) %&amp;gt;%
    graph_from_data_frame() %&amp;gt;%
    ggraph(layout = &amp;quot;linear&amp;quot;, circular = TRUE) +
    geom_edge_arc(aes(edge_alpha = n, edge_width = n),
                  width = 2.5, show.legend = FALSE) +
    geom_node_text(aes(label = name), repel = TRUE) +
    theme_void()
})

# Plot all graphs side-by-side
cowplot::plot_grid(plotlist = all_season_plots, labels = seasons)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/2017-10-16-scraping-and-visualizing-how-i-met-your-mother_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;960&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are reasonable changes for all non-Ted relationship! For example, for season 2 the relationship Marshall-Lily-Ted becomes much stronger and it disappears in season 3. Let’s remember that these results might be affected by the fact that I excluded some episodes because of low number of dialogue lines. Keeping that in mind, we also see that for season 7 the Robin-Barney relationship became much stronger (is this the season the started dating?). All in all, the relationships don’t look dramatically different from the previous plot. Everyone seems to be strongly related to Ted. The main difference is the changes in relationship between the other members of the cast.&lt;/p&gt;
&lt;p&gt;This dataset has a lot of potential and I’m sure I’ve scratched the surface of what one can do with this data. I encourage anyone interested in the topic to use the code to analyze the data further. One idea I might explore in the future is to build a model that attempts to predict who said what for all dialogue lines that didn’t have a character member. This can be done by extracting features from all sentences and using these patterns try to classify which. Any feedback is welcome, so feel free to message me at &lt;a href=&#34;mailto:cimentadaj@gmail.com&#34;&gt;cimentadaj@gmail.com&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The LOO and the Bootstrap</title>
      <link>/blog/2017-09-07-the-loo-and-the-bootstrap/the-loo-and-the-bootstrap/</link>
      <pubDate>Thu, 07 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-09-07-the-loo-and-the-bootstrap/the-loo-and-the-bootstrap/</guid>
      <description>&lt;p&gt;This is the second entry, and probably the last, on model validation methods. These posts are inspired by the work of Kohavi (1995), which I totally recommend reading. This post will talk talk about the Leave-One-Out Cross Validation (LOOCV), which is the extreme version of the K-Fold Cross Validation and the Bootstrap for model assessment.&lt;/p&gt;
&lt;p&gt;Let’s dive in!&lt;/p&gt;
&lt;div id=&#34;the-leave-one-out-cv-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Leave-One-Out CV method&lt;/h2&gt;
&lt;p&gt;The LOOCV is actually a very intuitive idea if you know how the K-Fold CV works.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LOOCV: Let’s imagine a data set with 30 rows. We separate the 1st row to be the test data and the remaining 29 rows to be the training data. We fit the model on the training data and then predict the one observation we left out. We record the model accuracy and then repeat but predicting the 2nd row from training the model on row 1 and 3:30. We repeat until every row has been predicted.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is surprisingly easy to implement in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

set.seed(21341)
loo_result &amp;lt;-
  map_lgl(1:nrow(mtcars), ~ {
  test &amp;lt;- mtcars[.x, ] # Pick the .x row of the iteration to be the test
  train &amp;lt;- mtcars[-.x, ] # Let the training be all the data EXCEPT that row
  
  train_model &amp;lt;- glm(am ~ mpg + cyl + disp, family = binomial(), data = train) # Fit any model
  
  # Since the prediction is in probabilities, pass the probability
  # to generate either a 1 or 0 based on the probability
  prediction &amp;lt;- predict(train_model, newdata = test, type = &amp;quot;response&amp;quot;) %&amp;gt;% rbinom(1, 1, .)
  
  test$am == prediction # compare whether the prediction matches the actual value
})

summary(loo_result %&amp;gt;% as.numeric) # percentage of accurate results
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#  0.0000  0.0000  1.0000  0.5938  1.0000  1.0000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like our model had nearly 60% accuracy, not very good. But not entirely bad given our very low sample size.&lt;/p&gt;
&lt;p&gt;Advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Just as with the K-Fold CV, this approach is useful because it uses all the data. At some point, every rows gets to be the test set and training set, maximizing information.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;In fact, it uses almost ALL the data as the original data set as the training set is just N - 1 (this method uses even more than the K-Fold CV).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disadvantage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;This approach is very heavy on your computer. We need to refit de model N times (although there is a shortcut for linear regreesion, see &lt;a href=&#34;https://gerardnico.com/wiki/lang/r/cross_validation&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Given that the test set is of only 1 observation, there might be a lot of variance in the prediction, making the accuracy test more unreliable (that is, relative to K-Fold CV)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-bootstrap-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Bootstrap method&lt;/h2&gt;
&lt;p&gt;The bootstrap method is a bit different. Maybe you’ve heard about the bootstrap for estimating standard errors, and in fact for model assessment it’s very similar.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Boostrap method: Take the data from before with 30 rows. Suppose we resample this dataset with replacement. That is, the dataset will have the same 30 rows, but row 1 might be repeated 3 times, row 2 might be repeated 4 times, row 3 might not be in the dataset anymore, and so on. Now, take this resampled data and use it to train the model. Now test your predictions on the actual data (the one with 30 unique rows) and calculate the model accuracy. Repeat N times.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Again, the R implementation is very straightforward.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
set.seed(21314)
bootstrap &amp;lt;-
  map_dbl(1:500, ~ {
  train &amp;lt;- mtcars[sample(nrow(mtcars), replace = T), ] # randomly sample rows with replacement
  test &amp;lt;- mtcars
  
  train_model &amp;lt;- glm(am ~ mpg + cyl + disp, family = binomial(), data = train) # fit any model
  
  # Get predicted probabilities and assign a 1 or 0 based on the probability
  prediction &amp;lt;- predict(train_model, newdata = test, type = &amp;quot;response&amp;quot;) %&amp;gt;% rbinom(nrow(mtcars), 1, .)
  accuracy &amp;lt;- test$am == prediction # compare whether the prediction matches the actual value
  
  mean(accuracy) # get the proportion of correct predictions
})

summary(bootstrap)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#  0.4375  0.6875  0.7500  0.7468  0.8125  0.9375&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We got a better accuracy with the bootstrap (probably biased, see below) and a range of possible values going from 0.43 to 0.93. Note that if you run these models you’ll get a bunch of warnings like &lt;code&gt;glm.fit: fitted probabilities numerically 0 or 1 occurred&lt;/code&gt; because we just have too few observations to be including covariates, resulting in a lot of overfitting.&lt;/p&gt;
&lt;p&gt;Advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Variance is small considering both train and test have the same number of rows.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disadvantages&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It gives more biased results than the CV methods because it repeats data, rather than keep unique observations for training and testing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the end, it’s a trade-off against what you’re looking for. In some instances, it’s alright to have a slightly biased estimate (either pessimistic or optimistic) as long as its reliable (bootstrap). On other instances, it’s better to have a very exact prediction but that is less unreliable (CV methods).&lt;/p&gt;
&lt;p&gt;Some rule of thumbs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For large sample sizes, the variance issues become less important and the computational part is more of an issues. I still would stick by repeated CV for small and large sample sizes. See &lt;a href=&#34;https://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cross validation is a good tool when deciding on the model – it helps you avoid fooling yourself into thinking that you have a good model when in fact you are overfitting. When your model is fixed, then using the bootstrap makes more sense to assess accuracy (to me at least). See again &lt;a href=&#34;https://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio&#34;&gt;here&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Again, this is a very crude approach, and the whole idea is to understand the inner workings of these algorithms in practice. For more thorough approaches I suggest using the &lt;code&gt;cv&lt;/code&gt; functions from the &lt;code&gt;boot&lt;/code&gt; package or &lt;code&gt;caret&lt;/code&gt; or &lt;code&gt;modelr&lt;/code&gt;. I hope this was useful. I will try to keep doing these things as they help me understand these techniques better.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kohavi, Ron. “A study of cross-validation and bootstrap for accuracy estimation and model selection.” Ijcai. Vol. 14. No. 2. 1995.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Holdout and cross-validation</title>
      <link>/blog/2017-09-06-holdout-and-crossvalidation/holdout-and-crossvalidation/</link>
      <pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-09-06-holdout-and-crossvalidation/holdout-and-crossvalidation/</guid>
      <description>&lt;p&gt;In a recent attempt to bring a bit of discipline into my life, I’ve been forcing myself to read papers after lunch, specifically concentrated on data science topics. The whole idea is to educated myself every day, but if I find something cool that I can implement in R, I’ll do it right away.&lt;/p&gt;
&lt;p&gt;This blogpost is the first of a series of entries I plan to post explaining the main concepts of Kohavi (1995), which compares cross-validation methods and bootstrap methods for model selection. This first post will implement a K-Fold cross validation from scratch in order to understand more deeply what’s going on behind the scenes.&lt;/p&gt;
&lt;p&gt;Before we explain the concept of K-Fold cross validation, we need to define what the ‘Holdout’ method is.&lt;/p&gt;
&lt;div id=&#34;holdout-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Holdout method&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Holdout method: Imagine we have a dataset with house prices as the dependent variable and two independent variables showing the square footage of the house and the number of rooms. Now, imagine this dataset has &lt;code&gt;30&lt;/code&gt; rows. The whole idea is that you build a model that can predict house prices accurately. To ‘train’ your model, or see how well it performs, we randomly subset 20 of those rows and fit the model. The second step is to predict the values of those 10 rows that we excluded and measure how well our predictions were. As a rule of thumb, experts suggest to randomly sample 80% of the data into the training set and 20% into the test set.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A very quick example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(modelr)

holdout &amp;lt;- function(repeat_times) { # empty argument for later
  n &amp;lt;- nrow(mtcars)
  eighty_percent &amp;lt;- (n * 0.8) %&amp;gt;% floor
  train_sample &amp;lt;- sample(1:n, eighty_percent) # randomly pick 80% of the rows
  test_sample &amp;lt;- setdiff(1:n, train_sample) # get the remaining 20% of the rows
  
  train &amp;lt;- mtcars[train_sample, ] # subset the 80% of the rows
  test &amp;lt;- mtcars[test_sample, ] # subset 20% of the rows
  
  train_model &amp;lt;- lm(mpg ~ ., data = train)
  
  test %&amp;gt;%
    add_predictions(train_model) %&amp;gt;% # add the predicted mpg values to the test data
    summarize(average_error = (pred - mpg) %&amp;gt;% mean %&amp;gt;% round(2)) %&amp;gt;%
    pull(average_error)
  # calculate the average difference of the predicition from the actual value
}

set.seed(2131)
holdout()
# [1] 3.59&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that on average the training set over predicts the actual values by about 3.6 points. An even more complex approach is what Kohavi (1995) calls “random subsampling”.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-subsampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random subsampling&lt;/h2&gt;
&lt;p&gt;In a nutshell, repeat the previous &lt;code&gt;N&lt;/code&gt; times and calculate the average and standard deviation of your metric of interest.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2134)

random_subsampling &amp;lt;- map_dbl(1:500, holdout)
summary(random_subsampling)
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# -7.3100 -0.7525  0.4000  0.4255  1.5550  9.1500&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get a mean error of 0.42, a maximum of 9.15 and a minimum of -7.31. Quite some variation, eh? It is precisely for this reason that Kohavi (1995) highlights that random subsampling has an important problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Each time we resample, some observations might’ve been in the previous resample, leading to non-independence and making the training dataset unrepresentative of the original dataset.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What happens when you try to predict Y from an unrepresented X, 500 times? What we just saw before.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;k-fold-cross-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;K-Fold cross validation&lt;/h2&gt;
&lt;p&gt;Let’s move on to cross validation. K-Fold cross validation is a bit trickier, but here is a simple explanation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;K-Fold cross validation: Take the house prices dataset from the previous example, divide the dataset into 10 parts of equal size, so if the data is 30 rows long, you’ll have 10 datasets of 3 rows each. Each split contains unique rows not present in other splits. In the first iteration, take the first dataset as the test dataset and merge the remaining 9 datasets as the train dataset. Fit the model on the training data, predict on the test data and record model accuracy. Repeat a new iteration where dataset 2 is the test set and data set 1 and 3:10 merged is the training set. Repeat for all K slices.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can implement this in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k_slicer &amp;lt;- function(data, slices) {
  stopifnot(nrow(data) &amp;gt; slices) # the number of rows must be bigger than the K slices
  slice_size &amp;lt;- (nrow(data) / slices) %&amp;gt;% floor
  
  rows &amp;lt;- 1:nrow(data)
  data_list &amp;lt;- rep(list(list()), slices) # create empty list of N slices

  # Randomly sample slice_size from the rows available, but exclude these rows
  # from the next sample of rows. This makes sure each slice has unique rows.
  for (k in 1:slices) {
    specific_rows &amp;lt;- sample(rows, slice_size) # sample unique rows for K slice
    rows &amp;lt;- setdiff(rows, specific_rows) # exclue those rows
    data_list[[k]] &amp;lt;- data[specific_rows, ] # sample the K slice and save in empty list
  }
  
  data_list
}

mtcars_sliced &amp;lt;- k_slicer(mtcars, slices = 5) # data sliced in K slices&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All good so far? We took a dataset and split it into K mutually exclusive datasets. The next step is to run the modeling on &lt;code&gt;K = 2:10&lt;/code&gt; and test on &lt;code&gt;K = 1&lt;/code&gt;, and then repeat on &lt;code&gt;K = c(1, 3:10)&lt;/code&gt; as training and test on &lt;code&gt;K = 2&lt;/code&gt;, and repeat for al &lt;code&gt;K’s&lt;/code&gt;. Below we implement it in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
k_fold_cv &amp;lt;-
  map_dbl(seq_along(mtcars_sliced), ~ {
  test &amp;lt;- mtcars_sliced[[.x]] # Take the K fold
  
  # Note the -.x, for excluding that K
  train &amp;lt;- mtcars_sliced[-.x] %&amp;gt;% reduce(bind_rows) # bind row all remaining K&amp;#39;s
  
  lm(mpg ~ ., data = train) %&amp;gt;%
    rmse(test) # calculate the root mean square error of predicting the test set
})

k_fold_cv %&amp;gt;%
  summary
#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#   3.192   3.746   3.993   3.957   4.279   4.574&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we get a summary of the root mean square error, a metric we decided to use now, instead of predictions. We can asses how accurate our model is this way and compare several specification of models and choose the one which better fits the data.&lt;/p&gt;
&lt;p&gt;The main advantage of this approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We maximize the use of data because all data is used, at some point, as test and training.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is very interesting in contrast to the holdout method in which we can’t maximize our data! Take data out of the test set and the predictions will have wider uncertainty intervals, take data out of the train set and get biased predictions.&lt;/p&gt;
&lt;p&gt;This approach, as any other, has disadvantages.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It is computationally intensive, given that we have to run the model K-1 times. In this setting it’s trivial, but in more complex modeling this can be quite costly.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If in any of the K iterations the predictions are bad, the overall accuracy will be bad, considering that other K iterations will also likely be bad. In other words, predictions need to be stable across all K iterations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Building on the previous point, once the model is stable, increasing the number of folds (5, 10, 20, 25…) generates little change considering that the accuracy will be similar (and the variance of different K-folds will be similar as well).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, if Y consists of categories, and one of these categories is very minimal, the best K-Fold CV can do is predict the class with more observations. If an observation of this minimal class gets to be in the test set in one of the iterations, then the training model will have very little accuracy for that category. See Kohavi (1995) page 3, example 1 for a detailed example.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This was my first attempt at manually implementing the Holdout method and the K-Fold CV. These examples are certainly flawed, like rounding the decimal number of rows correct for the unique number of rows in each K-Fold slice. If anyone is interested in correcting thes, please do send a pull request. For those interested in using more reliable approaches, take a look at the &lt;code&gt;caret&lt;/code&gt; and the &lt;code&gt;modelr&lt;/code&gt; package. In the next entry I will implement the LOO method and the bootstrap (and maybe the stratified K-Fold CV)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kohavi, Ron. “A study of cross-validation and bootstrap for accuracy estimation and model selection.” Ijcai. Vol. 14. No. 2. 1995.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>perccalc package</title>
      <link>/blog/2017-08-01-perccalc-package/perccalc-package/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-08-01-perccalc-package/perccalc-package/</guid>
      <description>&lt;p&gt;Reardon (2011) introduced a very interesting concept in which he calculates percentile differences from ordered categorical variables. He explains his procedure very much in detail in the appendix of the book chapter but no formal implementation has been yet available on the web. With this package I introduce a function that applies the procedure, following a step-by-step Stata script that Sean Reardon kindly sent me.&lt;/p&gt;
&lt;p&gt;In this vignette I show you how to use the function and match the results to the Stata code provided by Reardon himself.&lt;/p&gt;
&lt;p&gt;For this example, we’ll use a real world data set, one I’m very much familiar with: PISA. We’ll use the PISA 2012 wave for Germany because it asked parents about their income category. For this example we’ll need the packages below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(c(&amp;quot;devtools&amp;quot;, &amp;quot;matrixStats&amp;quot;, &amp;quot;tidyverse&amp;quot;))
# devtools::install_github(&amp;quot;pbiecek/PISA2012lite&amp;quot;)

library(matrixStats)
library(tidyverse)
library(haven)
library(PISA2012lite)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you haven’t installed any of the packages above, uncomment the first two lines to install them. Beware that the &lt;code&gt;PISA2012lite&lt;/code&gt; package contains the PISA 2012 data and takes a while to download.&lt;/p&gt;
&lt;p&gt;Let’s prepare the data. Below we filter only German students, select only the math test results and calculate the median of all math plausible values to get one single math score. Finally, we match each student with their corresponding income data from their parents data and their sample weights.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ger_student &amp;lt;- student2012 %&amp;gt;%
  filter(CNT == &amp;quot;Germany&amp;quot;) %&amp;gt;%
  select(CNT, STIDSTD, matches(&amp;quot;^PV*.MATH$&amp;quot;)) %&amp;gt;%
  transmute(CNT, STIDSTD,
            avg_score = rowMeans(student2012[student2012$CNT == &amp;quot;Germany&amp;quot;, paste0(&amp;quot;PV&amp;quot;, 1:5, &amp;quot;MATH&amp;quot;)]))

ger_parent &amp;lt;-
  parent2012 %&amp;gt;%
  filter(CNT == &amp;quot;Germany&amp;quot;) %&amp;gt;%
  select(CNT, STIDSTD, PA07Q01)

ger_weights &amp;lt;-
  student2012weights %&amp;gt;%
  filter(CNT == &amp;quot;Germany&amp;quot;) %&amp;gt;%
  select(CNT, STIDSTD, W_FSTUWT)

dataset_ready &amp;lt;-
  ger_student %&amp;gt;%
  left_join(ger_parent, by = c(&amp;quot;CNT&amp;quot;, &amp;quot;STIDSTD&amp;quot;)) %&amp;gt;%
  left_join(ger_weights, by = c(&amp;quot;CNT&amp;quot;, &amp;quot;STIDSTD&amp;quot;)) %&amp;gt;%
  as_tibble() %&amp;gt;%
  rename(income = PA07Q01,
         score = avg_score,
         wt = W_FSTUWT) %&amp;gt;%
  select(-CNT, -STIDSTD)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final results is this dataset:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##      score         income       wt
##      &amp;lt;dbl&amp;gt;         &amp;lt;fctr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 439.5622 Less than &amp;lt;$A&amp;gt; 137.3068
## 2 523.1422 Less than &amp;lt;$A&amp;gt; 170.0566
## 3 291.4083 Less than &amp;lt;$A&amp;gt; 162.3794
## 4 436.6023 Less than &amp;lt;$A&amp;gt; 162.3794
## 5 367.4326 Less than &amp;lt;$A&amp;gt; 114.6644
## # ... with 5 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the minimum dataset that the function will accept. This means that it needs to have at least a categorical variable and a continuous variable (the vector of weights is optional).&lt;/p&gt;
&lt;p&gt;The package is called &lt;code&gt;perccalc&lt;/code&gt;, short for percentile calculator and we can install and load it with this code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;perccalc&amp;quot;, repo = &amp;quot;https://cran.rediris.es/&amp;quot;)
## 
## The downloaded binary packages are in
##  /var/folders/w0/pscnb7zx5y9g_qf13cxhl0_r0000gn/T//Rtmphlh1k0/downloaded_packages
library(perccalc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The package has two functions, which I’ll show some examples. The first one is called &lt;code&gt;perc_diff&lt;/code&gt; and it’s very easy to use, we just specify the data, the name of the categorical and continuous variable and the percentile difference we want.&lt;/p&gt;
&lt;p&gt;Let’s put it to use!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, percentiles = c(90, 10))
## Error: is_ordered_fct is not TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I generated that error on purpose to raise a very important requirement of the function. The categorical variable needs to be an ordered factor (categorical). It is very important because otherwise we could be calculating percentile differences of categorical variables such as married, single and widowed, which doesn’t make a lot of sense.&lt;/p&gt;
&lt;p&gt;We can turn it into an ordered factor with the code below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset_ready &amp;lt;-
  dataset_ready %&amp;gt;%
  mutate(income = factor(income, ordered = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now it’ll work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, percentiles = c(90, 10))
## difference         se 
##   97.00706    8.74790&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can play around with other percentiles&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, percentiles = c(50, 10))
## difference         se 
##  58.776200   8.291083&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And we can add a vector of weights&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, how are we sure that these estimates are as accurate as the Reardon (2011) implementation? We can compare the Stata ouput using this data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Saving the dataset to a path
dataset_ready %&amp;gt;%
  write_dta(path = &amp;quot;/Users/cimentadaj/Downloads/pisa_income.dta&amp;quot;, version = 13)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Running the code below using the &lt;code&gt;pisa_income.dta&lt;/code&gt;..&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;*--------
use &amp;quot;/Users/cimentadaj/Downloads/pisa_income.dta&amp;quot;, clear

tab income, gen(inc)
*--------

/*-----------------------
    Making a data set that has 
    one observation per income category
    and has mean and se(mean) in each category
    and percent of population in the category
------------------------*/

tempname memhold
tempfile results
postfile `memhold&amp;#39; income mean se_mean per using `results&amp;#39;

forv i = 1/6 {
    qui sum inc`i&amp;#39; [aw=wt]
    loc per`i&amp;#39; = r(mean)    
                                
    qui sum score if inc`i&amp;#39;==1 
                            
    if `r(N)&amp;#39;&amp;gt;0 {
        qui regress score if inc`i&amp;#39;==1 [aw=wt]
        post `memhold&amp;#39; (`i&amp;#39;) (_b[_cons]) (_se[_cons]) (`per`i&amp;#39;&amp;#39;)
                            
    }               
}
postclose `memhold&amp;#39; 

/*-----------------------
    Making income categories
    into percentiles
------------------------*/


    use `results&amp;#39;, clear

    sort income
    gen cathi = sum(per)
    gen catlo = cathi[_n-1]
    replace catlo = 0 if income==1
    gen catmid = (catlo+cathi)/2
    
    /*-----------------------
        Calculate income 
        achievement gaps
    ------------------------*/

    sort income
    
    g x1 = catmid
    g x2 = catmid^2 + ((cathi-catlo)^2)/12
    g x3 = catmid^3 + ((cathi-catlo)^2)/4

    g cimnhi = mean + 1.96*se_mean
    g cimnlo = mean - 1.96*se_mean

    reg mean x1 x2 x3 [aw=1/se_mean^2] 

    twoway (rcap cimnhi cimnlo catmid) (scatter mean catmid) ///
        (function y = _b[_cons] + _b[x1]*x + _b[x2]*x^2 + _b[x3]*x^3, ran(0 1)) 
    
    loc hi_p = 90
    loc lo_p = 10

    loc d1 = [`hi_p&amp;#39; - `lo_p&amp;#39;]/100
    loc d2 = [(`hi_p&amp;#39;)^2 - (`lo_p&amp;#39;)^2]/(100^2)
    loc d3 = [(`hi_p&amp;#39;)^3 - (`lo_p&amp;#39;)^3]/(100^3)

    lincom `d1&amp;#39;*x1 + `d2&amp;#39;*x2 + `d3&amp;#39;*x3
    loc diff`hi_p&amp;#39;`lo_p&amp;#39; = r(estimate)
    loc se`hi_p&amp;#39;`lo_p&amp;#39; = r(se)
    
    di &amp;quot;`hi_p&amp;#39;-`lo_p&amp;#39; gap:     `diff`hi_p&amp;#39;`lo_p&amp;#39;&amp;#39;&amp;quot;
    di &amp;quot;se(`hi_p&amp;#39;-`lo_p&amp;#39; gap): `se`hi_p&amp;#39;`lo_p&amp;#39;&amp;#39;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I get that the 90/10 difference is &lt;code&gt;95.22&lt;/code&gt; with a standard error of &lt;code&gt;8.45&lt;/code&gt;. Does it sound familiar?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second function of the package is called &lt;code&gt;perc_dist&lt;/code&gt; and instead of calculating the difference of two percentiles, it returns the score and standard error of every percentile. The arguments of the function are exactly the same but without the &lt;code&gt;percentiles&lt;/code&gt; argument, because this will return the whole set of percentiles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_dist(dataset_ready, income, score)
##     percentile   estimate std.error
## 1            1   3.693889  1.327722
## 2            2   7.280584  2.591314
## 3            3  10.762009  3.792189
## 4            4  14.140090  4.931759
## 5            5  17.416754  6.011441
## 6            6  20.593925  7.032650
## 7            7  23.673529  7.996804
## 8            8  26.657492  8.905323
## 9            9  29.547739  9.759628
## 10          10  32.346196 10.561142
## 11          11  35.054789 11.311287
## 12          12  37.675443 12.011489
## 13          13  40.210083 12.663175
## 14          14  42.660636 13.267774
## 15          15  45.029026 13.826714
## 16          16  47.317180 14.341427
## 17          17  49.527023 14.813345
## 18          18  51.660481 15.243900
## 19          19  53.719479 15.634527
## 20          20  55.705943 15.986660
## 21          21  57.621798 16.301735
## 22          22  59.468970 16.581186
## 23          23  61.249385 16.826450
## 24          24  62.964968 17.038960
## 25          25  64.617644 17.220150
## 26          26  66.209340 17.371453
## 27          27  67.741982 17.494296
## 28          28  69.217493 17.590108
## 29          29  70.637801 17.660310
## 30          30  72.004830 17.706321
## 31          31  73.320507 17.729551
## 32          32  74.586757 17.731404
## 33          33  75.805505 17.713275
## 34          34  76.978678 17.676548
## 35          35  78.108200 17.622596
## 36          36  79.195997 17.552774
## 37          37  80.243995 17.468422
## 38          38  81.254120 17.370860
## 39          39  82.228297 17.261387
## 40          40  83.168451 17.141273
## 41          41  84.076509 17.011761
## 42          42  84.954395 16.874063
## 43          43  85.804036 16.729352
## 44          44  86.627357 16.578762
## 45          45  87.426284 16.423385
## 46          46  88.202741 16.264261
## 47          47  88.958656 16.102380
## 48          48  89.695953 15.938677
## 49          49  90.416558 15.774024
## 50          50  91.122396 15.609233
## 51          51  91.815394 15.445050
## 52          52  92.497476 15.282149
## 53          53  93.170569 15.121139
## 54          54  93.836598 14.962558
## 55          55  94.497488 14.806873
## 56          56  95.155165 14.654486
## 57          57  95.811555 14.505735
## 58          58  96.468584 14.360901
## 59          59  97.128176 14.220213
## 60          60  97.792258 14.083859
## 61          61  98.462754 13.951996
## 62          62  99.141592 13.824764
## 63          63  99.830695 13.702302
## 64          64 100.531991 13.584764
## 65          65 101.247404 13.472342
## 66          66 101.978860 13.365285
## 67          67 102.728285 13.263927
## 68          68 103.497604 13.168712
## 69          69 104.288742 13.080222
## 70          70 105.103627 12.999208
## 71          71 105.944182 12.926622
## 72          72 106.812333 12.863647
## 73          73 107.710007 12.811728
## 74          74 108.639129 12.772604
## 75          75 109.601624 12.748328
## 76          76 110.599418 12.741293
## 77          77 111.634436 12.754239
## 78          78 112.708605 12.790256
## 79          79 113.823849 12.852771
## 80          80 114.982095 12.945522
## 81          81 116.185267 13.072509
## 82          82 117.435292 13.237935
## 83          83 118.734096 13.446126
## 84          84 120.083602 13.701443
## 85          85 121.485739 14.008184
## 86          86 122.942430 14.370488
## 87          87 124.455601 14.792251
## 88          88 126.027178 15.277052
## 89          89 127.659088 15.828107
## 90          90 129.353254 16.448235
## 91          91 131.111603 17.139861
## 92          92 132.936061 17.905030
## 93          93 134.828552 18.745443
## 94          94 136.791004 19.662499
## 95          95 138.825340 20.657348
## 96          96 140.933487 21.730939
## 97          97 143.117371 22.884074
## 98          98 145.378916 24.117446
## 99          99 147.720049 25.431683
## 100        100 150.142695 26.827377&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also add the optional set of weights and graph it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_dist(dataset_ready, income, score, wt) %&amp;gt;%
  mutate(ci_low = estimate - 1.96 * std.error,
         ci_hi = estimate + 1.96 * std.error) %&amp;gt;%
  ggplot(aes(percentile, estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_hi))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-08-01-perccalc-package/2017-08-01-perccalc-package_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Please note that for calculating the difference between two percentiles it is more accurate to use the &lt;code&gt;perc_diff&lt;/code&gt; function. The &lt;code&gt;perc_diff&lt;/code&gt; calculates the difference through a linear combination of coefficients resulting in a different standard error.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_dist(dataset_ready, income, score, wt) %&amp;gt;%
  filter(percentile %in% c(90, 10)) %&amp;gt;%
  summarize(diff = diff(estimate),
            se_diff = diff(std.error))
##       diff  se_diff
## 1 95.22852 5.679855&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;compared to&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;perc_diff(dataset_ready, income, score, weights = wt, percentiles = c(90, 10))
## difference         se 
##  95.228517   8.454902&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They both have the same point estimate but a different standard error.&lt;/p&gt;
&lt;p&gt;I hope this was a convincing example, I know this will be useful for me. All the intelectual ideas come from Sean Reardon and the Stata code was written by Sean Reardon, Ximena Portilla, and Jenna Finch. The R implemention is my own work.&lt;/p&gt;
&lt;p&gt;You can find the package repository &lt;a href=&#34;https://github.com/cimentadaj/perccalc&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reardon, Sean F. “The widening academic achievement gap between the rich and the poor: New evidence and possible explanations.” Whither opportunity (2011): 91-116.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Replicating Dupriez and Dumay (2006)</title>
      <link>/blog/2017-04-13-replicating-dupriez-and-dumay-2006/replicating-dupriez-and-dumay-2006/</link>
      <pubDate>Thu, 13 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-04-13-replicating-dupriez-and-dumay-2006/replicating-dupriez-and-dumay-2006/</guid>
      <description>&lt;p&gt;So, I was bored for two days and decided I’d replicate a paper I had read. The paper is called ‘Inequalities in school systems: effect of school structure or of society structure?’ written by Vincent Dupriez and Xavier Dumay which you can read from &lt;a href=&#34;http://www.tandfonline.com/doi/full/10.1080/03050060600628074?scroll=top&amp;amp;needAccess=true&#34;&gt;here&lt;/a&gt;. I made available two version, one in &lt;a href=&#34;https://github.com/cimentadaj/Inequality_schools_replication/raw/master/replication_tracking_pisa.pdf&#34;&gt;pdf&lt;/a&gt; and another in &lt;a href=&#34;https://cimentadaj.github.io/Inequality_schools_replication/replication_tracking_pisa.html&#34;&gt;html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you’re interested in looking at the code or suggesting some changes, do send me an email at &lt;code&gt;cimentadaj@gmail.com&lt;/code&gt; or check the &lt;a href=&#34;https://github.com/cimentadaj/Inequality_schools_replication&#34;&gt;Github repository&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My PISA twitter bot</title>
      <link>/blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/</link>
      <pubDate>Wed, 08 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/</guid>
      <description>&lt;p&gt;I’ve long wanted to prepare a project with R related to education. I knew I’d found the idea when I read Thomas Lumley’s &lt;a href=&#34;http://notstatschat.tumblr.com/post/156007757906/a-bus-watching-bot&#34;&gt;attempt to create a Twitter bot in which he tweeted bus arrivals in New Zealand&lt;/a&gt;. Quoting him, “Is it really hard to write a bot? No. Even I can do it. And I’m old.”&lt;/p&gt;
&lt;p&gt;So I said to myself, alright, you have to create a Twitter bot but it has to be related to education. It’s an easy project which shouldn’t take a lot of your time. I then came up with this idea: what if you could randomly sample questions from the &lt;a href=&#34;http://www.oecd.org/pisa/aboutpisa/&#34;&gt;PISA databases&lt;/a&gt; and create a sort of random facts generator. The result would be one graph a day, showing a question for some random sample of countries. I figured, why not prepare a post (both for me to remember how I did it but also so others can contribute to the project) where I explained step-by-step how I did it?&lt;/p&gt;
&lt;p&gt;The repository for the project is &lt;a href=&#34;https://github.com/cimentadaj/PISAfacts_twitterBot&#34;&gt;here&lt;/a&gt;, so feel free to drop any comments or improvements. The idea is to load the &lt;a href=&#34;http://vs-web-fs-1.oecd.org/pisa/PUF_SPSS_COMBINED_CMB_STU_QQQ.zip&#34;&gt;PISA 2015 data&lt;/a&gt;, randomly pick a question that doesn’t have a lot of labels (because then it’s very difficult to plot it nicely), and based on the type of question create an appropriate graph. Of course, all of this needs to be done on the fly, without human assistance. You can follow this twitter account at &lt;span class=&#34;citation&#34;&gt;@DailyPISA_Facts&lt;/span&gt;. Let’s start!&lt;/p&gt;
&lt;div id=&#34;data-wrangling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data wrangling&lt;/h2&gt;
&lt;p&gt;First we load some of the packages we’ll use and read the PISA 2015 student data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(forcats)
library(haven)
library(intsvy) # For correct estimation of PISA estimates
library(countrycode) # For countrycodes
library(cimentadaj) # devtools::install_github(&amp;quot;cimentadaj/cimentadaj&amp;quot;)
library(lazyeval)
library(ggthemes) # devtools::install_github(&amp;quot;jrnold/ggthemes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;file_name &amp;lt;- file.path(tempdir(), &amp;quot;pisa.zip&amp;quot;)

download.file(
  &amp;quot;http://vs-web-fs-1.oecd.org/pisa/PUF_SPSS_COMBINED_CMB_STU_QQQ.zip&amp;quot;,
  destfile = file_name
)

unzip(file_name, exdir = tempdir())

pisa_2015 &amp;lt;- read_spss(file.path(tempdir(), &amp;quot;CY6_MS_CMB_STU_QQQ.sav&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Downloading the data takes a bit but make sure to download the zip file and unzip it as I’ve just outlined.&lt;/p&gt;
&lt;p&gt;The idea is to generate a script that can be used with all PISA datasets, so at some point we should be able not only to randomly pick question but also randomly pick PISA surveys (PISA has been implemented since the year 2000 in three year intervals). We create some places holders for the variable country name, the format of the country names and the missing labels we want to ignore for each question (I think these labels should be the same across all surveys).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;country_var &amp;lt;- &amp;quot;cnt&amp;quot;
country_types &amp;lt;- &amp;quot;iso3c&amp;quot;

missing_labels &amp;lt;- c(&amp;quot;Valid Skip&amp;quot;,
                    &amp;quot;Not Reached&amp;quot;,
                    &amp;quot;Not Applicable&amp;quot;,
                    &amp;quot;Invalid&amp;quot;,
                    &amp;quot;No Response&amp;quot;)

int_data &amp;lt;- pisa_2015 # Create a safe copy of the data, since it takes about 2 mins to read.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After this, I started doing some basic data manipulation. Each line is followed by a comment on why I did it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(int_data) &amp;lt;- tolower(names(int_data)) # It&amp;#39;s easier to write variable names as lower case
int_data$region &amp;lt;- countrycode(int_data[[country_var]], country_types, &amp;quot;continent&amp;quot;)
# Create a region variable to add regional colours to plots at some point.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most PISA datasets are in SPSS format, where the variable’s question has been written as a label. If you’ve used SPSS or SAS you know that labels are very common; they basically outline the question of that variable. In R, this didn’t properly exists until the &lt;code&gt;foreign&lt;/code&gt; and &lt;code&gt;haven&lt;/code&gt; package. With &lt;code&gt;read_spss()&lt;/code&gt;, each variable has now two important attributes called &lt;code&gt;label&lt;/code&gt; and &lt;code&gt;labels&lt;/code&gt;. Respectively, the first one contains the question, while the second contains the value labels (assuming the file to be read has these labels). This information will be vital to our PISA bot. In fact, this script works only if the data has these two attributes. If you’re feeling particularly adventurous, you can fork this repository and make the script work also with metadata!&lt;/p&gt;
&lt;p&gt;Have a look at the country names in &lt;code&gt;int_data[[country_var]][1:10]&lt;/code&gt;. They’re all written as 3-letter country codes. But to our luck, the &lt;code&gt;labels&lt;/code&gt; attribute has the correct names with the 3-letter equivalent. We can save these attributes and recode the 3-letter country name to long names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Saving country names to change 3 letter country name to long country names
country_labels &amp;lt;- attr(int_data[[country_var]], &amp;quot;labels&amp;quot;)

# Reversing the 3-letter code to names so I can search for countries
# in a lookup table
country_names &amp;lt;- reverse_name(country_labels)

# Lookup 3-letter code and change them for long country names
int_data[, country_var] &amp;lt;- country_names[int_data[[country_var]]]
attr(int_data[[country_var]], &amp;quot;labels&amp;quot;) &amp;lt;- country_labels&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next thing I’d like to do is check which variables will be valid, i.e. those which have a &lt;code&gt;labels&lt;/code&gt; attribute, have 2 or more &lt;code&gt;labels&lt;/code&gt; aside from the &lt;code&gt;missing&lt;/code&gt; category of labels and are not either characters or factors (remember that all variables should be numeric with an attribute that contains the labels; character columns are actually invalid here). This will give me the list of variables that I’ll be able to use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;subset_vars &amp;lt;- 
  int_data %&amp;gt;%
  map_lgl(function(x)
    !is.null(attr(x, &amp;quot;labels&amp;quot;)) &amp;amp;&amp;amp;
    length(setdiff(names(attr(x, &amp;quot;labels&amp;quot;)), missing_labels)) &amp;gt;= 2 &amp;amp;&amp;amp;
    !typeof(x) %in% c(&amp;quot;character&amp;quot;, &amp;quot;factor&amp;quot;)) %&amp;gt;%
  which()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great, we have our vector of valid columns.&lt;/p&gt;
&lt;p&gt;The next steps are fairly straight forward. I randomply sample one of those indexes (which have the variale name as a &lt;code&gt;names&lt;/code&gt; attribute, check &lt;code&gt;subset_vars&lt;/code&gt;), together with the &lt;code&gt;cnt&lt;/code&gt; and &lt;code&gt;region&lt;/code&gt; variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;valid_df_fun &amp;lt;- function(data, vars_select) {
  data %&amp;gt;%
  select_(&amp;quot;cnt&amp;quot;, &amp;quot;region&amp;quot;, sample(names(vars_select), 1)) %&amp;gt;%
  as.data.frame()
}

valid_df &amp;lt;- valid_df_fun(int_data, subset_vars)
random_countries &amp;lt;- unique(valid_df$cnt) # To sample unique countries later on&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We also need to check how many labels we have, aside from the &lt;code&gt;missing&lt;/code&gt; labels. In any case, if those unique labels have more than 5, we need to resample a new variable. It’s difficult to understand a plot with that many labels. We need to make our plots as simple and straightforward as possible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;var_labels &amp;lt;- attr(valid_df[[names(valid_df)[3]]], &amp;#39;labels&amp;#39;) # Get labels

# Get unique labels
valid_labels &amp;lt;- function(variable_label, miss) {
  variable_label %&amp;gt;%
    names() %&amp;gt;%
    setdiff(miss)
}

len_labels &amp;lt;- length(valid_labels(var_labels, missing_labels)) # length of unique labels

# While the length of the of the labels is &amp;gt; 4, sample a new variable.
while (len_labels &amp;gt; 4) {
  valid_df &amp;lt;- valid_df_fun(int_data, subset_vars)
  var_labels &amp;lt;- attr(valid_df[[names(valid_df)[3]]], &amp;#39;labels&amp;#39;) # Get labels
  len_labels &amp;lt;- length(valid_labels(var_labels, missing_labels))
}

# Make 100% sure we get the results:
stopifnot(len_labels &amp;lt;= 4)

(labels &amp;lt;- reverse_name(var_labels)) 
# Reverse vector names to objects and viceversa for 
# later recoding.

var_name &amp;lt;- names(valid_df)[3]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before estimating the &lt;code&gt;PISA&lt;/code&gt; proportions, I want to create a record of all variables that have been used. Whenever a graph has something wrong we wanna know which variable it was, so we can reproduce the problem and fix it later in the future.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;new_var &amp;lt;- paste(var_name, Sys.Date(), sep = &amp;quot; - &amp;quot;)
write_lines(new_var, path = &amp;quot;./all_variables.txt&amp;quot;, append = T) 
# I create an empty .txt file to write the vars&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now comes the estimation section. Using the &lt;code&gt;pisa.table&lt;/code&gt; function from the package &lt;code&gt;intsvy&lt;/code&gt; we can correctly estimate the population proportions of any variable for any valid country. This table will be the core data behind our plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;try_df &amp;lt;-
  valid_df %&amp;gt;%
  filter(!is.na(region)) %&amp;gt;%
  pisa.table(var_name, data = ., by = &amp;quot;cnt&amp;quot;) %&amp;gt;%
  filter(complete.cases(.))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s check out the contents of &lt;code&gt;try_df&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##          cnt ec020q11na Freq Percentage Std.err.
## 1  Australia          1 1180      29.91        0
## 2  Australia          2 2336      59.21        0
## 3  Australia          3  429      10.87        0
## 4    Belgium          1  207      25.03        0
## 5    Belgium          2  487      58.89        0
## 6    Belgium          3  133      16.08        0
## 7   Bulgaria          1  826      30.11        0
## 8   Bulgaria          2 1402      51.11        0
## 9   Bulgaria          3  515      18.78        0
## 10   Croatia          1  593      26.05        0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! To finish with the data, we simply need one more thing: to recode the value labels with the &lt;code&gt;labels&lt;/code&gt; vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;try_df[var_name] &amp;lt;- labels[try_df[, var_name]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Awesome. We have the data ready, more or less. Let’s produce a dirty plot to check how long the title is.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;title_question &amp;lt;- attr(valid_df[, var_name], &amp;#39;label&amp;#39;)

ggplot(try_df, aes_string(names(try_df)[2], &amp;quot;Percentage&amp;quot;)) +
  geom_col() +
  xlab(title_question)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-03-08-my-pisa-twitter-bot/2017-03-08-my-pisa-twitter-bot_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: Because the question is randomly sampled, you might be getting a short title. Rerun the script and eventually you’ll get a long one.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So, the question &lt;em&gt;might&lt;/em&gt; have two problems. The wording is a bit confusing (something we can’t really do anything about because that’s how it’s written in the questionnaire) and it’s too long. For the second problem I created a function that cuts the title in an arbitrary cutoff point (based on experimental tests on how many letters fit into a ggplot coordinate plane) but it makes sure that the cutoff is not in the middle of a word, i.e. it searches for the closest end of a word.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Section: Get the title
cut &amp;lt;- 60 # Arbitrary cutoff

# This function accepts a sentence (or better, a title) and cuts it between
# the start and cutoff arguments (just as substr).
# But if the cutoff is not an empty space it will search +-1 index by
# index from the cutoff point until it reaches
# the closest empty space. It will return from start to the new cutoff
sentence_cut &amp;lt;- function(sentence, start, cutoff) {
  
  if (nchar(sentence) &amp;lt;= cutoff) return(substr(sentence, start, cutoff))
  
  excerpt &amp;lt;- substr(sentence, start, cutoff)
  actual_val &amp;lt;- cutoff
  neg_val &amp;lt;- pos_val &amp;lt;- actual_val
  
  if (!substr(excerpt, actual_val, actual_val) == &amp;quot; &amp;quot;) {
    
    expr &amp;lt;- c(substr(sentence, neg_val, neg_val) == &amp;quot; &amp;quot;, substr(sentence, pos_val, pos_val) == &amp;quot; &amp;quot;)
    
    while (!any(expr)) {
      neg_val &amp;lt;- neg_val - 1
      pos_val &amp;lt;- pos_val + 1
      
      expr &amp;lt;- c(substr(sentence, neg_val, neg_val) == &amp;quot; &amp;quot;, substr(sentence, pos_val, pos_val) == &amp;quot; &amp;quot;)
    }
    
    cutoff &amp;lt;- ifelse(which(expr) == 1, neg_val, pos_val)
    excerpt &amp;lt;- substr(sentence, start, cutoff)
    return(excerpt)
    
  } else {
    
    return(excerpt)
    
  }
}

# How many lines in ggplot2 should this new title have? Based on the cut off
sentence_vecs &amp;lt;- ceiling(nchar(title_question) / cut)

# Create an empty list with the length of `lines` of the title.
# In this list I&amp;#39;ll paste the divided question and later paste them together
list_excerpts &amp;lt;- replicate(sentence_vecs, vector(&amp;quot;character&amp;quot;, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just to make sure our function works, let’s do some quick tests. Let’s create the sentence &lt;code&gt;This is my new sentence&lt;/code&gt; and subset from index &lt;code&gt;1&lt;/code&gt; to index &lt;code&gt;17&lt;/code&gt;. Index &lt;code&gt;17&lt;/code&gt; is the letter &lt;code&gt;e&lt;/code&gt; from the word &lt;code&gt;sentence&lt;/code&gt;, so we should cut the sentence to the closest space, in our case, &lt;code&gt;This is my new&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sentence_cut(&amp;quot;This is my new sentence&amp;quot;, 1, 17)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;This is my new &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A more complicated test using &lt;code&gt;I want my sentence to be cut where no word is still running&lt;/code&gt;. Let’s pick from index &lt;code&gt;19&lt;/code&gt;, which is the space between &lt;code&gt;sentence&lt;/code&gt; and &lt;code&gt;to&lt;/code&gt;, the index &lt;code&gt;27&lt;/code&gt;, which is the &lt;code&gt;u&lt;/code&gt; of &lt;code&gt;cut&lt;/code&gt;. Because the length to a space &lt;code&gt;-1 and +1&lt;/code&gt; is the same both ways, the function always picks the shortest length as a defensive mechanism to long titles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sentence_cut(&amp;quot;I want my sentence to be cut where no word is still running&amp;quot;, 19, 27)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot; to be &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the function ready, we have to automate the process so that the first line is cut, then the second line should start where the first line left off and so on.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (list_index in seq_along(list_excerpts)) {
  
  non_empty_list &amp;lt;- Filter(f = function(x) !(is_empty(x)), list_excerpts)
  
  # If this is the first line, the start should 1, otherwise the sum of all characters
  # of previous lines
  start &amp;lt;- ifelse(list_index == 1, 1, sum(map_dbl(non_empty_list, nchar)))
  
  # Because start gets updated every iteration, simply cut from start to start + cut
  # The appropriate exceptions are added when its the first line of the plot.
  list_excerpts[[list_index]] &amp;lt;-
    sentence_cut(title_question, start, ifelse(list_index == 1, cut, start + cut))
}

final_title &amp;lt;- paste(list_excerpts, collapse = &amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above loop gives you a list with the title separate into N lines based on the cutoff point. For the ggplot title, we finish by collapsing the separate titles with the &lt;code&gt;\n&lt;/code&gt; as the separator.&lt;/p&gt;
&lt;p&gt;So, I wrapped all of this into this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;label_cutter &amp;lt;- function(variable_labels, cut) {
  
  variable_label &amp;lt;- unname(variable_labels)
  
  # This function accepts a sentence (or better, a title) and cuts it between
  # the start and cutoff arguments ( just as substr). But if the cutoff is not an empty space
  # it will search +-1 index by index from the cutoff point until it reaches
  # the closest empty space. It will return from start to the new cutoff
  sentence_cut &amp;lt;- function(sentence, start, cutoff) {
    
    if (nchar(sentence) &amp;lt;= cutoff) return(substr(sentence, start, cutoff))
    
    excerpt &amp;lt;- substr(sentence, start, cutoff)
    actual_val &amp;lt;- cutoff
    neg_val &amp;lt;- pos_val &amp;lt;- actual_val
    
    if (!substr(excerpt, actual_val, actual_val) == &amp;quot; &amp;quot;) {
      
      expr &amp;lt;- c(substr(sentence, neg_val, neg_val) == &amp;quot; &amp;quot;, substr(sentence, pos_val, pos_val) == &amp;quot; &amp;quot;)
      
      while (!any(expr)) {
        neg_val &amp;lt;- neg_val - 1
        pos_val &amp;lt;- pos_val + 1
        
        expr &amp;lt;- c(substr(sentence, neg_val, neg_val) == &amp;quot; &amp;quot;, substr(sentence, pos_val, pos_val) == &amp;quot; &amp;quot;)
      }
      
      cutoff &amp;lt;- ifelse(which(expr) == 1, neg_val, pos_val)
      excerpt &amp;lt;- substr(sentence, start, cutoff)
      return(excerpt)
      
    } else {
      
      return(excerpt)
      
    }
  }
  
  # How many lines should this new title have? Based on the cut off
  sentence_vecs &amp;lt;- ceiling(nchar(variable_label) / cut)
  
  # Create an empty list with the amount of lines for the excerpts
  # to be stored.
  list_excerpts &amp;lt;- replicate(sentence_vecs, vector(&amp;quot;character&amp;quot;, 0))
  
  for (list_index in seq_along(list_excerpts)) {
    
    non_empty_list &amp;lt;- Filter(f = function(x) !(is_empty(x)), list_excerpts)
    
    # If this is the first line, the start should 1, otherwise the sum of all characters
    # of previous lines
    start &amp;lt;- ifelse(list_index == 1, 1, sum(map_dbl(non_empty_list, nchar)))
    
    # Because start gets updated every iteration, simply cut from start to start + cut
    # The appropriate exceptions are added when its the first line of the plot.
    list_excerpts[[list_index]] &amp;lt;-
      sentence_cut(variable_label, start, ifelse(list_index == 1, cut, start + cut))
  }
  
  final_title &amp;lt;- paste(list_excerpts, collapse = &amp;quot;\n&amp;quot;)
  final_title
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function accepts a string and a cut off point. It will automatically create new lines if needed and return the separated title based on the cutoff point. We apply this function over the title and the labels, to make sure everything is clean.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_title &amp;lt;- label_cutter(title_question, 60)
labels &amp;lt;- map_chr(labels, label_cutter, 35)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, as I’ve outlined above, each question should have less then four labels. I though that it might be a good idea if I created different graphs for different number of labels. For example, for the two label questions, I thought a simple dot plot might be a good idea —— the space between the dots will sum up to one making it quite intuitive. However, for three and four labels, I though of a cutomized dotplot.&lt;/p&gt;
&lt;p&gt;At the time I was writing this bot I was learning object oriented programming, so I said to myself, why not create a generic function that generates different plots for different labels? First, I need to assign the data frame the appropriate class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;label_class &amp;lt;-
  c(&amp;quot;2&amp;quot; = &amp;quot;labeltwo&amp;quot;, &amp;#39;3&amp;#39; = &amp;quot;labelthree&amp;quot;, &amp;#39;4&amp;#39; = &amp;quot;labelfour&amp;quot;)[as.character(len_labels)]

class(try_df) &amp;lt;- c(class(try_df), label_class)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The generic function, together with its cousin functions, are located in the &lt;code&gt;ggplot_funs.R&lt;/code&gt; script in the PISA bot repository linked in the beginning.&lt;/p&gt;
&lt;p&gt;The idea is simple. Create a generic function that dispatches based on the class of the object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pisa_graph &amp;lt;- function(data, y_title, fill_var) UseMethod(&amp;quot;pisa_graph&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pisa_graph.labeltwo &amp;lt;- function(data, y_title, fill_var) {
  
  dots &amp;lt;- setNames(list(interp(~ fct_reorder2(x, y, z),
                               x = quote(cnt),
                               y = as.name(fill_var),
                               z = quote(Percentage))), &amp;quot;cnt&amp;quot;)
  # To make sure we can randomly sample a number lower than the length
  unique_cnt &amp;lt;- length(unique(data$cnt))
  
  data %&amp;gt;%
    filter(cnt %in% sample(unique(cnt), ifelse(unique_cnt &amp;gt;= 15, 15, 10))) %&amp;gt;%
    mutate_(.dots = dots) %&amp;gt;%
    ggplot(aes(cnt, Percentage)) +
    geom_point(aes_string(colour = fill_var)) +
    labs(y = y_title, x = NULL) +
    scale_colour_discrete(name = NULL) +
    guides(colour = guide_legend(nrow = 1)) +
    scale_y_continuous(limits = c(0, 100),
                       breaks = seq(0, 100, 20),
                       labels = paste0(seq(0, 100, 20), &amp;quot;%&amp;quot;)) +
    coord_flip() +
    theme_minimal() +
    theme(legend.position = &amp;quot;top&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is the graph for the &lt;code&gt;labeltwo&lt;/code&gt; class. Using a work around for non-standard evaluation, I reorder the &lt;code&gt;x&lt;/code&gt; axis. This took me some time to understand but it’s very easy once you’ve written two or three expressions. Create a list with the formula (this might be for &lt;code&gt;mutate&lt;/code&gt;, &lt;code&gt;filter&lt;/code&gt; or whatever &lt;code&gt;tidyverse&lt;/code&gt; function) and &lt;strong&gt;rename&lt;/strong&gt; the placeholders in the formula with the appropriate names. Make sure to name that list object with the new variable name you want for this variable. So, for my example, we’re creating a new variable called &lt;code&gt;cnt&lt;/code&gt; that will be the same variable reordered by the &lt;code&gt;fill_var&lt;/code&gt; and the &lt;code&gt;Percentage&lt;/code&gt; variable.&lt;/p&gt;
&lt;p&gt;After this, I just built a usual &lt;code&gt;ggplot2&lt;/code&gt; object (although notice that I used &lt;code&gt;mutate_&lt;/code&gt; instead of &lt;code&gt;mutate&lt;/code&gt; for the non-standard evaluation).&lt;/p&gt;
&lt;p&gt;If you’re interested in learning more about standard and non-standard evaluation, I found these resources very useful (&lt;a href=&#34;http://www.carlboettiger.info/2015/02/06/fun-standardizing-non-standard-evaluation.html&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://adv-r.had.co.nz/Computing-on-the-language.html&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/lazyeval/vignettes/lazyeval.html&#34;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;The generic for &lt;code&gt;labelthree&lt;/code&gt; and &lt;code&gt;labelfour&lt;/code&gt; are pretty much the same as the previous plot but using a slightly different &lt;code&gt;geom&lt;/code&gt;. Have a look at the original file &lt;a href=&#34;https://raw.githubusercontent.com/cimentadaj/PISAfacts_twitterBot/master/ggplot_funs.R&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We’ll, we’re almost there. After this, we simply, &lt;code&gt;source&lt;/code&gt; the &lt;code&gt;ggplot_funs.R&lt;/code&gt; script and produce the plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;source(&amp;quot;https://raw.githubusercontent.com/cimentadaj/PISAfacts_twitterBot/master/ggplot_funs.R&amp;quot;)
pisa_graph(data = try_df,
             y_title = final_title,
             fill_var = var_name)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2017-03-08-my-pisa-twitter-bot/2017-03-08-my-pisa-twitter-bot_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;file &amp;lt;- tempfile()
ggsave(file, device = &amp;quot;png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-the-twitter-bot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting the twitter bot&lt;/h2&gt;
&lt;p&gt;The final part is automating the twitter bot. I followed &lt;a href=&#34;https://www.r-bloggers.com/programming-a-twitter-bot-and-the-rescue-from-procrastination/&#34;&gt;this&lt;/a&gt; and &lt;a href=&#34;http://www.r-datacollection.com/blog/How-to-conduct-a-tombola-with-R/&#34;&gt;this&lt;/a&gt;. I won’t go into the specifics because I probably wouldn’t do justice to the the second post, but you have to create your account on Twitter, this will give you some keys that make sure you’re the right person. &lt;em&gt;You need to write these key-value pairs as environment variables&lt;/em&gt; (follow the second post) and then delete them from your R script (they’re secret! You shouldn’t keep them on your script but on some folder on your computer). Finally, make sure you identify your twitter account and make your first tweet!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(twitteR) # devtools::install_github(&amp;quot;geoffjentry/twitteR&amp;quot;)
setwd(&amp;quot;./folder_with_my_credentials/&amp;quot;)

api_key             &amp;lt;- Sys.getenv(&amp;quot;twitter_api_key&amp;quot;)
api_secret          &amp;lt;- Sys.getenv(&amp;quot;twitter_api_secret&amp;quot;)
access_token        &amp;lt;- Sys.getenv(&amp;quot;twitter_access_token&amp;quot;)
access_token_secret &amp;lt;- Sys.getenv(&amp;quot;twitter_access_token_secret&amp;quot;)
setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)

tweet(&amp;quot;&amp;quot;, mediaPath = file)
unlink(file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it! The last line should create the&lt;code&gt;tweet&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;automating-the-bot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Automating the bot&lt;/h2&gt;
&lt;p&gt;The only thing left to do is automate this to run every day. I’ll explain how I did it for OSx by following &lt;a href=&#34;http://www.techradar.com/how-to/computing/apple/terminal-101-creating-cron-jobs-1305651&#34;&gt;this&lt;/a&gt; tutorial. You can find a Windows explanation in step 3 &lt;a href=&#34;https://www.r-bloggers.com/programming-a-twitter-bot-and-the-rescue-from-procrastination/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, we need to figure out the specific time we want to schedule the script. We define the time by filling out five stars:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;*****&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first asterisk is for specifying the minute of the run (0-59)&lt;/li&gt;
&lt;li&gt;The second asterisk is for specifying the hour of the run (0-23)&lt;/li&gt;
&lt;li&gt;The third asterisk is for specifying the day of the month for the run (1-31)&lt;/li&gt;
&lt;li&gt;The fourth asterisk is for specifying the month of the run (1-12)&lt;/li&gt;
&lt;li&gt;The fifth asterisk is for specifying the day of the week (where Sunday is equal to 0, up to Saturday is equal to 6)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Taken from &lt;a href=&#34;http://www.techradar.com/how-to/computing/apple/terminal-101-creating-cron-jobs-1305651&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For example, let’s say we wanted to schedule the script for &lt;code&gt;3:00 pm&lt;/code&gt; every day, then the combination would be &lt;code&gt;0 15 * * *&lt;/code&gt;. If we wanted something every &lt;code&gt;15&lt;/code&gt; minutes, then &lt;code&gt;15 * * * *&lt;/code&gt; would do it. If we wanted to schedule the script for Mondays and Wednesdays at &lt;code&gt;15:00&lt;/code&gt; and &lt;code&gt;17:00&lt;/code&gt; respectively, then we would write &lt;code&gt;0 15,17 * * 1,3&lt;/code&gt;. In this last example the &lt;code&gt;* *&lt;/code&gt; are the placeholders for day of the month and month.&lt;/p&gt;
&lt;p&gt;In my example, I want the script to run every weekday at &lt;code&gt;9:30&lt;/code&gt; am, so my equivalent would be &lt;code&gt;30 9 * * 1-5&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To begin, we type &lt;code&gt;env EDITOR=nano crontab -e&lt;/code&gt; in the &lt;code&gt;terminal&lt;/code&gt; to initiate the &lt;code&gt;cron&lt;/code&gt; file that will run the script. Next, type our time schedule followed by the command that will run the script in R. The command is &lt;code&gt;RScript&lt;/code&gt;. However, because your terminal might not know where &lt;code&gt;RScript&lt;/code&gt; is we need to type the directory to where RScript is. Type &lt;code&gt;which RScript&lt;/code&gt; in the terminal and you shall get something like &lt;code&gt;/usr/local/bin/RScript&lt;/code&gt;. Then the expression would be something like &lt;code&gt;30 9 * * 1-5 /usr/local/bin/RScript path_to_your/script.R&lt;/code&gt;. See &lt;a href=&#34;https://support.rstudio.com/hc/en-us/articles/218012917-How-to-run-R-scripts-from-the-command-line&#34;&gt;here&lt;/a&gt; for the &lt;code&gt;RScript&lt;/code&gt; explanation.&lt;/p&gt;
&lt;p&gt;The whole sequence would be like this:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;env EDITOR=nano crontab -e
30 9 * * 1-5 /usr/local/bin/RScript path_to_your/script.R&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To save the file, press Control + O (to write out the file), then enter to accept the file name, then press Control + X (to exit nano). If all went well, then you should see “crontab: installing new crontab” without anything after that.&lt;/p&gt;
&lt;p&gt;Aaaaaand that’s it! You now have a working script that will be run from Monday to Friday at 9:30 am. This script will read the PISA data, pick a random variable, make a graph and tweet it. You can follow this twitter account at &lt;span class=&#34;citation&#34;&gt;[@DailyPISA_Facts]&lt;/span&gt;(&lt;a href=&#34;https://twitter.com/DailyPISA_Facts&#34; class=&#34;uri&#34;&gt;https://twitter.com/DailyPISA_Facts&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Hope this was useful!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cognitive inequality around the world – Shiny app</title>
      <link>/blog/2016-12-12-cognitive-inequality-around-the-world--shiny-app/cognitive-inequality-around-the-world--shiny-app/</link>
      <pubDate>Mon, 12 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-12-12-cognitive-inequality-around-the-world--shiny-app/cognitive-inequality-around-the-world--shiny-app/</guid>
      <description>&lt;p&gt;For the last month I’ve been working on this massive dataset that combines all PISA, TIMSS and PIRLS surveys into one major database. It has over 3 million students and over 2,000 variables, including student background and school and teacher information. I started playing around with it and ending up doing this: &lt;a href=&#34;https://cimentadaj.shinyapps.io/shiny/&#34; class=&#34;uri&#34;&gt;https://cimentadaj.shinyapps.io/shiny/&lt;/a&gt;. Feel free to check it out and drop any comments below.&lt;/p&gt;
&lt;p&gt;If you want to contribute, &lt;a href=&#34;https://github.com/cimentadaj/Inequality_Shinyapp&#34;&gt;this&lt;/a&gt; is the Github repository. I plan to keep adding some stuff to the app, including new surveys and automatic plot downloading, so don’t forget to check it out.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fitting the wrong model</title>
      <link>/blog/2016-11-10-fitting-the-wrong-model/fitting-the-wrong-model/</link>
      <pubDate>Thu, 10 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-11-10-fitting-the-wrong-model/fitting-the-wrong-model/</guid>
      <description>&lt;p&gt;These exercises are from the book &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/arm/&#34;&gt;Data Analysis Using Regression and Multilevel/Hierarchical Models&lt;/a&gt;. I’ve really gotten into completing these exercises and I guess that by posting them I’ve found an excuse to keep doing it. This time I went back to chapter 8 which deals with simulations. I picked the first exercise, page 165 exercise 8.6.1, which says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fitting the wrong model: suppose you have 100 data points that arose from the following model: y = 3 + 0.1×1 + 0.5×2 + error, with errors having a t distribution with mean 0, scale 5, and 4 degrees of freedom.We shall explore the implications of fitting a standard linear regression to these data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The (a) section of the exercises says as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Simulate data from this model. For simplicity, suppose the values of x1 are simply the integers from 1 to 100, and that the values of x2 are random and equally likely to be 0 or 1. Fit a linear regression (with normal errors) to these data and see if the 68% confidence intervals for the regression coefficients (for each, the estimates ±1 standard error) cover the true values.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is simple enough. Simulate some linear model but change the error term to be t distributed with a set of characteristics. Here’s the code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressWarnings(suppressMessages({
  library(arm)
  library(broom)
  library(hett)
  }))

set.seed(2131)
x1 &amp;lt;- 1:100
x2 &amp;lt;- rbinom(100, 1, 0.5)
error1 &amp;lt;- rt(100, df=4)*sqrt(5 * (4-2)/4) + 0 # t distributed errors
                                              # with df 4, mean 0 and var 5

y = 3 + 0.1*x1 + 0.5*x2 + error1

display(lm(y ~ x1 + x2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lm(formula = y ~ x1 + x2)
##             coef.est coef.se
## (Intercept) 3.30     0.43   
## x1          0.10     0.01   
## x2          0.33     0.40   
## ---
## n = 100, k = 3
## residual sd = 1.96, R-Squared = 0.67&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It looks like the true slope of x1 is contained in the 68% CI’s.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(upper = 0.10 + (1 * 0.01), lower = 0.10 + (-1 * 0.01))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## upper lower 
##  0.11  0.09&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For x2 it’s contained but the uncertainty is too high making the CI’s too wide.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c(upper = 0.33 + (1 * 0.40), lower = 0.33 + (-1 * 0.40))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## upper lower 
##  0.73 -0.07&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Put the above step in a loop and repeat 1000 times. Calculate the confidence coverage for the 68% intervals for each of the three coefficients in the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coefs &amp;lt;- array(NA, c(3, 1000))
se &amp;lt;- array(NA, c(3, 1000))

# Naturally, these estimates will be different for anyone who runs this code
# even if specifying set seed because the loop will loop new numbers each time.

for (i in 1:ncol(coefs)) {
  x1 &amp;lt;- 1:100
  x2 &amp;lt;- rbinom(100, 1, 0.5)
  error1 &amp;lt;- rt(100, df=4)*sqrt(5 * (4-2)/4) + 0 # t distributed errors
                                                # with df 4 and mean 0
  y = 3 + 0.1*x1 + 0.5*x2 + error1
  
  mod1 &amp;lt;- summary(lm(y ~ x1 + x2))
  coefs[1,i] &amp;lt;- tidy(mod1)[1,2]
  coefs[2,i] &amp;lt;- tidy(mod1)[2,2]
  coefs[3,i] &amp;lt;- tidy(mod1)[3,2]
  
  se[1,i] &amp;lt;- tidy(mod1)[1,3]
  se[2,i] &amp;lt;- tidy(mod1)[2,3]
  se[3,i] &amp;lt;- tidy(mod1)[3,3]
}

repl_coef &amp;lt;- rowMeans(coefs)
repl_se &amp;lt;- rowMeans(se)

cbind(repl_coef + (-1 * repl_se), repl_coef + (1 * repl_se))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]      [,2]
## [1,] 2.48215326 3.4808804
## [2,] 0.09255549 0.1079026
## [3,] 0.05919238 0.9495994&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Going back to previous block of code which contains the true parameters, the 68% interval for the intercept does contain 3, the 68% interval for x1 does contain 0.10 and both CI’s are quite precise. Finally, the confidence interval for x2 does contain 0.5 but the uncertainty is huge. What does this mean? The estimation of the slope for x2 does contain the true parameter but given that our error is too big and the normal distribution of &lt;code&gt;lm&lt;/code&gt; does not account for that, it presents much more uncertainty in the estimation of the slope. If we ran a t distributed &lt;code&gt;lm&lt;/code&gt; then it will certainly be more precise.&lt;/p&gt;
&lt;p&gt;The last section of the exercise asks you to do exactly that. Repeat the previous loop but instead of using &lt;code&gt;lm&lt;/code&gt;, use &lt;code&gt;tlm&lt;/code&gt; from the hett package which accounts for a t-distributed error term. Compare the CI’s and coefficients. Let’s do it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Repeat this simulation, but instead fit the model using t errors (see Exercise 6.6). The only change here is defining error1 as a t distribution instead of normally distributed&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coefs &amp;lt;- array(NA, c(3, 1000))
se &amp;lt;- array(NA, c(3, 1000))

for (i in 1:ncol(coefs)) {
  x1 &amp;lt;- 1:100
  x2 &amp;lt;- rbinom(100, 1, 0.5)
  error1 &amp;lt;- rt(100, df=4)*sqrt(5 * (4-2)/4) + 0 # t distributed errors
  y = 3 + 0.1*x1 + 0.5*x2 + error1
  
  mod1 &amp;lt;- summary(tlm(y ~ x1 + x2))
  coefs[1,i] &amp;lt;- mod1$loc.summary$coefficients[1,1]
  coefs[2,i] &amp;lt;- mod1$loc.summary$coefficients[2,1]
  coefs[3,i] &amp;lt;- mod1$loc.summary$coefficients[3,1]
  
  se[1,i] &amp;lt;- mod1$loc.summary$coefficients[1,2]
  se[2,i] &amp;lt;- mod1$loc.summary$coefficients[2,2]
  se[3,i] &amp;lt;- mod1$loc.summary$coefficients[3,2]
}

repl_coef &amp;lt;- rowMeans(coefs)
repl_se &amp;lt;- rowMeans(se)

cbind(repl_coef + (-1 * repl_se), repl_coef + (1 * repl_se))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]      [,2]
## [1,] 2.61212284 3.4265738
## [2,] 0.09335461 0.1058854
## [3,] 0.14971691 0.8769205&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Accounting for the t-distributed error (so the tails are much wider), the intervals for the intercept and x1 are quite similar (but narrower) and for x2 they’re certainly much more narrow. Note that the CI is still pretty big, reflecting the variance in the error term. But whenever this variance exceeds what a normal distribution can capture, we should account for it: it might help to reduce the uncertainty in the estimation. Note that, if you reran both simulations and compare the coefficients in &lt;code&gt;repl_coef&lt;/code&gt;, they’re practically the same. So the different estimations don’t affect the parameters, but rather the uncertainty with which we trust them.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multilevel modeling – Part 1</title>
      <link>/blog/2016-11-06-multilevel-modeling--part-1/multilevel-modeling--part-1/</link>
      <pubDate>Sun, 06 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-11-06-multilevel-modeling--part-1/multilevel-modeling--part-1/</guid>
      <description>&lt;p&gt;I’ve been reading Andrew Gelman’s and Jennifer Hill’s book again but this time concentrating on the multilevel section of the book. I finished the first chapter (chapter 12) and got fixed on the exercises 12.2, 12.3 and 12.4. I finally completed them and I thought I’d share the three exercises in two posts, mostly for me to come back to these in the future. The first exercise goes as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Write a model predicting CD4 percentage as a function of time with varying intercepts across children. Fit using lmer() and interpret the coefficient for time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Extend the model in (a) to include child-level predictors (that is, group-level predictors) for treatment and age at baseline. Fit using lmer() and interpret the coefficients on time, treatment, and age at baseline.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Investigate the change in partial pooling from (a) to (b) both graphically and numerically.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compare results in (b) to those obtained in part (c).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The data set they’re referring is called ‘CD4’ and as they authors explain in the book it measures ‘… CD4 percentages for a set of young children with HIV who were measured several times over a period of two years. The dataset also includes the ages of the children at each measurement..’. I’m not sure what CD4 means, but that shouldn’t stop us from at least interpreting the results and answering the questions. Let’s start with the exercises:&lt;/p&gt;
&lt;hr /&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Write a model predicting CD4 percentage as a function of time with varying intercepts across children. Fit using lmer() and interpret the coefficient for time. The data argument is excluding some NA’s because the next model is to be compared with this model and we need to have the same number of observations&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressWarnings(suppressMessages(library(arm)))
cd4 &amp;lt;- read.csv(&amp;quot;http://www.stat.columbia.edu/~gelman/arm/examples/cd4/allvar.csv&amp;quot;)

head(cd4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   VISIT newpid       VDATE CD4PCT arv   visage treatmnt CD4CNT baseage
## 1     1      1  6/29/1988      18   0 3.910000        1    323    3.91
## 2     4      1  1/19/1989      37   0 4.468333        1    610    3.91
## 3     7      1  4/13/1989      13   0 4.698333        1    324    3.91
## 4    10      1                 NA   0 5.005000        1     NA    3.91
## 5    13      1 11/30/1989      13   0 5.330833        1    626    3.91
## 6    16      1                 NA  NA       NA        1    220    3.91&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s transform the VDATE variable into date format
cd4$VDATE &amp;lt;- as.Date(cd4$VDATE, format = &amp;quot;%m/%d/%Y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in strptime(x, format, tz = &amp;quot;GMT&amp;quot;): unknown timezone &amp;#39;zone/tz/
## 2017c.1.0/zoneinfo/Europe/Madrid&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod1 &amp;lt;- lmer(CD4PCT ~
               VDATE +
               (1 | newpid),
             data = subset(cd4, !is.na(treatmnt) &amp;amp; !is.na(baseage)))

display(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lmer(formula = CD4PCT ~ VDATE + (1 | newpid), data = subset(cd4, 
##     !is.na(treatmnt) &amp;amp; !is.na(baseage)))
##             coef.est coef.se
## (Intercept) 66.04     9.48  
## VDATE       -0.01     0.00  
## 
## Error terms:
##  Groups   Name        Std.Dev.
##  newpid   (Intercept) 11.65   
##  Residual              7.31   
## ---
## number of obs: 1072, groups: newpid, 250
## AIC = 7914, DIC = 7885.8
## deviance = 7895.9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The time coefficient simply means that as time increases the percentage of CD4 decreases by 0.01 percent for each child. The effect size is really small, although significant. We can also see that most of the variation in CD4 is between children rather than within children (that is between time because that’s the variation within each child)&lt;/p&gt;
&lt;hr /&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Extend the model in (a) to include child-level predictors (that is, group-level predictors) for treatment and age at baseline. Fit using lmer() and interpret the coefficients on time, treatment, and age at baseline.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- lmer(CD4PCT ~
               VDATE +
               treatmnt +
               baseage +
               (1 | newpid),
             data = cd4)

display(mod2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lmer(formula = CD4PCT ~ VDATE + treatmnt + baseage + (1 | newpid), 
##     data = cd4)
##             coef.est coef.se
## (Intercept) 67.28     9.82  
## VDATE       -0.01     0.00  
## treatmnt     1.26     1.54  
## baseage     -1.00     0.34  
## 
## Error terms:
##  Groups   Name        Std.Dev.
##  newpid   (Intercept) 11.45   
##  Residual              7.32   
## ---
## number of obs: 1072, groups: newpid, 250
## AIC = 7906.3, DIC = 7878.8
## deviance = 7886.5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The time coefficients is exactly the same so neither the treatment or the base age is correlated with the date in which the students were measured. Those who were treated have on average about 1.26% more CD4 than the non-treated. And finally, children which were older at the base measure have about 1% less CD4 than younger children at base. The between-child variance went down from 11.65 to 11.45, so either treatment, baseage or both explained some of the differences between children. The within child variation is practically the same.&lt;/p&gt;
&lt;p&gt;The next exercises uses a term called ‘partial pooling’. This term took me some time to understand but it basically means that we’re neither running a regression ignoring any multilevel structure (complete pooling of the groups) or running a regression for each group separately (complete no-pooling). Running a partially pooled model means being able to have single parameters (like in a completely-pooled model), but estimated from separate regression models for each group(like in a complete-no-pooled model).&lt;/p&gt;
&lt;p&gt;How we can investigate the changes in partial pooling? A completely pooled model runs perfectly when you have little to no variation between groups. Whenever a set of predictors shrinks the between group variation, we’re getting closer to a model with less and less between group variation ( so completely pooled). How can we measure this? In our case, because we’re modeling a varying intercept, we can compare the confidence intervals of the intercept of each group intercept and see if the estimation has become more certain. Numerically, we can check whether the between group variation has decreased, becoming closer to a completely-pooled model.&lt;/p&gt;
&lt;hr /&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Investigate the change in partial pooling from (a) to (b) both graphically and numerically.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(suppressWarnings(library(ggplot2)))
# Change in standard errors

# First and second model intercepts
df1 &amp;lt;- coef(mod1)$newpid[,1 , drop = F]
df2 &amp;lt;- coef(mod2)$newpid[,1 , drop = F]
names(df1) &amp;lt;- c(&amp;quot;int&amp;quot;)
names(df2) &amp;lt;- c(&amp;quot;int&amp;quot;)

# Confidence intervals for each intercept for both moels
df1$ci_bottom &amp;lt;- df1$int + (-2 * se.ranef(mod1)$newpid[,1])
df1$ci_upper &amp;lt;- df1$int + (2 * se.ranef(mod1)$newpid[,1])

df2$ci_bottom &amp;lt;- df2$int + (-2 * se.ranef(mod2)$newpid[,1])
df2$ci_upper &amp;lt;- df2$int + (2 * se.ranef(mod2)$newpid[,1])

# Now we need to compare whether the CI&amp;#39;s shrunk from
# the first to the second model

# Calculate difference
df1$diff &amp;lt;- df1$ci_upper - df1$ci_bottom
df2$dff &amp;lt;- df2$ci_upper - df2$ci_bottom

# Create a df with both differences
df3 &amp;lt;- data.frame(cbind(df1$diff, df2$dff))

# Create a difference out of that
df3$diff &amp;lt;- df3$X1 - df3$X2

# Graph it
ggplot(df3, aes(diff)) + geom_histogram(bins = 100) +
  xlim(0, 0.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-11-06-multilevel-modeling--part-1/2016-11-06-multilevel-modeling--part-1_files/figure-html/fig.-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like the difference is always higher than zero which means that in the second model the difference between the upper and lower CI is smaller than in the first model. This suggests we have greater certainty of our estimation by including the two predictors in the model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Numerically, the between-child variance in the first
# model was:
display(mod1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lmer(formula = CD4PCT ~ VDATE + (1 | newpid), data = subset(cd4, 
##     !is.na(treatmnt) &amp;amp; !is.na(baseage)))
##             coef.est coef.se
## (Intercept) 66.04     9.48  
## VDATE       -0.01     0.00  
## 
## Error terms:
##  Groups   Name        Std.Dev.
##  newpid   (Intercept) 11.65   
##  Residual              7.31   
## ---
## number of obs: 1072, groups: newpid, 250
## AIC = 7914, DIC = 7885.8
## deviance = 7895.9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;11.65 / (11.65 + 7.31)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.6144515&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For the second model
display(mod2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lmer(formula = CD4PCT ~ VDATE + treatmnt + baseage + (1 | newpid), 
##     data = cd4)
##             coef.est coef.se
## (Intercept) 67.28     9.82  
## VDATE       -0.01     0.00  
## treatmnt     1.26     1.54  
## baseage     -1.00     0.34  
## 
## Error terms:
##  Groups   Name        Std.Dev.
##  newpid   (Intercept) 11.45   
##  Residual              7.32   
## ---
## number of obs: 1072, groups: newpid, 250
## AIC = 7906.3, DIC = 7878.8
## deviance = 7886.5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;11.45 / (11.45 + 7.32)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.610016&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The between variance went down JUST a little, in line with the tiny reduction in the standard errors of the intercept.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;The last question asks:&lt;/p&gt;
&lt;p&gt;Compare results in (b) to those obtained in part (c).&lt;/p&gt;
&lt;p&gt;It looks like from the results in (c) the second model a bit more certain in making estimations because it shrinks the partial pooling closer to complete-pooling. As Gelman and Hill explain in page 270, multilevel modeling is most effective when closest to complete-pooling because the estimation of individual group parameters can be done much more precisely, specially for groups with a small amount of observations.&lt;/p&gt;
&lt;p&gt;In the next post we’ll cover exercises 12.3 and 12.4 which build on the models outlined here.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multilevel modeling – Part 2</title>
      <link>/blog/2016-11-06-multilevel-modeling--part-2/multilevel-modeling--part-2/</link>
      <pubDate>Sun, 06 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-11-06-multilevel-modeling--part-2/multilevel-modeling--part-2/</guid>
      <description>&lt;p&gt;This is a continuation of the multilevel exercise 12.2 from chapter 12 of Data Analysis Using Regression and Multilevel/Hierarchical Models. In the last post (link to previous post) we explored the basic interpretation of multilevel modeling with a varying intercept and delved into comparing models and understanding partial pooling.&lt;/p&gt;
&lt;p&gt;This was all done by completing the exercise 12.2 from chapter 12 of the book Data Analysis Using Regression and Multilevel/Hierarchical Models. Exercises 12.3 and 12.4 continue using the same dataset and the same models, so I figured I’d post them here. Let’s start.&lt;/p&gt;
&lt;p&gt;Exercise 12.3 asks:&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Predictions for new observations and new groups:&lt;/li&gt;
&lt;/ol&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Use the model fit from Exercise 12.2 (b) to generate simulation of predicted CD4 percentages for each child in the dataset at a hypothetical next time point.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the same model fit to generate simulations of CD4 percentages at each of the time periods for a new child who was 4 years old at baseline.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s start:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressWarnings(suppressMessages(library(arm)))
cd4 &amp;lt;- read.csv(&amp;quot;http://www.stat.columbia.edu/~gelman/arm/examples/cd4/allvar.csv&amp;quot;)

# Let&amp;#39;s recreate the model from 12.2 (b)
cd4$VDATE &amp;lt;- as.Date(cd4$VDATE, format = &amp;quot;%m/%d/%Y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in strptime(x, format, tz = &amp;quot;GMT&amp;quot;): unknown timezone &amp;#39;zone/tz/
## 2017c.1.0/zoneinfo/Europe/Madrid&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod2 &amp;lt;- lmer(CD4PCT ~ VDATE + treatmnt + baseage + (1 | newpid), data = cd4)&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Use the model fit from Exercise 12.2 (b) to generate simulation of predicted CD4 percentages for each child in the dataset at a hypothetical next time point.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For this we have to create a hypothetical next time point. Let’s choose:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_data &amp;lt;- subset(cd4, !is.na(treatmnt) &amp;amp; !is.na(baseage))
pred_data$VDATE &amp;lt;- as.Date(&amp;quot;1989-01-01&amp;quot;)


# Let&amp;#39;s select only the independent variables from the model
pred_data &amp;lt;- pred_data[, -c(1, 4, 5, 6, 8)]

# Now we have a dataset with a fixed new date for each child
# but with the original values for all other variables.

# Let&amp;#39;s estimate the predicted CD4 percentage for each child:
newpred &amp;lt;- predict(mod2, newdata = pred_data)

# That it! We have the predicted percentage of CD4 for a fixed date for every child.
# Let&amp;#39;s look at the distribution
hist(newpred)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-11-06-multilevel-modeling--part-2/2016-11-06-multilevel-modeling--part-2_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There’s quite some variation going from 0% to even 60%. It’d be nice to know what CD4 is.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Question (b) is pretty similar but instead of asking for a new time point for each child, it asks to predict the CD4 percentage for a new child who was 4 years old at the baseline with a set of hypothetical time periods.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;Use the same model fit to generate simulations of CD4 percentages at each of the time periods for a new child who was 4 years old at baseline.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For that we have to first create the dataset with the hypothetical child:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# I&amp;#39;ll assume 7 hypothetical dates
hyp_child &amp;lt;- data.frame(newpid = 120,
                        VDATE = sample(cd4$VDATE[!is.na(cd4$VDATE)], 7),
                        baseage = 4,
                        treatmnt = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The exercise doesn’t say anything about the treatment variable but I have to assign a value to it because it’s present in the previous model. I’ll assume this new child was in the treatment.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;year_pred &amp;lt;- predict(mod2, newdata = hyp_child)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it! Now we have the predicted CD4 percentage for different time points for a hypothetical child.&lt;/p&gt;
&lt;p&gt;Finally, exercise 12.4 posits this question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Posterior predictive checking: continuing the previous exercise, use the fitted model from Exercise 12.2 (b) to simulate a new dataset of CD4 percentages (with the same sample size and ages of the original dataset) for the final time point of the study, and record the average CD4 percentage in this sample. Repeat this process 1000 times and compare the simulated distribution to the observed CD4 percentage at the final time point for the actual data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This problem was a bit confusing because I don’t understand what they mean by the ‘final time point of the study’. Is it the last date in the VDATE variable? But that couldn’t be it because the sample size would be 1. I think what they mean is to take the original data and replace the VDATE variable with the final date available. Predict CD4 percentages for each child with this date (using the original ages of the data set) and calculate the mean of these predictions. After that, do the 1000 replications and compare the distribution with the actual CD4 percentage of the specific date. Let’s start.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make the data similar to the model in mod2
finaltime_data &amp;lt;- subset(cd4, !is.na(treatmnt) &amp;amp; !is.na(baseage))

# Assign the final date to the date variable
finaltime_data$VDATE &amp;lt;- as.Date(max(finaltime_data$VDATE, na.rm = T))

# Only select the variables present in the model
finaltime_data &amp;lt;- finaltime_data[, -c(1, 4, 5, 6, 8)]

# Calculate the mean predicted CD4 percentage and it&amp;#39;s standard deviation
mean_cd4 &amp;lt;- mean(predict(mod2, newdata = finaltime_data), na.rm = T)
sd_cd4 &amp;lt;- sd(predict(mod2, newdata = finaltime_data), na.rm = T)

# Simulate 1000 observations considering the uncertainty
# and look at the distribution.
set.seed(421)

hist(rnorm(1000, mean_cd4, sd = sd_cd4))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-11-06-multilevel-modeling--part-2/2016-11-06-multilevel-modeling--part-2_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s incredibly wide and it even includes negative numbers, something impossible with percentages. But whats the actual CD4 percentage of that date?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cd4 &amp;lt;- subset(cd4, !is.na(VDATE))
cd4[cd4$VDATE == max(cd4$VDATE, na.rm = T), ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      VISIT newpid      VDATE CD4PCT arv   visage treatmnt CD4CNT  baseage
## 483     16     99 1991-01-14   39.0   1 1.497500        2    526 0.347500
## 1208    19    237 1991-01-14   21.2   0 2.510833        2   1036 1.073333&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the mean is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(c(39.0, 21.2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 30.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So our predictions are extremely uncertain. Over half our simulations are underestimating the date and a handful are overestimating the results. It looks like our model is terrible at predicting that final time point.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Well that’s it. Hope you enjoyed it. I’ll be completing more exercises in the future so remember to check back. ```&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Los pobres no son los peores pero los ricos sí son los mejores: desigualdad educativa</title>
      <link>/blog/2016-09-24-los-pobres-no-son-los-peores-pero-los-ricos-s%C3%AD-son-los-mejores/los-pobres-no-son-los-peores-pero-los-ricos-s%C3%AD-son-los-mejores-desigualdad-educativa/</link>
      <pubDate>Sat, 24 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-09-24-los-pobres-no-son-los-peores-pero-los-ricos-s%C3%AD-son-los-mejores/los-pobres-no-son-los-peores-pero-los-ricos-s%C3%AD-son-los-mejores-desigualdad-educativa/</guid>
      <description>&lt;p&gt;Hace poco escribí un artículo sobre la desigualdad educativa en países Latinoamericanos y describí como la República Dominicana no parece estar tan mal como creemos. Ofrecí dos posibles explicaciones de porque esto pudiera ser bueno o malo, dependiendo del enfoque con que se ve (missing link after the ‘,’ with this: &lt;a href=&#34;http://www.jorgecimentada.com/?p=137&#34; class=&#34;uri&#34;&gt;http://www.jorgecimentada.com/?p=137&lt;/a&gt;). Luego de escribirlo me quedé pensando en algo que di por sentado: ¿realmente los ‘peores’ estudiantes son los estudiantes pobres? ¿son los estudiantes pobres los ‘peores’ en las escuelas públicas y privadas? Y a raíz del artículo de &lt;a href=&#34;http://acento.com.do/2016/opinion/8377420-desigualdad-educativa-interpela-desafia/&#34;&gt;Dinorah Garcia&lt;/a&gt;, ¿existe desigualdad entre las zonas rurales y urbanas?&lt;/p&gt;
&lt;p&gt;Estas preguntas no solo ayudan a diagnosticar el estado actual de la desigualdad, más que eso, permiten inferir de donde pudieran provenir las fuentes de desigualdad. Si encontramos una diferencia bastante amplia entre los ‘mejores’ y ‘peores’ estudiantes, pero la diferencia es menor entre escuelas públicas y privada, pues la causa no es necesariamente las diferencias en calidad entre las tipos de escuelas. Sin embargo, si existen diferencias similares entre los estudiantes de escuelas rurales y urbanas, pues la razón puede ser precisamente en temas como la calidad educativa entre sectores pobres y ricos. Lo interesante de estas preguntas es que son todas preguntas empíricas, y se pueden responder con bases de datos como las de TERCE.&lt;/p&gt;
&lt;div id=&#34;realmente-los-peores-estudiantes-son-los-estudiantes-pobres&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. ¿realmente los ‘peores’ estudiantes son los estudiantes pobres?&lt;/h2&gt;
&lt;!-- ![1stplot](img/inquality_grade_topic.png) --&gt;
&lt;p&gt;&lt;img src=&#34;img/inquality_grade_topic.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 600px;&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Este gráfico presente claramente que no lo son. Tanto en matemáticas y lenguaje, los estudiantes más pobres (barra azul) suelen tener una mejor puntuación que los ‘peores’ estudiantes (barra roja). Esto parecer ser el caso sin importar que sean de la zona rural o urbana. Sería informativo comparar estos resultados desagregados por tipo de escuela, es decir, privada o pública, para identificar si los ‘peores’ estudiantes son los de las escuelas públicas.&lt;/p&gt;
&lt;p&gt;En contraste, los estudiantes ricos (barra morada) si tienen la misma puntuación que los ‘mejores’ estudiantes (barra verde). De hecho, en zonas rurales los estudiantes ricos se desempeñan tan bien que obtienen una puntuación más alta que los ‘mejores’ estudiantes. Este gráfico cuenta una historia muy interesante. En resumen, los estudiantes pobres no reflejan ser los ‘peores’ del sistema educativo. Y por otro lado, los estudiantes ricos sí son los mejores estudiantes en sus respectivas materias.&lt;/p&gt;
&lt;p&gt;Desde un punto de vista más general, la distribución entre la zona rural y urbana es muy similar, sugiriendo que no existe una desigualdad muy marcada.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;son-los-estudiantes-pobres-los-peores-en-las-escuelas-publicas-y-privadas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. ¿son los estudiantes pobres los ‘peores’ en las escuelas públicas y privadas?&lt;/h2&gt;
&lt;!-- ![2ndplot](img/inquality_public_topic.png) --&gt;
&lt;p&gt;&lt;img src=&#34;img/inquality_public_topic.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 650px;&#34;/&gt;&lt;/p&gt;
&lt;p&gt;En este gráfico el panorama es un poco diferente. Los estudiantes pobres (barra verde) sí se desempeñan peor que los estudiantes promedios (barra roja) en las escuelas privadas. Esto no significa que son los ‘peores’ ya que el punto de comparación es el estudiante promedio y no los ‘peores’ estudiantes de las escuelas privadas. No obstante, si se nota una estratificación bastante clara: los estudiantes pobres tienen puntuación más baja que los estudiantes promedio, mientras que los estudiantes ricos (barra azul) tienen mayor puntuación que el estudiante average.&lt;/p&gt;
&lt;p&gt;Viendo la otra cara de la moneda, las escuelas públicas, los estudiantes pobres si tienen el mismo desempeño que el estudiante promedio. Esto no significa que han mejorado en comparación con la escuela privada. Todo lo contrario. Los estudiantes pobres tienen un desempeño similar tanto en las escuelas públicas y privadas; lo que pasa es que el estudiante promedio de la escuela pública (barra roja) tienen una puntuación mucho más baja que el mismo estudiante en la escuela privada. Esto sugiere que, en promedio, los estudiantes de escuelas públicas tienen un peor desempeño que los de las escuelas privadas. De hecho, los ricos son igual de buenos sin importar el tipo de escuela. Los pobres, por otro lado, tienen una puntuación similar entre escuelas públicas y privadas, pero tienen un desempeño ligeramente menor en las escuelas públicas.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;existe-desigualdad-entre-las-zonas-rurales-y-urbanas&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. ¿existe desigualdad entre las zonas rurales y urbanas?&lt;/h2&gt;
&lt;!-- ![3rdplot](img/inquality_ruralidad_topic.png) --&gt;
&lt;p&gt;&lt;img src=&#34;img/inquality_ruralidad_topic.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 650px;&#34;/&gt;&lt;/p&gt;
&lt;p&gt;Este gráfico presenta que tanta desigualdad existe entre los estudiantes en 3ro y 6to de primaria en matemática y lengua para las zonas rurales y urbanas. Empecemos con la lengua española, la sección con las puntuaciones en color rojo. Para los estudiantes de 3ro de primaria, resulta que hay una desigualdad mucho más grande en la zona urbana que en la zona rural. Pero esta desigualdad es en gran parte debido a que los ‘mejores’ estudiantes de la zona urbana son mucho mejor que los de la zona rural, mientras que los ‘peores’ estudiantes son prácticamente iguales. ¿Será este el efecto escuela privada? Para 6to de primaria la desigualdad se ha doblado entre ambas zonas. Esto es consistente con la teoría de capital humano propuesta por James Heckman, que sugiere que mientras más pasa el tiempo, más difícil se hace cerrar la brecha entre los ‘mejores’ y ‘peores’ estudiantes. Aquí se nota la gran brecha que existe entre las zonas urbanas y zonas rurales. En la sección de matemáticas (puntuaciones en color azul), la diferencia entre la zona rural y urbana es más pequeña, aunque se nota un patron claro: los estudiantes urbanos son mejores que los rurales. Cualquier persona que &lt;a href=&#34;http://linkis.com/www.listindiario.com/AG90F&#34;&gt;sabe los pocos recursos que se le asignan a las provincias fuera de Santiago y Santo Domingo&lt;/a&gt; no se sorprendería en encontrar estos resultados. No obstante, siempre es bueno confirmar las especulaciones con datos empíricos.&lt;/p&gt;
&lt;div id=&#34;que-significa-todo-esto&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;¿Que significa todo esto?&lt;/h4&gt;
&lt;p&gt;Realmente, toda la información presentada aquí es pura descripción. No podemos concluir que vivir en las zonas rurales o ir a una escuela pública hace que tengas un peor desempeño, por más que quisiera. La ciencia no funciona de esta manera. Aparentemente existen brechas importantes entre las zonas urbanas y rurales, y tal y como sugiere Ceara Hatton, la distribución del presupuesto nacional abarca en su mayoría a Santo Domingo y Santiago. Se podría empezar por priorizar el gasto educativo a los que más lo necesitan, que suelen ser las zonas rurales. Esto tampoco es una solución inmediata; hay que priorizar la calidad del gasto y no el monto.&lt;/p&gt;
&lt;p&gt;Por otro lado, podemos ver que los estudiantes pobres si tienen un peor desempeño que los estudiantes ricos, tanto en las escuelas privadas como en las públicas. Sugerir que se debe mejorar la calidad educativa pública ya se sabe, y si esta fuera homogénea a lo largo de las escuelas esperaríamos que los estudiantes ricos tuvieran la misma puntuación que los pobres, algo que no es el caso. Me parece que una de las explicaciones puede estar en el hecho de que los pobres tienen una ‘racha’ o acumulación de malas experiencias educativas a lo largo de la vida. Al momento de llegar a 3ro o 6to de primaria ya están muy por detrás de sus compañeros de mayores recursos. &lt;a href=&#34;http://www.diariolibre.com/opinion/dialogo-libre/el-problema-es-que-la-economia-crece-pero-no-se-distribuye-la-riqueza-EE5262978&#34;&gt;El acceso a educación temprana, de muy alta calidad, de forma universal, puede ayudar a remediar este problema de inmovilidad social&lt;/a&gt;. Esto es algo que muchas personas ya han pedido en otros países y no es nuevo.&lt;/p&gt;
&lt;p&gt;Para terminar en una nota buena, no nos debemos sentir tan mal por estos resultados: como dije aquí(poner link &lt;a href=&#34;http://www.jorgecimentada.com/?p=137&#34; class=&#34;uri&#34;&gt;http://www.jorgecimentada.com/?p=137&lt;/a&gt;), en comparación con cualquier otro país latinoamericano, tenemos mucha menor desigualdad, lo importante sería asegurarnos que no crezca, porque es muy difícil cerrarlo después.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Notas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;La pobreza en este caso se mide como los estudiantes que vienen de familias donde sus padres no se graduaron de la escuela secundaria. Opuestamente, los ricos son los estudiantes que vienen de familias donde los padres han obtenido un grado universitario o más.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;El promedio de los ‘peores’ estudiantes es el promedio de todos los niños que están en el 25% de la parte inferior de la distribución, mientras que el promedio de los ‘mejores’ estudiantes es el promedio del 25% de la parte superior de la distribución.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cualquier persona interesada en reproducir estos análisis, &lt;a href=&#34;https://github.com/cimentadaj/blog-articles/tree/master/private_public_schools&#34;&gt;aquí&lt;/a&gt; está el código&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Obtaining robust standard errors and odds ratios for logistic regression in R</title>
      <link>/blog/2016-09-19-obtaining-robust-standard-errors-and-odds-ratios/obtaining-robust-standard-errors-and-odds-ratios-for-logistic-regression-in-r/</link>
      <pubDate>Mon, 19 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-09-19-obtaining-robust-standard-errors-and-odds-ratios/obtaining-robust-standard-errors-and-odds-ratios-for-logistic-regression-in-r/</guid>
      <description>&lt;p&gt;I’ve always found it frustrating how it’s so easy to produce robust standard errors in Stata and in R it’s so complicated. First, we have to estimate the standard errors separately and then replace the previous standard errors with the new ones. Second, if you want to estimate odds ratios instead of logit coefficients, then the robust standard errors need to be scaled. All of that is as simple as adding robust or in the Stata logit command.&lt;/p&gt;
&lt;p&gt;I decided to make it as simple in R.&lt;/p&gt;
&lt;p&gt;First, I have to give credit to &lt;a href=&#34;http://stackoverflow.com/users/4233642/achim-zeileis&#34;&gt;Achim Zeileis&lt;/a&gt; in this &lt;a href=&#34;http://stackoverflow.com/questions/27367974/different-robust-standard-errors-of-logit-regression-in-stata-and-r&#34;&gt;question&lt;/a&gt; because he provided part of code to generate the robust standard errors.&lt;/p&gt;
&lt;p&gt;The function accepts a glm object and can return logit coefficients with robust standard errors, odd ratios with adjusted robust standard errors or probability scaled coefficients with adjusted robust standard errors.&lt;/p&gt;
&lt;p&gt;Here’s the function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This function estimates robust standad error for glm objects and
# returns coefficients as either logit, odd ratios or probabilities.
# logits are default
# argument x must be glm model.


# Credit to Achim here:
# http://stackoverflow.com/questions/27367974/
# different-robust-standard-errors-of-logit-regression-in-stata-and-r
# for the code in line 14 and 15

robustse &amp;lt;- function(x, coef = c(&amp;quot;logit&amp;quot;, &amp;quot;odd.ratio&amp;quot;, &amp;quot;probs&amp;quot;)) {
suppressMessages(suppressWarnings(library(lmtest)))
suppressMessages(suppressWarnings(library(sandwich)))

    sandwich1 &amp;lt;- function(object, ...) sandwich(object) *
                                       nobs(object) / (nobs(object) - 1)
    # Function calculates SE&amp;#39;s
    mod1 &amp;lt;- coeftest(x, vcov = sandwich1) 
    # apply the function over the variance-covariance matrix
    
    if (coef == &amp;quot;logit&amp;quot;) {
    return(mod1) # return logit with robust SE&amp;#39;s
    } else if (coef == &amp;quot;odd.ratio&amp;quot;) {
    mod1[, 1] &amp;lt;- exp(mod1[, 1]) # return odd ratios with robust SE&amp;#39;s
    mod1[, 2] &amp;lt;- mod1[, 1] * mod1[, 2]
    return(mod1)
    } else {
    mod1[, 1] &amp;lt;- (mod1[, 1]/4) # return probabilites with robust SE&amp;#39;s
    mod1[, 2] &amp;lt;- mod1[, 2]/4
    return(mod1)
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s give it a try. Let’s estimate two models, one with logit coefficients and robust SE’s and the same for odds ratios. Just to make sure, let’s compare it with the Stata output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# In R for logit coefficients and robust standard errors:
suppressMessages(suppressWarnings(library(haven)))

dat &amp;lt;- read_dta(&amp;quot;http://www.stata-press.com/data/r9/quad1.dta&amp;quot;)
mod1 &amp;lt;- glm(z ~ x1 + x2 + x3, dat, family = binomial)
robustse(mod1, coef = &amp;quot;logit&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## z test of coefficients:
## 
##             Estimate Std. Error z value  Pr(&amp;gt;|z|)    
## (Intercept) 0.091740   0.025869  3.5464 0.0003905 ***
## x1          0.024050   0.025869  0.9297 0.3525395    
## x2          0.157143   0.089715  1.7516 0.0798448 .  
## x3          0.190162   0.089418  2.1267 0.0334490 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# In Stata for logit coefficients and robust standard errors:
use http://www.stata-press.com/data/r9/quad1.dta, clear
logit z x1 x2 x3, robust

# ------------------------------------------------------------------------------
#              |               Robust
#            z |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
# -------------+----------------------------------------------------------------
#           x1 |     .02405   .0258693     0.93   0.353    -.0266528    .0747529
#           x2 |   .1571428   .0897145     1.75   0.080    -.0186944      .33298
#           x3 |    .190162   .0894185     2.13   0.033      .014905    .3654189
#        _cons |     .09174   .0258686     3.55   0.000     .0410386    .1424415
# ------------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# In R for odd ratios with adjusted standard errors
robustse(mod1, coef = &amp;quot;odd.ratio&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## z test of coefficients:
## 
##             Estimate Std. Error z value  Pr(&amp;gt;|z|)    
## (Intercept) 1.096080   0.028354  3.5464 0.0003905 ***
## x1          1.024342   0.026499  0.9297 0.3525395    
## x2          1.170163   0.104981  1.7516 0.0798448 .  
## x3          1.209445   0.108147  2.1267 0.0334490 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# In Stata for odd ratios with adjusted standard errors
logit z x1 x2 x3, robust or

# ------------------------------------------------------------------------------
#              |               Robust
#            z | Odds Ratio   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
# -------------+----------------------------------------------------------------
#           x1 |   1.024342    .026499     0.93   0.353     .9736992    1.077618
#           x2 |   1.170163   .1049806     1.75   0.080     .9814792    1.395119
#           x3 |   1.209445   .1081467     2.13   0.033     1.015017    1.441118
#        _cons |    1.09608    .028354     3.55   0.000     1.041892    1.153086
# ------------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also use the &lt;code&gt;stargazer&lt;/code&gt; package to produce nicely formatted tables with these new estimates and it should be exactly the same.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;UPDATE: I’ve included this function my personal package which you can install with &lt;code&gt;devtools::install_github(&amp;quot;cimentadaj/cimentadaj&amp;quot;)&lt;/code&gt;. Feel free to make any pull requests in the github repo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulations and model predictions in R</title>
      <link>/blog/2016-09-13-simulations-and-model-predictions-in-r/simulations-and-model-predictions-in-r/</link>
      <pubDate>Tue, 13 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-09-13-simulations-and-model-predictions-in-r/simulations-and-model-predictions-in-r/</guid>
      <description>&lt;p&gt;I was on a flight from Asturias to Barcelona yesterday and I finally had some free time to open Gelman and Hill’s book and submerge in some studying. After finishing the chapter on simulations, I tried doing the first exercise and enjoyed it very much.&lt;/p&gt;
&lt;p&gt;The exercise goes as follows:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Discrete probability simulation: suppose that a basketball player has a 60% chance of making a shot, and he keeps taking shots until he misses two in a row. Also assume his shots are independent (so that each shot has 60% probability of success, no matter what happened before).&lt;/em&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Write an R function to simulate this process.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Put the R function in a loop to simulate the process 1000 times. Use the simulation to estimate the mean, standard deviation, and distribution of the total number of shots that the player will take.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using your simulations, make a scatterplot of the number of shots the player will take and the proportion of shots that are successes.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below you can find my answer with some comments on how I did it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# a)
# The probs argument sets the probability of making a shot. In this case it&amp;#39;ll be 0.60
thrower &amp;lt;- function(probs) {
  vec &amp;lt;- replicate(2, rbinom(1, 1, probs)) 
  # create a vector with two random numbers of either 1 or 0,
  # with a probability of probs for 1
  
  # While the sum of the last and the second-last element is not 0
  while ((vec[length(vec)] + vec[length(vec) - 1]) != 0) { 
    
      vec &amp;lt;- c(vec, rbinom(1, 1, probs))
      # keep adding random shots with a probability of probs
  }
return(vec)
}
# The loop works because whenever the sum of the last two elements is 0,
# then the last two elements must be 0 meaning that the player missed two
# shots in a row.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# test
thrower(0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 0 1 0 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 0 1 0 1 0 0
# Last two elements are always zero&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# b)
attempts &amp;lt;- replicate(1000, thrower(0.60))
mean(sapply(attempts, length)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8.977&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# mean number of shots until two shots are missed in a row&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sd(sapply(attempts, length)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 7.613569&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# standard deviation of shots made
# until two shots are missed in a row&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(sapply(attempts, length)) # distribution of shots made&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-09-13-simulations-and-model-predictions-in-r/2016-09-13-simulations-and-model-predictions-in-r_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# c)
df &amp;lt;- cbind(sapply(attempts, mean), sapply(attempts, length)) 
# data frame with % of shots made and number of shots thrown
plot(df[, 2], df[, 1])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/2016-09-13-simulations-and-model-predictions-in-r/2016-09-13-simulations-and-model-predictions-in-r_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That was fun. I think the key take away here is that you can use these type of simulations to asses the accuracy of model predictions, for example. If you have the probability of being in either 1 or 0 in any dependent variable, then simulation can help determine its reliability by looking at the distribution of the replications.&lt;/p&gt;
&lt;p&gt;Whenever I have some free time I’ll go back to the next exercises.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Children: public or private goods?</title>
      <link>/blog/2016-08-29-children-public-or-private-goods/children-public-or-private-goods/</link>
      <pubDate>Mon, 29 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-08-29-children-public-or-private-goods/children-public-or-private-goods/</guid>
      <description>&lt;p&gt;The role of the welfare state, as agreed by many scholars, is to de-commodify its citizens. Some argue that this should be the way that the generosity of the state should be measured; instead of net public and private social expenditure, it should be the extent to which citizens are de-commodified by the services and transfers of the welfare state.&lt;/p&gt;
&lt;p&gt;The aim of educational investment can be seen from different lenses, but today we’ll focus on the economic and social returns to education. An important share of the state budget is aimed at the formation and education of citizens through public education. In countries where there is a substantial investment, human capital, innovation and production are maximized creating a highly productive working population that can sustain the young and the elderly through pensions, health and child care; note that in some scenarios an uneducated workforce can sustain those who depend on them but it will possible be with more quantity of work (hours) and less quality [1]. Following this logic, it is arguable that because most of today’s workers will benefit from pensions, retirement schemes, care assistance and healthcare, they are to take care of their young population, the next generation’s workers. Covering kids’ education through higher taxes could be seen as loan: A young worker covers the expenses of a child, and that child, eventually, will cover the pension of that worker; this is a simplified version of the argument.&lt;/p&gt;
&lt;p&gt;The scheme I just outlined is virtually widespread across western European countries. But it is important to note that in other countries, due to the intrinsic organization of the state, it is unimaginable to do so. In the counterfactual situation where you don’t have a strong welfare state, maybe it’s unfair to tax those who are not benefiting from the goods that the taxes are being spent on. In places like the United States, where the generosity of the welfare system is minimal, some people don’t find it reasonable to pay for everyone’s kids, even more, when you’re paying already, for instance, a private school for your own.&lt;/p&gt;
&lt;p&gt;There’s also another important case: childless couples. Perhaps the exact reason why they didn’t have children was not to have to spend a big proportion of their lifelong income. So, why should they pay? According to their position it seems very unfair. Their long held argument can be synthesized in one sentence: those who want children should be the ones who pay for them.&lt;/p&gt;
&lt;p&gt;After presenting very briefly the arguments of the positions in the debate, it is interesting then to posit the important question concerning it: Should children be a public or private good? I presume this question is easy to answer but not easy to apply; adjusting a state to this premise can take decades and it’s an expensive endeavor. So, evaluating this question must be accompanied by hard scientific thinking.&lt;/p&gt;
&lt;p&gt;I will start off by stating my position on this dilemma. I support the notion of kids being a public good. I support it not just for the fact that they will pay or not for my pension schemes but because everyone deserves the same opportunities. And if we don’t provide them with equal opportunities, then the gap we see between poor and rich will be as it is today: very sizable. Despite that I sometimes question myself: can they entirely be a public good? Adopting the notion that the family is a private sphere, and at least the states considers it because it is reluctant to interfere within it, then children should not just be considered as a public good. If we pay attention to authors like McDonald (2000) and Lewis &amp;amp; Astrom (1992), we constantly see that the way the state promotes gender equality in the family is not through hands-on, direct implementation of intrusive policies like reeducating couples and telling them what’s right and what’s not. Instead, most of the family policies try to promote gender equality by integrating women into the labor force, reducing the gender wage gap and promoting female and male parental leave. Kids then can never be entirely public goods.&lt;/p&gt;
&lt;p&gt;Nevertheless, do they contribute to society, meaning, are the interrelated with the “public”?&lt;/p&gt;
&lt;p&gt;I think this question is rooted in the intrinsic dynamics of society. We are social animals and it is unfashionable to think that we are independent of each other. The most independent citizen living within society is never unattached of its surroundings. Everyone’s paths cross indirectly and directly with the decisions and actions of other people. It might be even irresponsible to think that one’s actions only affect themselves. The principal idea to take from this question is that those who think of themselves as independent actors of society are committing a fallacy that could be the exact reason why the levels and indexes of inequalities are so high nowadays.&lt;/p&gt;
&lt;p&gt;The Nobel laureate James Heckman has been arguing for the past decade one of his most controversial findings: the first years of a child are the most important ones. Contrary to the long held thought of concentrating the bigger part of the educational budget on tertiary education, he finds that the return of investment of human beings are the highest in the early years and it decreases with time. This means that the first years should be the ones where the highest investment should be: be it monetary and emotional. With the evaluation of the Perry preschool program and the Abecedarian project, Heckman finds that you can trace inequalities between children as early as 22 months old. The principal reason being that some kids get better early childhood education than others. Among others, I think this an important reason to reflect why children should be a public good.&lt;/p&gt;
&lt;p&gt;Inequalities are being reproduced from an early age, leading to a more polarized society where you have lucky and unlucky kids. Those that receive the better education will at the end, have better jobs, less odds of committing crime, more prone of a stable union (possibly due to the fact that they will have a stable job). Furthermore, and I think this is a key argument against those that don’t want to contribute, these educated children will in most of the cases, help society better than the ones who aren’t educated. This help can be manifested in direct and indirect ways: respecting traffic lights, avoiding corruption in public institutions, being fair, meritocratic, reporting robberies, not stealing, respecting others, among an endless number of thing. This point makes an important argument: the focus on which some of the parents and scholars are viewing the debate might be wrongly specified. As I have succinctly showed, the benefits of having an educated population are not only personal, but also public. I think that the principal reason why there is a strong opposition to this idea is because they evaluate it as taking care of someone else’s children when nobody is taking care of theirs; that is a strong point, but I urge everyone to see it with another lense. If you turned the coin around and you start seeing this as an investment towards societal problems, which in the short and long run affects us all, then it becomes a tax deduction, just as any other one.&lt;/p&gt;
&lt;p&gt;As I argued before, we are all interrelated and it is our business what our fellow citizens do. So, someone who doesn’t have children, still participates in society, interacts within it, and is affected by criminality, public services, and private services. The problem with providing this argument for a country with a weak welfare state is that you won’t receive quality elderly care from this children when you retire. Conversely, in strong-welfare countries, the argument is the main (pragmatic) reason to do so. However, the second idea, that we should do it for everyone to have the same opportunities, is more than enough to warrant the changes. This would be an even more noble reason because you won’t be contributing to receive something but because everyone deserves the same opportunities. For the U.S, this is a reason to tackle one of the most debatable topics of the last 40 years: inequalities.&lt;/p&gt;
&lt;p&gt;Now, if we were to delve more into the inequality problem, which shows itself to be the major issue when comparing educational opportunities, it is also important to think of the struggles of having children. Not particularly of the difficulties of childrearing, which are a consequence of the decision to child bear, but the fact that it is too hard to combine work and childrearing now a days. This is quite notorious in the lean welfare state countries and southern European ones.&lt;/p&gt;
&lt;p&gt;The demands of the work space and the schedules that are imposed to parents and workers make it virtually impossible for men or women to dedicated high quality time to their children. The most common solution that we’ve seen is for women to quit their jobs or to sacrifice leisure time to take care of the child. So, in the end, this deems either the education of the future population or the ambitions and life course perspective of a mother.&lt;/p&gt;
&lt;p&gt;Catherine Hakim’s preference theory, posits that women now have the preference of having a child but this was not entirely the case in the past where the bargaining power of the man was higher than hers. But a woman who has professional ambitions might also want to have a baby. Does this mean that she has to renounce her ambitions? This will certainly harm the tax system (less taxes paid) but it will help society because she will dedicate more time to the child (And if you acknowledge this, then you are implicitly arguing that children are indeed a public good). But as one of Esping-Andersen’s chapter from his book the Incomplete Revolution shows, permitting the woman to go back to work after parental leave, by providing free high quality childcare, will in the long run pay the costs of this childcare service and even produce a surplus for the welfare state. And that is why I think this is not an easy question to answer, conversely to what Paula England and Nancy Folbre imply. The implication of implementing this idea entails a complete reorganization and readjustment of the tax and welfare system, if there is one [2]. And also, what would be the consequences of this new regime besides the public childcare? Will the problem of inequalities be solved or others problems will arise as side effects?&lt;/p&gt;
&lt;p&gt;And here I will talk about something that is intrinsically related to having healthy children: the labor perspectives of the mother. The Incomplete Revolution, book by Esping-Andersen, takes on the job of explaining the problem with many of today’s countries: the poor family policies that they implement (if any) when concerning women’s labor market inclusion and combination of working and childrearing. Specifically in two chapters he argues that children should be a public good because children benefit everyone, but furthermore, an important step is to include women in the labor market given that it will help the income prospects of the household and the egalitarianism in the family. How can this be related to children as being a public good, one may ask? This is a complex relation, on which everything is potentially endogenous, but all present evidence shows that if the mother goes back to work after the parental leave, it does not have an adverse effect on the development of the child. This is, assuming that the child is in early high quality childcare. This has important implications! It means that the inclusion of mothers into the labor force will increase the bargaining power of the women in the family, it will heighten the income of the family household, it will expand the human capital of the women; in short, it will not impose the opportunity costs that she will otherwise would’ve had to bear for the remaining part of her life if she would’ve stayed home. This will increase the chances of being in a more egalitarian family, thus the children will be exposed to more equal surroundings.&lt;/p&gt;
&lt;p&gt;From the point of view that I see it, it’s about heightening the bar of educational attainment. Instead of having the two extremes that we see in the liberal welfare state regimes (excluding UK), extremely poor and uneducated groups and highly rich and educated group, you will set a bar of minimum education. By doing this you will inevitable give secureness to families, opportunities, possibly change the economic structure of the country by increasing the odds of turning it to a knowledge based economy (although this is way more complex then I suggest).&lt;/p&gt;
&lt;p&gt;As a conclusion, I shall finish off with another question, which I think is as important as the one which initiated this essay. I have tried to argue that children are faring under the destiny that is imposed to them. There has been, since after the World War II, persistent inequalities between classes and all the evidence shows that they are not shrinking. In some countries, namely European ones, the decision to help children might be an economic one or an altruistic one. This is worrisome. Are children being taken care of in Europe because they are the payers of the pensioners or because of the noble argument of equal opportunities? I really don’t know the reason for which Europe or the U.S is doing it for. But should everyone take care of children for the economic benefits or for the equal opportunity argument?&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;[1] Let’s not get into the major changes that the quality of job production will have if the population is highly educated. For a book with interesting ideas, read The Race between Education and Technology by Claudia Goldin &amp;amp; Lawrence Katz.&lt;/p&gt;
&lt;p&gt;[2] In the case where there is no welfare state, then this is another story.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Producing stargazer tables with odds ratios and standard errors in R</title>
      <link>/blog/2016-08-22-producing-stargazer-tables-with-odds-ratios-and-standard-errors-in-r/producing-stargazer-tables-with-odds-ratios-and-standard-errors-in-r/</link>
      <pubDate>Mon, 22 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-08-22-producing-stargazer-tables-with-odds-ratios-and-standard-errors-in-r/producing-stargazer-tables-with-odds-ratios-and-standard-errors-in-r/</guid>
      <description>&lt;p&gt;Whoa, what a day. I’ve been using the stargazer package for producing my (beautiful) regression tables in R for a while now. Among all the arguments of its main function (&lt;code&gt;stargazer()&lt;/code&gt; ) are &lt;code&gt;apply.coef&lt;/code&gt;, &lt;code&gt;apply.se&lt;/code&gt;, &lt;code&gt;apply.ci&lt;/code&gt;, … and so on for all the other statistics of a regression output. Each of these arguments, if specified, applies a function over the specified statistic. So, for calculating the odds ratios I would simply apply the &lt;code&gt;exp()&lt;/code&gt; function over the set of log odds. It turns out that if you apply any function over the coefficients (or any other statistic), stargazer automatically recalculates t values with the new coefficients! This means that the significance of my model will depend on the new values and we surely wouldn’t want that.&lt;/p&gt;
&lt;p&gt;Let’s show a reproducible example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;stargazer&amp;quot;) # in case you don&amp;#39;t have this package
suppressMessages(library(stargazer))

m1 &amp;lt;- glm(mtcars$vs ~ mtcars$hp + mtcars$mpg)

stargazer(m1, type = &amp;quot;text&amp;quot;) # Our standard log odds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## =============================================
##                       Dependent variable:    
##                   ---------------------------
##                               vs             
## ---------------------------------------------
## hp                         -0.004**          
##                             (0.001)          
##                                              
## mpg                          0.022           
##                             (0.017)          
##                                              
## Constant                     0.566           
##                             (0.519)          
##                                              
## ---------------------------------------------
## Observations                  32             
## Log Likelihood              -11.217          
## Akaike Inf. Crit.           28.434           
## =============================================
## Note:             *p&amp;lt;0.1; **p&amp;lt;0.05; ***p&amp;lt;0.01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stargazer(m1, apply.coef = exp, type = &amp;quot;text&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## =============================================
##                       Dependent variable:    
##                   ---------------------------
##                               vs             
## ---------------------------------------------
## hp                         0.996***          
##                             (0.001)          
##                                              
## mpg                        1.022***          
##                             (0.017)          
##                                              
## Constant                   1.762***          
##                             (0.519)          
##                                              
## ---------------------------------------------
## Observations                  32             
## Log Likelihood              -11.217          
## Akaike Inf. Crit.           28.434           
## =============================================
## Note:             *p&amp;lt;0.1; **p&amp;lt;0.05; ***p&amp;lt;0.01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficients are correct, but look at the significance levels! Those are some really undesirable results. I was actually using this for quite some time without noticing. In light of this problem I decided to create a small function that extracted the statistics separately and applied the appropriate conversion when needed. It’s far from being a flexible function, but it can surely help you run some quick-and-dirty logistic regressions with odds ratios instead of log odds.&lt;/p&gt;
&lt;p&gt;Here’s the function and an example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stargazer2 &amp;lt;- function(model, odd.ratio = F, ...) {
  if(!(&amp;quot;list&amp;quot; %in% class(model))) model &amp;lt;- list(model)
    
  if (odd.ratio) {
    coefOR2 &amp;lt;- lapply(model, function(x) exp(coef(x)))
    seOR2 &amp;lt;- lapply(model, function(x) exp(coef(x)) * summary(x)$coef[, 2])
    p2 &amp;lt;- lapply(model, function(x) summary(x)$coefficients[, 4])
    stargazer(model, coef = coefOR2, se = seOR2, p = p2, ...)
    
  } else {
    stargazer(model, ...)
  }
}

stargazer(m1, type = &amp;quot;text&amp;quot;) # Our standard log odds&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## =============================================
##                       Dependent variable:    
##                   ---------------------------
##                               vs             
## ---------------------------------------------
## hp                         -0.004**          
##                             (0.001)          
##                                              
## mpg                          0.022           
##                             (0.017)          
##                                              
## Constant                     0.566           
##                             (0.519)          
##                                              
## ---------------------------------------------
## Observations                  32             
## Log Likelihood              -11.217          
## Akaike Inf. Crit.           28.434           
## =============================================
## Note:             *p&amp;lt;0.1; **p&amp;lt;0.05; ***p&amp;lt;0.01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stargazer2(m1, odd.ratio = T, type = &amp;quot;text&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## =============================================
##                       Dependent variable:    
##                   ---------------------------
##                               vs             
## ---------------------------------------------
## hp                          0.996**          
##                             (0.001)          
##                                              
## mpg                          1.022           
##                             (0.017)          
##                                              
## Constant                     1.762           
##                             (0.915)          
##                                              
## ---------------------------------------------
## Observations                  32             
## Log Likelihood              -11.217          
## Akaike Inf. Crit.           28.434           
## =============================================
## Note:             *p&amp;lt;0.1; **p&amp;lt;0.05; ***p&amp;lt;0.01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Now the coefficients and significance is correct!&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# You can also use lists
m1 &amp;lt;- glm(mtcars$vs ~ mtcars$mpg)
m2 &amp;lt;- glm(mtcars$vs ~ mtcars$mpg + mtcars$hp)
m3 &amp;lt;- glm(mtcars$vs ~ mtcars$mpg + mtcars$hp + mtcars$am)

models &amp;lt;- list(m1, m2, m3)

stargazer(models, type = &amp;quot;text&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ===============================================
##                        Dependent variable:     
##                   -----------------------------
##                                vs              
##                      (1)        (2)      (3)   
## -----------------------------------------------
## mpg                0.056***    0.022    0.041* 
##                    (0.011)    (0.017)  (0.022) 
##                                                
## hp                           -0.004**  -0.003* 
##                               (0.001)  (0.002) 
##                                                
## am                                      -0.223 
##                                        (0.173) 
##                                                
## Constant          -0.678***    0.566    0.141  
##                    (0.239)    (0.519)  (0.611) 
##                                                
## -----------------------------------------------
## Observations          32        32        32   
## Log Likelihood     -14.669    -11.217  -10.299 
## Akaike Inf. Crit.   33.338    28.434    28.599 
## ===============================================
## Note:               *p&amp;lt;0.1; **p&amp;lt;0.05; ***p&amp;lt;0.01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stargazer2(models, odd.ratio = T, type = &amp;quot;text&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ===============================================
##                        Dependent variable:     
##                   -----------------------------
##                                vs              
##                      (1)        (2)      (3)   
## -----------------------------------------------
## mpg                1.057***    1.022    1.042* 
##                    (0.012)    (0.017)  (0.023) 
##                                                
## hp                            0.996**   0.997* 
##                               (0.001)  (0.002) 
##                                                
## am                                      0.800  
##                                        (0.139) 
##                                                
## Constant           0.508***    1.762    1.151  
##                    (0.121)    (0.915)  (0.703) 
##                                                
## -----------------------------------------------
## Observations          32        32        32   
## Log Likelihood     -14.669    -11.217  -10.299 
## Akaike Inf. Crit.   33.338    28.434    28.599 
## ===============================================
## Note:               *p&amp;lt;0.1; **p&amp;lt;0.05; ***p&amp;lt;0.01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Same significance but different coefficients and SE&amp;#39;s&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Caveats:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;It only accepts one model or one list containing several models. I did this because I didn’t want to get into distinguishing between several separate models. If you want to improve it, &lt;a href=&#34;https://github.com/cimentadaj/cimentadaj/blob/master/R/stargazer2.R&#34;&gt;here’s&lt;/a&gt; the Github website, submit a pull request!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It doesn’t calculate confidence intervals as the formula is more complicated and I didn’t need them for now.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;Update: I included this function in my personal package which you can install like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;devtools&amp;quot;)
devtools::install_github(&amp;quot;cimentadaj/cimentadaj&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>T-tests, regression (and ANOVA): They&#39;re all the same!</title>
      <link>/blog/2016-08-20-ttests-regression-and-anova-theyre-all-the-same/ttests-regression-and-anova-theyre-all-the-same/</link>
      <pubDate>Sat, 20 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/blog/2016-08-20-ttests-regression-and-anova-theyre-all-the-same/ttests-regression-and-anova-theyre-all-the-same/</guid>
      <description>&lt;p&gt;Upon reading “How Not To Lie with Statistics: Avoiding Common Mistakes in Quantitative Political Science” from Gary King, I stumbled into a section which proved that t-tests, ANOVA and Linear Regression are intimately related, both conceptually and algebraically. As a late-comer in statistics, one usually does not pay attention to these nuances. I decided to make a short simulation in R just to make sure my intuition was right.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)

income &amp;lt;- sample(1000:5000, 100, replace = T)
gender &amp;lt;- rep(c(1, 0), 50)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t &amp;lt;- t.test(income ~ gender)
unname(t$estimate[2] - t$estimate[1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 201.46&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(model &amp;lt;- lm(income ~ gender))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)      gender 
##     2970.64      201.46&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The ANOVA model is actually computed through the lm
# call but we can use the anova() function to check if
# the differences are significant as well.

anova(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Analysis of Variance Table
## 
## Response: income
##           Df    Sum Sq Mean Sq F value Pr(&amp;gt;F)
## gender     1   1014653 1014653  0.8842 0.3494
## Residuals 98 112463658 1147588&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s fun to find out about these things and prove that they make sense. However, the strength of linear models is that you can ‘adjust’ for other important variables and get an adjusted estimated difference. Let’s add another variable called kids with the number of children per person and see how the difference changes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kids &amp;lt;- sample(1:4, 100, replace = T)
lm(income ~ gender + kids)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = income ~ gender + kids)
## 
## Coefficients:
## (Intercept)       gender         kids  
##    2945.911      200.471        9.891&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, we can now say that the difference in income between male and females is of about 200 adjusted for the number of children per person.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>&lt;p&gt;&lt;center&gt;&lt;img src=&#34;/img/headshot.jpg&#34; alt=&#34;Drawing&#34; style=&#34;width: 200px;&#34;/&gt;&lt;/center&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Hi, I’m Jorge Cimentada and I’m a PhD candidate in Sociology at Pompeu Fabra University in Barcelona, Spain. I belong to the &lt;a href=&#34;https://www.upf.edu/web/survey&#34;&gt;Research and Expertise Centre for Survey Methodology (RECSM)&lt;/a&gt; and my main research interests are the study of achievement inequality, the role of schools in reproducing these inequalities and early education as a remedy to achievement inequalities. I’m currently working at the &lt;a href=&#34;https://www.upf.edu/web/survey&#34;&gt;Research and Expertise Centre for Survey Methodology (RECSM)&lt;/a&gt; as an R developer and Data Scientists. Previously I worked on a project related to class mobility and how it has changed over time in industrialized countries in the &lt;a href=&#34;https://www.upf.edu/web/demosoc&#34;&gt;DEMOSOC&lt;/a&gt; research group.&lt;/p&gt;

&lt;p&gt;I am very passionate about statistical programming with R , data visualization, statistics and drums. &lt;strong&gt;I&amp;rsquo;ve started making a transition from academia to the private sector to work as a Data Scientist&lt;/strong&gt;. If you feel that I&amp;rsquo;d be a good match for your company, let&amp;rsquo;s talk!&lt;/p&gt;

&lt;p&gt;If you wish to contact me send me an email to &lt;code&gt;cimentadaj@gmail.com&lt;/code&gt;. I’m also on &lt;a href=&#34;https://github.com/cimentadaj&#34;&gt;Github&lt;/a&gt; and &lt;a href=&#34;https://www.linkedin.com/in/jorge-cimentada-1740877a/&#34;&gt;Linkedin&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Curriculum Vitae</title>
      <link>/vitae/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/vitae/</guid>
      <description>&lt;p&gt;You can download my academic CV from &lt;a href=&#34;/vitae/academia_CV.pdf&#34;&gt;here&lt;/a&gt; or my industry CV from &lt;a href=&#34;/vitae/industry_CV.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/projects/</guid>
      <description>&lt;p&gt;&lt;p&gt;
&lt;div style=&#34;width: auto&#34;&gt;
&lt;img src=&#34;img/teaching_r.jpg&#34; alt=&#34;Me.&#34; style=&#34;float: center; width: 100%;max-height: 130%; PADDING-LEFT: 30px&#34;/&gt;
&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;center&gt;&lt;i&gt; Teaching R! &lt;/i&gt; &lt;/center&gt;
&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;p&gt;In this section you can find bits of material I&amp;rsquo;ve written over time and some of my personal projects, mainly concentrated on the R programming software&lt;p&gt;&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Data analysis &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;blog/2018-02-12-what-time-should-i-ride-my-bike/2018-02-12-what-time-should-i-ride-my-bike/index.html&#34;&gt;How long should I wait for my bike?&lt;/a&gt;: This is a series of posts (currently active) in which I set up a private server and interact with the Barcelona public bicycle system API. I&amp;rsquo;ve performed analysis of waiting times and I plan to add a live predictive model of waiting time for stations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;blog/2017-10-16-scraping-and-visualizing-how-i-met-your-mother/scraping-and-visualizing-how-i-met-your-mother/index.html&#34;&gt;Analyzing &amp;ldquo;How I Met Your Mother&amp;rdquo;&lt;/a&gt;: In this blog post I scraped and analyzed the scripts of the American TV series &amp;ldquo;How I Met Your Mother&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; R packages &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;perccalc (&lt;a href=&#34;https://cran.r-project.org/web/packages/perccalc/index.html&#34;&gt;CRAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/cimentadaj/perccalc&#34;&gt;Github&lt;/a&gt;): An implementation of two functions that estimate values for percentiles from an ordered categorical variable as described by Reardon (2011, isbn:978-0-87154-372-1).&lt;/p&gt;

&lt;p&gt;essurvey (&lt;a href=&#34;https://cran.r-project.org/web/packages/essurvey/index.html&#34;&gt;CRAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/ropensci/essurvey&#34;&gt;Github&lt;/a&gt;, &lt;a href=&#34;https://ropensci.github.io/essurvey/&#34;&gt;Pkg website&lt;/a&gt;): Download data from the European Social Survey directly into R from their website &lt;a href=&#34;http://www.europeansocialsurvey.org/&#34;&gt;http://www.europeansocialsurvey.org/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;cimentadaj (&lt;a href=&#34;https://github.com/cimentadaj/cimentadaj&#34;&gt;Github&lt;/a&gt;): My personal package&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Miscellaneous &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/index.html&#34;&gt;My PISA twitter bot&lt;/a&gt;: I programmed a Twitter bot that tweets graphs from the &lt;a href=&#34;http://www.oecd.org/pisa/&#34;&gt;PISA&lt;/a&gt; database. How did I do it? See &lt;a href=&#34;blog/2017-03-08-my-pisa-twitter-bot/my-pisa-twitter-bot/index.html&#34;&gt;here&lt;/a&gt; and follow &lt;a href=&#34;https://twitter.com/DailyPISA_Facts&#34;&gt;@DailyPISA_Facts&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cimentadaj.shinyapps.io/shiny/&#34;&gt;Educational Inequality around the world&lt;/a&gt;: Visualize educational inequality for over 70 countries from 2000 to 2015 using this Shinyapp.&lt;/p&gt;

&lt;p&gt;I completed all solutions to the &lt;a href=&#34;http://r4ds.had.co.nz/&#34;&gt;R for Data Science book&lt;/a&gt;. Solutions can be found &lt;a href=&#34;https://github.com/cimentadaj/R4DS-Solutions&#34;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I translated some Stata code to R for the &lt;a href = &#34;https://www.povertyactionlab.org/&#34; target = &#34;_blank&#34;&gt; The Abdul Latif Jameel Poverty Action Lab (J-PAL) &lt;/a&gt; and was featured on their &lt;a href = &#34;https://www.povertyactionlab.org/research-resources/software-and-tools&#34; target = &#34;_blank&#34;&gt; website&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Teaching R &lt;/b&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In April 2016, together with &lt;a href = &#34;https://sites.google.com/site/brunoarpino/&#34; target = &#34;_blank&#34;&gt;Bruno Arpino&lt;/a&gt; and some colleagues from the &lt;a href = &#34;https://www.upf.edu/survey/&#34; target = &#34;_blank&#34;&gt; RECSM&lt;/a&gt;, I organized a seminar series introducing  R at Pompeu Fabra University. As part of the lectures I programmed some interactive lessons which can help get you started. These lessons assume that you have never used R and are self-standing lessons. Go to the &lt;a href = &#34;https://github.com/cimentadaj/Rseminars&#34; target = &#34;_blank&#34;&gt; Github&lt;/a&gt; repository: follow the instructions and you&amp;rsquo;ll be able to install everything and follow the courses.  &lt;i&gt;Note: Ignore the first lesson since it was a live presentation.&lt;/i&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I gave a short course at Pompeu Fabra University introducing the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt;. I uploaded the syllabus online (program, slides and exercises) in my &lt;a href=&#34;https://github.com/cimentadaj/tidyverse_seminars&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;I gave a talk at &lt;a href=&#34;https://pydata.org/barcelona2017/&#34;&gt;PyData 2017&lt;/a&gt; to introduce the tidyverse to follow data analysts. If you&amp;rsquo;re interested, I uploaded the material in my &lt;a href=&#34;https://github.com/cimentadaj/PyData_2017&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
