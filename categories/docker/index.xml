<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>docker on Jorge Cimentada</title>
    <link>https://cimentadaj.github.io/categories/docker/</link>
    <description>Recent content in docker on Jorge Cimentada</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 11 Jan 2021 00:00:00 +0000</lastBuildDate>
    
    <atom:link href="https://cimentadaj.github.io/categories/docker/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Talking between containers in docker-compose</title>
      <link>https://cimentadaj.github.io/blog/2021-01-11-talking-between-containers-in-dockercompose/talking-between-containers-in-docker-compose/</link>
      <pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2021-01-11-talking-between-containers-in-dockercompose/talking-between-containers-in-docker-compose/</guid>
      <description><![CDATA[
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>When using <code>docker-compose</code>, we can define several containers which are connected <a href="blog/2020-12-07-lightweight-airflow-deployment-with-docker/lightweight-airflow-deployment-with-docker/index.html">through a network</a>. How can we speak between containers? More specifically, suppose one container has a MySQL database and the other container has some R/Python/Julia container in which you want to query/append data. Usually we would only refer to the IP of the database when defining the connection but all of these containers sort of share the same IP, right?</p>
<p>I tried this and it could not connect to the localhost because it couldn’t find the database. After browsing a bit, it was easier that I thought. Here’s a <code>docker-compose.yml</code> with a MySQL image:</p>
<pre><code>version: &#39;2.1&#39;
services:
  # Database to append data to
  db:
    hostname: sql_db
    image: mysql:8.0
    restart: always
    ports:
      - 3306:3306
    environment:
      - MYSQL_ROOT_PASSWORD=123456
    volumes:
      - ./sql:/docker-entrypoint-initdb.d
      - ./data:/var/lib/mysql
      - ./sql_conf:/etc/mysql/conf.d</code></pre>
<p>Notice that there’s a <code>hostname</code> argument? That is the IP we’ll use. Assume that the entrypoint for Docker defines a database called <code>test</code> with a table named <code>test_table</code>. We make up some username/password just for this example. In Python parlance, for example, we could define the connection like this and query the contents:</p>
<pre class="python"><code># Make sure you have the package mysql and mysql-connector from pip as well
import pandas as pd
import sqlalchemy

db_username=&#39;test_user&#39;
db_password=&#39;123&#39;

# Here is the name of the container
db_ip=&#39;sql_db&#39;
db_name=&#39;test&#39;
db_port=3306
db_connection=sqlalchemy.create_engine(&#39;mysql+mysqlconnector://{0}:{1}@{2}:{3}/{4}?auth_plugin=mysql_native_password&#39;.format(db_username, db_password, db_ip, db_port, db_name))
df=pd.read_sql_query(&quot;SELECT * FROM test.test_table&quot;, db_connection)</code></pre>
<p>Note that this works when the code above is executed from <strong>another</strong> container inside the network (and not locally). <a href="blog/2020-12-07-lightweight-airflow-deployment-with-docker/lightweight-airflow-deployment-with-docker/index.html">Although this looks like a simple trick it opens a whole new world for talking between containers in a stable manner</a>.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Lightweight Airflow Deployment with Docker in 5 steps</title>
      <link>https://cimentadaj.github.io/blog/2020-12-07-lightweight-airflow-deployment-with-docker/lightweight-airflow-deployment-with-docker/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2020-12-07-lightweight-airflow-deployment-with-docker/lightweight-airflow-deployment-with-docker/</guid>
      <description><![CDATA[
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>There’s a bunch of tutorials out there on how to deploy Airflow for scaling tasks across clusters. This is another one of those tutorials. However, I’m interested in doing the above without much hassle, meaning that I don’t want to spend 2 hours installing Airflow, dependencies, PostgreSQL, and so on. This tutorial extends most internet tutorials on Airflow to deploy a full-fledged example using Docker in the least amount of code possible.</p>
<p>For the sake of not repeating content on the internet, I’m gonna limit my definition of Airflow to this: it’s a way of repeating a program in a given interval of time, like within a specified schedule. That sounds oddly similar to <a href="https://en.wikipedia.org/wiki/Cron">cron</a>. It is in fact a cron-like system but it has other stuff like automatic email warning when your task fails, detailed logs, scaling thousands of tasks within a cluster and an infrastructure for creating dependencies between tasks. Interested? See <a href="https://airflow.apache.org/docs/apache-airflow/stable/_images/graph.png">this</a>.</p>
<p>Enough of definitions. Airflow is not particularly easy to install so let’s get to the point. At the end of this tutorial we’ll have a very simple python program that runs every minute of every single day setup on Airflow with email warning whenever this program fails. That, without having to install anything other than Docker on your local computer.</p>
<p>If you don’t have docker and docker-compose installed, follow <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-20-04">this</a> tutorial for docker and <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-20-04">this one</a> for docker-compose for installing both.</p>
<div id="summarized-version" class="section level1">
<h1>Summarized version</h1>
<p>If you’re here for the code, here’s the summarized versions. Skip down for detailed explanations. <strong>Make sure you replace everything that is signaled that needs replacing</strong>.</p>
<pre class="bash"><code># Replace this with the directory that you want
# to use to save your Airflow directory.
TEST_DIR=~/Downloads/

# Create the directory structure for Airflow
mkdir -p $TEST_DIR/airflow_test2
cd $TEST_DIR/airflow_test2
mkdir dags
mkdir scripts
mkdir airflow-logs

# If an &#39;event not found&#39; error arises, see https://stackoverflow.com/questions/42798737/bash-usr-bin-env-event-not-found
echo -e &#39;#!/usr/bin/env bash
airflow upgradedb
airflow webserver
&#39; &gt; scripts/airflow-entrypoint.sh

# Save environmental variables needed by Airflow in a separate
# .env file.
echo &quot;AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgres+psycopg2://airflow:airflow@postgres:5432/airflow
AIRFLOW_CONN_METADATA_DB=postgres+psycopg2://airflow:airflow@postgres:5432/airflow
AIRFLOW_VAR__METADATA_DB_SCHEMA=airflow
AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=10
&quot; &gt; .env

# Create a docker-compose file with the container
# specification for the database, the scheduler
# and the webserver for docker.
echo &quot;version: &#39;2.1&#39;
services:
  postgres:
    image: postgres:12
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - &#39;5433:5432&#39;

  scheduler:
    image: apache/airflow
    restart: always
    depends_on:
      - postgres
      - webserver
    env_file:
      - .env
    ports:
      - &#39;8793:8793&#39;
    volumes:
      - ./dags:/opt/airflow/dags
      - ./airflow-logs:/opt/airflow/logs
    command: scheduler
    healthcheck:
      test: [&#39;CMD-SHELL&#39;, &#39;[ -f /usr/local/airflow/airflow-webserver.pid ]&#39;]
      interval: 30s
      timeout: 30s
      retries: 3

  webserver:
    image: apache/airflow
    hostname: webserver
    restart: always
    depends_on:
      - postgres
    env_file:
      - .env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./airflow-logs:/opt/airflow/logs
    ports:
      - &#39;8080:8080&#39;
    entrypoint: ./scripts/airflow-entrypoint.sh
    healthcheck:
      test: [&#39;CMD-SHELL&#39;, &#39;[ -f /usr/local/airflow/airflow-webserver.pid ]&#39;]
      interval: 30s
      timeout: 30s
      retries: 32
&quot; &gt; docker-compose.yml

# Create the task that will be ran within a schedule

# Replace `start_date` with the starting date of your process as well as the
# owner and email. Moreover, you can change `schedule_interval` for specifying
# the schedule for when you want your ask to run. For customizing additional
# arguments, see https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html
echo &quot;from datetime import timedelta, datetime
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.dummy_operator import DummyOperator

# Define default arguments for the DAG
default_args = {
    &#39;owner&#39;: &#39;cimentadaj&#39;,
    &#39;depends_on_past&#39;: False,
    &#39;start_date&#39;: datetime(2020, 12, 7, 13, 20),
    &#39;email&#39;: [&#39;airflow@example.com&#39;],
    &#39;email_on_failure&#39;: True,
    &#39;retries&#39;: 1,
    &#39;retry_delay&#39;: timedelta(seconds=5)
}

# Define the main DAG: this is a general
# process that can have many tasks inside
dag = DAG(
    &#39;tutorial&#39;,
    default_args=default_args,
    schedule_interval=&#39;*/1 * * * *&#39;,
)

# Define what task_1 will do
def task_1():
    &#39;Print task 1 complete&#39;
    print(&#39;Task 1 complete&#39;)

# Define what task_2 will do
def task_2():
    &#39;Print task 2 complete&#39;
    print(&#39;Task 2 complete&#39;)

# Create initial empty task
task0 = DummyOperator(
    task_id=&#39;start&#39;,
    dag=dag
)

# Create task1
task1 = PythonOperator(
    task_id=&#39;task_1&#39;,
    python_callable=task_1,
    dag=dag
)

# Create task2
task2 = PythonOperator(
    task_id=&#39;task_2&#39;,
    python_callable=task_2,
    dag=dag
)

# Create dependencies between tasks.
# task0 is first, then task1 then task2
task0 &gt;&gt; task1 &gt;&gt; task2
&quot; &gt; dags/airflow-test.py

# Airflow can have some issues with permissions, so we set all
# possible permissions available for our Airflow folder.
sudo chmod -R 777 $TEST_DIR/airflow_test2

# Deploy everything
docker-compose up -d</code></pre>
<pre><code># Creating network &quot;airflow_test2_default&quot; with the default driver
# Starting airflow_test2_postgres_1 ... 
# Starting airflow_test2_webserver_1 ... 
# Starting airflow_test2_scheduler_1 ... </code></pre>
<p>Visit <a href="http://localhost:8080/admin/" class="uri">http://localhost:8080/admin/</a> and you should see something like this:</p>
<p><img src="/img/airflow.png" alt="drawing" width="120%"/></p>
<p>We can toggle our tasks to begin, inspect the dependency of the different tasks and also begin the task process manually. Below is a GIF showing you how to do it. <strong>Make sure you click on the image for it to start</strong>:</p>
<p><img src="/img/airflow_workflow.gif" alt="drawing" width="120%"/></p>
<p>Next to the dag ‘Tutorial’ (on the left) there is an Off/On slider. Swipe the slider to be ‘On’ and this task will run according to the cron expression in ‘Schedule’ (on the right of ‘Tutorial’) on the starting date that we defined in the Python script <code>dags/airflow-test.py</code> (just search for <code>start_date</code> and you’ll find it).</p>
</div>
<div id="detailed-version" class="section level1">
<h1>Detailed version</h1>
<ol style="list-style-type: decimal">
<li>Create the folder structure for saving our Airflow files/logs/scripts</li>
</ol>
<pre class="bash"><code># Replace this with the directory that you want
# to use save your Airflow directory.
TEST_DIR=~/Downloads/
mkdir -p $TEST_DIR/airflow_test2
cd $TEST_DIR/airflow_test2
mkdir dags
mkdir scripts
mkdir airflow-logs</code></pre>
<p>After creating the main folder, we create three folders: <code>dags</code>, <code>scripts</code> and <code>airflow-logs</code>. Later on you’ll see why we need these.</p>
<ol start="2" style="list-style-type: decimal">
<li>Create an entry point for the Airflow webserver.</li>
</ol>
<pre class="bash"><code># If an &#39;event not found&#39; error arises, see https://stackoverflow.com/questions/42798737/bash-usr-bin-env-event-not-found
echo -e &#39;#!/usr/bin/env bash
airflow upgradedb
airflow webserver
&#39; &gt; scripts/airflow-entrypoint.sh</code></pre>
<p>The Airflow webserver is essentially the front-end of Airflow. It’s the website that you see that allows you to toggle on/off different processes, look at their logs and their schedule. In the bash script <code>scripts/airflow-entrypoint.sh</code> we need to ‘initialize’ that webserver. This literally just means: deploy the front-end. <code>airflow upgradedb</code> makes sure all tables related to the database hosting the logs, dags and so on is complete (no missing tables, for example) and <code>airflow webserver</code> deploys the website that we’ll use to control Airflow.</p>
<ol start="3" style="list-style-type: decimal">
<li>Save all environmental variables to a file called <code>.env</code></li>
</ol>
<pre class="bash"><code>echo &quot;AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgres+psycopg2://airflow:airflow@postgres:5432/airflow
AIRFLOW_CONN_METADATA_DB=postgres+psycopg2://airflow:airflow@postgres:5432/airflow
AIRFLOW_VAR__METADATA_DB_SCHEMA=airflow
AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=10
&quot; &gt; .env</code></pre>
<p>These are some of the core environmental variables that Airflow uses. For example, <code>AIRFLOW__CORE__SQL_ALCHEMY_CONN</code> specifies the direct connection to the back-end database hosting all the logs/dags of your Airflow instance. All of this is set with default values that can work well with this toy problem. If you’re planning to do some serious Airflow deployment then you should <a href="https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html">learn more</a> about what these variables do for better customization. For example, in <code>AIRFLOW__CORE__SQL_ALCHEMY_CONN</code> we’re directly storing the user/password to access the database. <strong>This is not safe</strong> and warrants further customization.</p>
<ol start="4" style="list-style-type: decimal">
<li>Write a <code>docker-compose</code> file that deploys the back-end database, the airflow scheduler and the webserver.</li>
</ol>
<pre class="bash"><code>echo &quot;version: &#39;2.1&#39;
services:
  postgres:
    image: postgres:12
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - &#39;5433:5432&#39;

  scheduler:
    image: apache/airflow
    restart: always
    depends_on:
      - postgres
      - webserver
    env_file:
      - .env
    ports:
      - &#39;8793:8793&#39;
    volumes:
      - ./dags:/opt/airflow/dags
      - ./airflow-logs:/opt/airflow/logs
    command: scheduler
    healthcheck:
      test: [&#39;CMD-SHELL&#39;, &#39;[ -f /usr/local/airflow/airflow-webserver.pid ]&#39;]
      interval: 30s
      timeout: 30s
      retries: 3

  webserver:
    image: apache/airflow
    hostname: webserver
    restart: always
    depends_on:
      - postgres
    env_file:
      - .env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./airflow-logs:/opt/airflow/logs
    ports:
      - &#39;8080:8080&#39;
    entrypoint: ./scripts/airflow-entrypoint.sh
    healthcheck:
      test: [&#39;CMD-SHELL&#39;, &#39;[ -f /usr/local/airflow/airflow-webserver.pid ]&#39;]
      interval: 30s
      timeout: 30s
      retries: 32
&quot; &gt; docker-compose.yml</code></pre>
<p>I don’t want to go very in depth about how Airflow works because there’s just too many tutorials out there. Just search for ‘airflow tutorial’ on Google and you’ll find a bunch. I’ll limit myself to very simple explanations of how Airflow works.</p>
<p>Airflow has three components: a database, a scheduler and a webserver. The database hosts all information that is used by the scheduler and webserver such as how many tasks you have created, whether certain tasks failed on a given day/time and so on. It’s just that: a database for hosting all information needed by the other processes.</p>
<p>On the other hand, the scheduler makes sure that your tasks run on the start date and schedule that you’ve specified. This is the ‘cron’ part of Airflow. It speaks directly to the database we defined above to make sure we’re up to date on running everything. It won’t work if it doesn’t find the database we defined above because it needs to figure out whether certain tasks are behind schedule.</p>
<p>Finally, the webserver is a process for visualizing all of this in an intuitive way. Among other things, it creates maps of how your tasks are connected and shows the logs stored in the database in a convenient way.</p>
<p><code>docker-compose.yml</code> hosts these three processes and allows them to speak to each other. I’ll focus on the important options from this <code>docker-compose</code> file.</p>
<p>The first service uses a PostgreSQL image tagged at version 12 and specifies the port at which will be used together with some environmental variables used to access the database:</p>
<pre><code>postgres:
  image: postgres:12
  environment:
    - POSTGRES_USER=airflow
    - POSTGRES_PASSWORD=airflow
    - POSTGRES_DB=airflow
  ports:
    - &#39;5433:5432&#39;</code></pre>
<p>This means that we don’t have to download the database engine, this image will host it directly. How can we be sure that Airflow speaks to <em>this</em> database? We haven’t linked them or anything, right? Well, think again! <code>docker-compose</code> creates a network between the docker images that we specify. Notice how we’re specifying the ports <code>5433:5432</code>? We also specified those in <code>AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgres+psycopg2://airflow:airflow@postgres:5432/airflow</code>. We’re slowly connecting the different parts we’ve been defining.</p>
<p>The next part of the <code>docker-compose</code> specifies the scheduler.</p>
<pre><code>scheduler:
  image: apache/airflow
  restart: always
  depends_on:
    - postgres
    - webserver
  env_file:
    - .env
  ports:
    - &#39;8793:8793&#39;
  volumes:
    - ./dags:/opt/airflow/dags
    - ./airflow-logs:/opt/airflow/logs
  command: scheduler
  healthcheck:
    test: [&#39;CMD-SHELL&#39;, &#39;[ -f /usr/local/airflow/airflow-webserver.pid ]&#39;]
    interval: 30s
    timeout: 30s
    retries: 3</code></pre>
<p>The image we use for the scheduler comes from the official Airflow docker image (<code>apache/airflow</code>). Remember that this scheduler will not work if it’s not connected to the database? We specify that in <code>depends_on</code>, saying that this image has to wait until these other containers are ready and deployed. Remember the <code>.env</code> file hosting the PostgreSQL and other environmental variables? We also specify it here because the Airflow image needs these to correctly know where to find things such as the PostgreSQL connection. As I said, I’ll only focus on the most important parts.</p>
<p>Volumes is perhaps the most important thing to explain here. Volumes are links between your local computer and the virtual docker container. This means that if I link my local folder <code>./dags/</code> to the container’s folder <code>/opt/airflow/dags</code> everything that is inside <code>./dags/</code> will be inside <code>/opt/airflow/dags</code> and viceversa. It essentially creates a mirror between the folders on your local computer and your container.</p>
<p>In this image we’re saying: anything that we put inside the folder <code>dags</code> needs to be placed in the container’s folder for dags. This is <strong>key</strong> because this is where you define your tasks that will be ran on a given schedule. We do the same with the logs because we might want to see the logs later on in our local computer.</p>
<p>The final part of <code>docker-compose</code> has the webserver.</p>
<pre><code>webserver:
  image: apache/airflow
  hostname: webserver
  restart: always
  depends_on:
    - postgres
  env_file:
    - .env
  volumes:
    - ./dags:/opt/airflow/dags
    - ./scripts:/opt/airflow/scripts
    - ./airflow-logs:/opt/airflow/logs
  ports:
    - &#39;8080:8080&#39;
  entrypoint: ./scripts/airflow-entrypoint.sh
  healthcheck:
    test: [&#39;CMD-SHELL&#39;, &#39;[ -f /usr/local/airflow/airflow-webserver.pid ]&#39;]
    interval: 30s
    timeout: 30s
    retries: 32</code></pre>
<p>This container shares a lot of similarity to the schedule container (same image, depends on the PostgreSQL container, passing the <code>.env</code> file) but there’s one thing that we should stop to explain: the entry point. Can you see that we’re linking our <code>scripts</code> folder to the a scripts folder within the container as a volume? This is because we will run the file <code>airflow-entrypoint.sh</code> once this container has been deployed. That is, docker already interprets entry points as scripts that it needs to run once it has been deployed; we just have to specify which script.</p>
<p>If you remember correctly, this entry point script ‘refreshes’ the database (through <code>airflow upgradedb</code>, which just checks up that everything is fine in terms of all tables available, connections, etc..) and deploys the webserver (through <code>airflow webserver</code>). The key thing here is that the <code>entrypoint</code> option for this container makes sure that all of this is ran once everything is deployed.</p>
<p>That is all for our <code>docker-compose.yml</code> file. This part is a bit tricky and if you don’t understand everything so far, it’s fine. You can ignore some of these details as this example you’re developing should work out of the box. However, if you’re planning to use Airflow for serious projects which might hold sensitive data, you should study this in depth.</p>
<ol start="5" style="list-style-type: decimal">
<li>Create your tasks that will be executed on a schedule</li>
</ol>
<pre class="{bash"><code>echo &quot;from datetime import timedelta, datetime
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.dummy_operator import DummyOperator

# Define default arguments for the DAG
default_args = {
    &#39;owner&#39;: &#39;cimentadaj&#39;,
    &#39;depends_on_past&#39;: False,
    &#39;start_date&#39;: datetime(2020, 12, 7, 13, 20),
    &#39;email&#39;: [&#39;airflow@example.com&#39;],
    &#39;email_on_failure&#39;: True,
    &#39;retries&#39;: 1,
    &#39;retry_delay&#39;: timedelta(seconds=5)
}

# Define the main DAG: this is a general
# process that can have many tasks inside
dag = DAG(
    &#39;tutorial&#39;,
    default_args=default_args,
    schedule_interval=&#39;*/1 * * * *&#39;,
)

# Define what task_1 will do
def task_1():
    &#39;Print task 1 complete&#39;
    print(&#39;Task 1 complete&#39;)

# Define what task_2 will do
def task_2():
    &#39;Print task 2 complete&#39;
    print(&#39;Task 2 complete&#39;)

# Create initial empty task
task0 = DummyOperator(
    task_id=&#39;start&#39;,
    dag=dag
)

# Create task1
task1 = PythonOperator(
    task_id=&#39;task_1&#39;,
    python_callable=task_1,
    dag=dag
)

# Create task2
task2 = PythonOperator(
    task_id=&#39;task_2&#39;,
    python_callable=task_2,
    dag=dag
)

# Create dependencies between tasks.
# task0 is first, then task1 then task2
task0 &gt;&gt; task1 &gt;&gt; task2
&quot; &gt; dags/airflow-test.py</code></pre>
<p>The above is some Python code which we save to <code>dags/airflow-test.py</code>. I wouldn’t do justice to the superb explanation of how to create a DAG file as the <a href="https://airflow.apache.org/docs/apache-airflow/1.10.7/tutorial.html">Airflow team has done</a> so I’ll limit myself to briefly explaining what I did in the example above.</p>
<p>After loading the Airflow module I set some default arguments which you should update accordingly with your details (owner name, email, <code>start_date</code>, etc..). I then defined the DAG name as ‘tutorial’ where I specify <code>schedule_interval</code>, which has the cron expression for when to run this task (I set it to every minute of every day of the year). I then define some Python functions that I use with the <code>PythonOperator</code> to create the Airflow tasks.</p>
<p>Once you have your tasks created you can create the dependencies between these tasks. For example, by writing <code>task0 &gt;&gt; task1 &gt;&gt; task2</code> I’m saying that <code>task2</code> depends on <code>task1</code> and <code>task1</code> depends on <code>task0</code>. Or the other way around: we begin with <code>task0</code>, once it’s finished we follow with <code>task1</code> and once that’s done, we continue with <code>task2</code>. Depending on another task in this context means that <strong>only</strong> when a given task has ended, the other task will begin. This creates a beautiful framework for creating complex dependencies in no time.</p>
<p>Airflow has different operators for <a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/">different languages</a>, and if your preferred language is not present, the <code>BashOperator</code> makes it easy to run anything not covered (such as R or Julia, for example).</p>
<p>Remember that we linked our <code>dags</code> folder to the container’s <code>dags</code> folder so anything we add in this folder will be automatically synced to the container.</p>
<div id="putting-everything-together" class="section level2">
<h2>Putting everything together</h2>
<p>Before we deploy everything, we need to make sure that this entire folder has all permissions available. Due to some issues I’ve yet to understand, our containers can have some permission problems while trying to access the volumes we specified between our local computer and the containers. Let’s give full access to everyone for that folder:</p>
<pre class="bash"><code>sudo chmod -R 777 $TEST_DIR/airflow_test2</code></pre>
<p>With that out of the way, deploy everything with <code>docker-compose up -d</code>:</p>
<pre class="bash"><code>docker-compose up -d</code></pre>
<pre><code># Creating network &quot;airflow_test2_default&quot; with the default driver
# Starting airflow_test2_postgres_1 ... 
# Starting airflow_test2_webserver_1 ... 
# Starting airflow_test2_scheduler_1 ... </code></pre>
<p>How would all of this look like? Visit the website <a href="http://localhost:8080/admin/" class="uri">http://localhost:8080/admin/</a>.</p>
<p>You should see something like this:</p>
<p><img src="/img/airflow.png" alt="drawing" width="120%"/></p>
<p>We can toggle our task to start, inspect the dependency of the different tasks and also begin the process manually. Below is a GIF showing you how to do it. <strong>Make sure you click on the image for the GIF to start</strong>:</p>
<p><img src="/img/airflow_workflow.gif" alt="drawing" width="120%"/></p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>This was a long post because I wanted to touch upon several details related to Airflow. However, if you want to just make this work, you can copy the worked out example from the beginning and edit your tasks accordingly. I hope this was useful!</p>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>Deploying MySQL database using Docker</title>
      <link>https://cimentadaj.github.io/blog/2020-11-25-deploying-mysql-database-using-docker/deploying-mysql-database-using-docker/</link>
      <pubDate>Wed, 25 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://cimentadaj.github.io/blog/2020-11-25-deploying-mysql-database-using-docker/deploying-mysql-database-using-docker/</guid>
      <description><![CDATA[
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>I have a few projects on which I use databases to collect data. Each time I begin some of these projects I have to install the databases and what not. This is usually burden with a lot of back and forth, following tutorials on how to install the dependencies and so on. It’s NEVER an easy task, taking more than one day to go from wanting to install a database engine and actually having the database ready to connect from local/remote machines with the correct permissions.</p>
<p>Docker is suppose to make this much easier and here I provide a set of easy steps for going from (1) want to install MySQL to (2) having the database deployed and working in no time.</p>
<p>The only two requirements you’ll need for this is <code>docker</code> and <code>docker-compose</code>. <code>docker</code> is the tool for lifting the containers with MySQL and <code>docker-compose</code> is just a convenient way for us to specify some options related to that container in a readable file (instead of a long <code>docker</code> statement with long arguments). Note that <code>docker-compose</code> is used for orchestrating many containers into a single network but we’ll use it just for the convenience of readability. If you don’t have any of these two tools, follow this tutorial for <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-20-04">docker</a> and <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-20-04">this one</a> for <code>docker-compose</code>.</p>
<div id="summarized-version" class="section level2">
<h2>Summarized version</h2>
<p>If you’re here for the code, here’s the summarized versions. Skip down for detailed explanations:</p>
<pre class="bash"><code># Create folder that will host the data and other files
mkdir ~/Downloads/sql_test/
cd ~/Downloads/sql_test
touch .gitignore
echo &quot;data/&quot; &gt;&gt; ./.gitignore

# Create sql folder to host database/table creation scripts
mkdir -p ./sql/

# Append this SQL code to the file sql/init_db.sql to create database/table
echo &quot;
CREATE DATABASE test;
CREATE USER &#39;test_user&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123&#39;;
GRANT ALL ON test.* TO &#39;test_user&#39;@&#39;%&#39;;

/* Make sure the privileges are installed */
FLUSH PRIVILEGES;

USE test;

CREATE TABLE test_table (
  name VARCHAR(30)
);
&quot; &gt;&gt; sql/init_db.sql

# Create a MySQL config file to allow to append data
mkdir sql_conf
echo &quot;
[mysqld]
# This just makes sure we can append data from outside the container
local-infile=1
&quot; &gt;&gt; sql_conf/mysql.cnf

echo &#39;
version: &quot;3.1&quot;
services:
  db:
    container_name: sql-test
    image: mysql:8.0
    restart: always
    ports:
      - 3306:3306
    environment:
      - MYSQL_ROOT_PASSWORD=123456
    volumes:
      - /home/jorge/Downloads/sql_test/sql:/docker-entrypoint-initdb.d
      - /home/jorge/Downloads/sql_test/data:/var/lib/mysql
      - /home/jorge/Downloads/sql_test/sql_conf:/etc/mysql/conf.d
&#39; &gt;&gt; docker-compose.yml

# Lift the container with MySQL
docker-compose up -d</code></pre>
<p>If you get some errors due to some port already in use this could be due to two reasons. Either you have another container redirecting to the port 3306 (use <code>docker ps</code> and <code>docker stop</code> to stop it) or you have a MySQL instance running locally already serving the port. You can stop it with <code>sudo service mysql stop</code>.</p>
<p>With R you can append/read data:</p>
<pre class="r"><code>library(DBI)
library(RMySQL)

con &lt;- dbConnect(MySQL(),
                 host = &#39;127.0.0.1&#39;,
                 dbname = &#39;test&#39;,
                 port = 3306,
                 user = &#39;test_user&#39;,
                 password = &#39;123&#39;)

# Append data
dt &lt;- data.frame(name = &quot;Jorge is my name&quot;)
field_types &lt;- c(name = &quot;TEXT&quot;)
dbWriteTable(conn = con,
             name = &quot;test_table&quot;,
             value = dt,
             append = TRUE,
             row.names = FALSE,
             overwrite = FALSE,
             field.types = field_types)</code></pre>
<pre><code># [1] TRUE</code></pre>
<p>and then extract the data you just appended:</p>
<pre class="r"><code>dbGetQuery(con, &quot;SELECT * FROM test.test_table&quot;)</code></pre>
<pre><code>#               name
# 1 Jorge is my name</code></pre>
</div>
<div id="detailed-version" class="section level2">
<h2>Detailed version</h2>
<p>Here are the detailed steps:</p>
<ul>
<li>Create folder that will contain the data</li>
</ul>
<pre class="bash"><code>mkdir ~/Downloads/sql_test/</code></pre>
<ul>
<li>Navigate to the folder and add stuff we wouldn’t like to commit to Github. Note that <em>we haven’t</em> created a data folder. You’ll see why this is important later.</li>
</ul>
<pre class="bash"><code>cd ~/Downloads/sql_test
touch .gitignore
echo &quot;data/&quot; &gt;&gt; ./.gitignore</code></pre>
<ul>
<li>Create a <code>sql</code> folder and create a file inside (the name doesn’t matter as long as it ends in <code>.sql</code>) that creates the database, the users and the tables that you’re interested in:</li>
</ul>
<pre class="bash"><code># Create sql foldr
mkdir -p ./sql/

# Append this SQL code to the file sql/init_db.sql
echo &quot;
CREATE DATABASE test;
CREATE USER &#39;test_user&#39;@&#39;%&#39; IDENTIFIED WITH mysql_native_password BY &#39;123&#39;;
GRANT ALL ON test.* TO &#39;test_user&#39;@&#39;%&#39;;

/* Make sure the privileges are installed */
FLUSH PRIVILEGES;

USE test;

CREATE TABLE test_table (
  name VARCHAR(30)
);
&quot; &gt;&gt; sql/init_db.sql</code></pre>
<p>Just to make sure we’re on the right track, the previous statement just added some SQL code to a file called <code>init_db.sql</code> which will create all the databases/tables/users you’re interested in the database to have. If you DON’T want to create any database/table, you don’t even have to create a <code>sql</code> folder.</p>
<ul>
<li>Create MySQL config to allow to append data to the database</li>
</ul>
<pre class="bash"><code>mkdir sql_conf
echo &quot;
[mysqld]
# This just makes sure we can append data from outside the container
local-infile=1
&quot; &gt;&gt; sql_conf/mysql.cnf</code></pre>
<ul>
<li>Create a <code>docker-compose</code> file which has the <code>MySQL</code> image with some options.</li>
</ul>
<pre class="bash"><code>echo &#39;
version: &quot;3.1&quot;
services:
  db:
    container_name: sql-test
    image: mysql:8.0
    restart: always
    ports:
      - 3306:3306
    environment:
      - MYSQL_ROOT_PASSWORD=123456
    volumes:
      - /home/jorge/Downloads/sql_test/sql:/docker-entrypoint-initdb.d
      - /home/jorge/Downloads/sql_test/data:/var/lib/mysql
      - /home/jorge/Downloads/sql_test/sql_conf:/etc/mysql/conf.d
&#39; &gt;&gt; docker-compose.yml</code></pre>
<p>I’ll stop here to explain the important fields.</p>
<p><code>container_name</code> is the name that the container will be assigned. You can refer to this name and even log in to that container to execute code inside.</p>
<p><code>image</code> contains the <code>MySQL</code> image tagged at version 8.0. This makes sure that we always download the same version of <code>MySQL</code> and is reproducible within the same OSx.</p>
<p><code>MYSQL_ROOT_PASSWORD</code> is an environmental variable that is integrated in the image <code>mysql:8.0</code>. It assigns a password to the root user in <code>MySQL</code>. This is useful to have a default root password if you want to log in as root to execute higher level privileges.</p>
<p><code>volumes</code> is perhaps the most important thing to explain here. It has three entries with some paths. In docker parlance, volumes are links between your <em>local</em> computer and the virtual docker container. This means that if I link my <em>local</em> folder <code>~/Downloads/my_sql/data/</code> to the container’s folder <code>/var/lib/mysql/</code> everything that is inside <code>~/Downloads/my_sql/data/</code> will be inside <code>/var/lib/mysql/</code> and vice versa. It essentially creates a mirror between the folders on your <em>local</em> computer and your container.</p>
<p>We have three fields here. This first one is <code>/home/jorge/Downloads/sql_test/sql:/docker-entrypoint-initdb.d</code>. The first part, <code>/home/jorge/Downloads/sql_test/sql</code> points to our <code>sql</code> folder containing the SQL code that creates the database/users/tables. <strong>This</strong> particular MySQL image has a folder <code>/docker-entrypoint-initdb.d</code> inside the container which the container will <strong>execute</strong> once it is created. In other words, anything inside <code>/docker-entrypoint-initdb.d</code> with a <code>sql</code> file extension will be executed once the container is deployed. That’s why we’re making a mirror between our script to create the database/table and the folder that will execute everything inside the container.</p>
<p>To specify volums, we separate the two paths with a <code>:</code> where the left path is the local directory and the right path is the container’s directory: <code>/home/jorge/Downloads/sql_test/sql</code><strong>:</strong><code>/docker-entrypoint-initdb.d</code></p>
<p>The second field is <code>/home/jorge/Downloads/sql_test/data:/var/lib/mysql</code>. This field links our data folder <code>/home/jorge/Downloads/sql_test/data</code> to the folder <em>inside</em> the container <code>/var/lib/mysql</code> <em>where</em> <code>MySQL</code> saves all the data. Yep, this means that even if we have MySQL inside the docker container, the data will be saved in our <em>local</em> computer. Best of both worlds! No burden of installing MySQL/dependencies, yet we can transfer all the MySQL data to our local machine. This means that we can stop the container any time, rerun it with the same <code>docker-compose.yml</code> and it will populate all databases with the data in the local computer. We’ll do an example in just a second.</p>
<p>Note that we <em>haven’t</em> created a data folder yet. This is because our <code>MySQL</code> image works this way: if it finds that the link between the data folders has been created, it assumes that there is some data in there and it <em>won’t</em> execute the SQL files for creating the database/tables. So for the first deployment of the docker container, we don’t create the folder, allowing the container to create the database/tables. Once we stop the container and rerun it again, it won’t execute the SQL scripts and just read the data in the linked volumes.</p>
<p>The third volume <code>/home/jorge/Downloads/sql_test/sql_conf:/etc/mysql/conf.d/</code> links our MySQL config to the place where MySQL searches for config files inside the container (<code>/etc/mysql/conf.d</code>). This small config just allows users outside the container to append data.</p>
<ul>
<li>Deploy the docker container</li>
</ul>
<p>The previous step might be the most daunting but let me break it to you: that is it. There’s nothing else to do but deploy everything.</p>
<pre class="bash"><code>docker-compose up -d</code></pre>
<pre><code># Creating network &quot;sql_test_default&quot; with the default driver
# Creating sql-test ... </code></pre>
<p>This command can take some time, as it’s downloading the image at first.</p>
<p>If you get some errors due to some port already in use this could be due to two reasons. Either you have another container redirecting to the port 3306 (use <code>docker ps</code> and <code>docker stop</code> to stop it) or you have a MySQL instance running locally already serving the port. You can stop it with <code>sudo service mysql stop</code>.</p>
<ul>
<li>Log in to the container and check the test database is there.</li>
</ul>
<p>Note that the container needs a few seconds to create the database/users/tables so wait a minute or two before running the queries below.</p>
<pre class="bash"><code># Remember that the password we specified was 123
mysql -h 127.0.0.1 -P 3306 -u test_user  -p -e &quot;SHOW DATABASES;&quot;</code></pre>
<pre><code># +--------------------+
# | Database           |
# +--------------------+
# | information_schema |
# | test               |
# +--------------------+</code></pre>
<p>You should see a database called test (the one we created). You can also connect from R and append data:</p>
<pre class="r"><code>library(DBI)
library(RMySQL)

con &lt;- dbConnect(MySQL(),
                 host = &#39;127.0.0.1&#39;,
                 dbname = &#39;test&#39;,
                 port = 3306,
                 user = &#39;test_user&#39;,
                 password = &#39;123&#39;)

# Append data
dt &lt;- data.frame(name = &quot;Jorge is my name&quot;)
field_types &lt;- c(name = &quot;TEXT&quot;)
dbWriteTable(conn = con,
             name = &quot;test_table&quot;,
             value = dt,
             append = TRUE,
             row.names = FALSE,
             overwrite = FALSE,
             field.types = field_types)</code></pre>
<pre><code># [1] TRUE</code></pre>
<p>and then extract the data you just appended:</p>
<pre class="r"><code>dbGetQuery(con, &quot;SELECT * FROM test.test_table&quot;)</code></pre>
<pre><code>#               name
# 1 Jorge is my name</code></pre>
<p>The cool thing about this is that we can take down the database with <code>docker-compose down</code>, put it up again with <code>docker-compose up -d</code> and the data we saved is still there:</p>
<pre class="bash"><code>docker-compose down</code></pre>
<pre><code># Stopping sql-test ... 
# Removing sql-test ... 
# Removing network sql_test_default</code></pre>
<pre class="bash"><code>docker-compose up -d</code></pre>
<pre><code># Creating network &quot;sql_test_default&quot; with the default driver
# Creating sql-test ... </code></pre>
<pre class="bash"><code># Remember the password is 123
mysql -h 127.0.0.1 -P 3306 -u test_user  -p -e &quot;SELECT * FROM test.test_table;&quot;</code></pre>
<pre><code># +------------------+
# | name             |
# +------------------+
# | Jorge is my name |
# +------------------+</code></pre>
<p>If you were deploying this on a remote server, everything applies and you would only have to change the IP to your server’s IP.</p>
</div>
]]>
      </description>
    </item>
    
  </channel>
</rss>
